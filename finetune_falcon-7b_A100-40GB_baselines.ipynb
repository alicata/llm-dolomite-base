{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Dolomite-Baseline: for fine-tuning Falcon-7b over A100-40GB\n",
        "\n",
        "* repo setup\n",
        "* checkpoint download - tiiuae/falcon-7b\n",
        "* inference test\n",
        "* finetune: reproduce stable baseline 50k_iters  \n",
        "* finetune: search new baseline with auto-checkpoints and longer training\n",
        "\n"
      ],
      "metadata": {
        "id": "6oiFo0vOJfoz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vV9rRJOUqHe5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3089c0a-d2c5-44a6-f2be-1595b586b667"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Aug 14 04:47:51 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    44W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phase 1. Setup repo, install dependencies, download model weights"
      ],
      "metadata": {
        "id": "DYhDOvo-tz4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Lightning-AI/lit-gpt\n",
        "!git clone https://github.com/alicata/llm-dolomite-base.git\n",
        "!cp llm-dolomite-base/lora_* lit-gpt/finetune\n",
        "\n",
        "%cd lit-gpt\n",
        "\n",
        "# Main repo with CUDA and flash attention\n",
        "!pip install --index-url https://download.pytorch.org/whl/nightly/cu118 --pre 'torch>=2.1.0dev' -q\n",
        "# Remove audio dependency - integration bug\n",
        "!pip uninstall -y torchaudio\n",
        "# install the dependencies\n",
        "!pip install -r requirements.txt\n",
        "!pip install huggingface_hub\n",
        "\n",
        "# download the original Falcon-7B weights and convert into Lit-GPT compatible model format\n",
        "!python scripts/download.py --repo_id tiiuae/falcon-7b\n",
        "!python scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/tiiuae/falcon-7b"
      ],
      "metadata": {
        "id": "-VFKqx5Xt0gm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adc6a23b-9002-455a-897e-4103ed29a8c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lit-gpt'...\n",
            "remote: Enumerating objects: 3254, done.\u001b[K\n",
            "remote: Counting objects: 100% (265/265), done.\u001b[K\n",
            "remote: Compressing objects: 100% (167/167), done.\u001b[K\n",
            "remote: Total 3254 (delta 151), reused 184 (delta 93), pack-reused 2989\u001b[K\n",
            "Receiving objects: 100% (3254/3254), 1005.33 KiB | 2.25 MiB/s, done.\n",
            "Resolving deltas: 100% (2183/2183), done.\n",
            "Cloning into 'llm-dolomite-base'...\n",
            "remote: Enumerating objects: 92, done.\u001b[K\n",
            "remote: Counting objects: 100% (92/92), done.\u001b[K\n",
            "remote: Compressing objects: 100% (83/83), done.\u001b[K\n",
            "remote: Total 92 (delta 42), reused 4 (delta 1), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (92/92), 15.59 MiB | 5.11 MiB/s, done.\n",
            "Resolving deltas: 100% (42/42), done.\n",
            "/content/lit-gpt\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m873.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.1.0.dev20230813+cu118 which is incompatible.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 2.1.0.dev20230813+cu118 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 2.1.0.dev20230813+cu118 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 2.1.0.dev20230813+cu118 which is incompatible.\n",
            "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 2.1.0.dev20230813+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mFound existing installation: torchaudio 2.0.2+cu118\n",
            "Uninstalling torchaudio-2.0.2+cu118:\n",
            "  Successfully uninstalled torchaudio-2.0.2+cu118\n",
            "Collecting lightning@ git+https://github.com/Lightning-AI/lightning@master (from -r requirements.txt (line 2))\n",
            "  Cloning https://github.com/Lightning-AI/lightning (to revision master) to /tmp/pip-install-bkzq7g1_/lightning_2fd7e0f587c3421889d571b265742720\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Lightning-AI/lightning /tmp/pip-install-bkzq7g1_/lightning_2fd7e0f587c3421889d571b265742720\n",
            "  Resolved https://github.com/Lightning-AI/lightning to commit 3142ed5e4403d5c4cae26662e52e0d6a5d3668db\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Encountered 31 file(s) that should have been pointers, but weren't:\n",
            "        .notebooks/course_UvA-DL/01-introduction-to-pytorch.ipynb\n",
            "        .notebooks/course_UvA-DL/02-activation-functions.ipynb\n",
            "        .notebooks/course_UvA-DL/03-initialization-and-optimization.ipynb\n",
            "        .notebooks/course_UvA-DL/04-inception-resnet-densenet.ipynb\n",
            "        .notebooks/course_UvA-DL/05-transformers-and-MH-attention.ipynb\n",
            "        .notebooks/course_UvA-DL/06-graph-neural-networks.ipynb\n",
            "        .notebooks/course_UvA-DL/07-deep-energy-based-generative-models.ipynb\n",
            "        .notebooks/course_UvA-DL/08-deep-autoencoders.ipynb\n",
            "        .notebooks/course_UvA-DL/09-normalizing-flows.ipynb\n",
            "        .notebooks/course_UvA-DL/10-autoregressive-image-modeling.ipynb\n",
            "        .notebooks/course_UvA-DL/11-vision-transformer.ipynb\n",
            "        .notebooks/course_UvA-DL/12-meta-learning.ipynb\n",
            "        .notebooks/course_UvA-DL/13-contrastive-learning.ipynb\n",
            "        .notebooks/flash_tutorials/electricity_forecasting.ipynb\n",
            "        .notebooks/flash_tutorials/image_classification.ipynb\n",
            "        .notebooks/flash_tutorials/tabular_classification.ipynb\n",
            "        .notebooks/flash_tutorials/text_classification.ipynb\n",
            "        .notebooks/lightning_examples/augmentation_kornia.ipynb\n",
            "        .notebooks/lightning_examples/barlow-twins.ipynb\n",
            "        .notebooks/lightning_examples/basic-gan.ipynb\n",
            "        .notebooks/lightning_examples/cifar10-baseline.ipynb\n",
            "        .notebooks/lightning_examples/datamodules.ipynb\n",
            "        .notebooks/lightning_examples/finetuning-scheduler.ipynb\n",
            "        .notebooks/lightning_examples/mnist-hello-world.ipynb\n",
            "        .notebooks/lightning_examples/mnist-tpu-training.ipynb\n",
            "        .notebooks/lightning_examples/reinforce-learning-DQN.ipynb\n",
            "        .notebooks/lightning_examples/text-transformers.ipynb\n",
            "        .notebooks/lightning_examples/warp-drive.ipynb\n",
            "        .notebooks/templates/img-classify.ipynb\n",
            "        .notebooks/templates/simple.ipynb\n",
            "        .notebooks/templates/titanic.ipynb\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tokenizers (from -r requirements.txt (line 3))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonargparse[signatures] (from -r requirements.txt (line 4))\n",
            "  Downloading jsonargparse-4.23.1-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes>=0.40.0 (from -r requirements.txt (line 5))\n",
            "  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.10.1)\n",
            "Collecting datasets (from -r requirements.txt (line 7))\n",
            "  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zstandard (from -r requirements.txt (line 8))\n",
            "  Downloading zstandard-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Jinja2<5.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (3.1.2)\n",
            "Requirement already satisfied: PyYAML<8.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (6.0.1)\n",
            "Collecting arrow<3.0,>=1.2.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading arrow-1.2.3-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff<4.0,>=2.2.1 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: beautifulsoup4<6.0,>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (4.11.2)\n",
            "Requirement already satisfied: click<10.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (8.1.6)\n",
            "Collecting croniter<1.5.0,>=1.3.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading croniter-1.4.1-py2.py3-none-any.whl (19 kB)\n",
            "Collecting dateutils<2.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading dateutils-0.6.12-py2.py3-none-any.whl (5.7 kB)\n",
            "Collecting deepdiff<8.0,>=5.7.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading deepdiff-6.3.1-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.7/70.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi<2.0,>=0.92.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading fastapi-0.101.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec<2025.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (2023.6.0)\n",
            "Collecting inquirer<5.0,>=2.10.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading inquirer-3.1.3-py3-none-any.whl (18 kB)\n",
            "Collecting lightning-cloud>=0.5.37 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading lightning_cloud-0.5.37-py3-none-any.whl (596 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.7/596.7 kB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightning-utilities<2.0,>=0.8.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (23.1)\n",
            "Requirement already satisfied: psutil<7.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (5.9.5)\n",
            "Requirement already satisfied: pydantic<2.2.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (2.1.1)\n",
            "Collecting python-multipart<2.0,>=0.0.5 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<4.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: rich<15.0,>=12.3.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (13.5.2)\n",
            "Collecting starlette (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading starlette-0.31.0-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.8/69.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starsessions<2.0,>=1.2.1 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading starsessions-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: torch<4.0,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (2.1.0.dev20230813+cu118)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading torchmetrics-1.0.3-py3-none-any.whl (731 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.6/731.6 kB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (4.66.0)\n",
            "Requirement already satisfied: traitlets<7.0,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (5.7.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (4.7.1)\n",
            "Requirement already satisfied: urllib3<4.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (2.0.4)\n",
            "Collecting uvicorn<2.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: websocket-client<3.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (1.6.1)\n",
            "Collecting websockets<13.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading pytorch_lightning-2.0.6-py3-none-any.whl (722 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m722.8/722.8 kB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docstring-parser>=0.15 (from jsonargparse[signatures]->-r requirements.txt (line 4))\n",
            "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
            "Collecting typeshed-client>=2.1.0 (from jsonargparse[signatures]->-r requirements.txt (line 4))\n",
            "  Downloading typeshed_client-2.3.0-py3-none-any.whl (581 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.6/581.6 kB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 7)) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets->-r requirements.txt (line 7))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 7)) (1.5.3)\n",
            "Collecting xxhash (from datasets->-r requirements.txt (line 7))\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets->-r requirements.txt (line 7))\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 7)) (3.8.5)\n",
            "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets->-r requirements.txt (line 7))\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from arrow<3.0,>=1.2.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<6.0,>=4.8.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (2.4.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from dateutils<2.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (2023.3)\n",
            "Collecting ordered-set<4.2.0,>=4.0.2 (from deepdiff<8.0,>=5.7.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Collecting starlette (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets->-r requirements.txt (line 7)) (3.12.2)\n",
            "Collecting blessed>=1.19.0 (from inquirer<5.0,>=2.10.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-editor>=1.0.4 (from inquirer<5.0,>=2.10.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
            "Collecting readchar>=3.0.6 (from inquirer<5.0,>=2.10.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading readchar-4.0.5-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<5.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (2.1.3)\n",
            "Requirement already satisfied: pyjwt in /usr/lib/python3/dist-packages (from lightning-cloud>=0.5.37->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (2.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from lightning-cloud>=0.5.37->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.2.0,>=1.7.4->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.2.0,>=1.7.4->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (2.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<4.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<4.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (2023.7.22)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<15.0,>=12.3.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<15.0,>=12.3.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (2.16.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (3.7.1)\n",
            "Requirement already satisfied: itsdangerous<3.0.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from starsessions<2.0,>=1.2.1->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (2.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.11.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.11.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (3.1)\n",
            "Requirement already satisfied: pytorch-triton==2.1.0+e6216047b8 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.11.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (2.1.0+e6216047b8)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from typeshed-client>=2.1.0->jsonargparse[signatures]->-r requirements.txt (line 4)) (6.0.1)\n",
            "Collecting h11>=0.8 (from uvicorn<2.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (1.1.2)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.19.0->inquirer<5.0,>=2.10.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (0.2.6)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<15.0,>=12.3.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (0.1.2)\n",
            "Requirement already satisfied: setuptools>=41.0 in /usr/local/lib/python3.10/dist-packages (from readchar>=3.0.6->inquirer<5.0,>=2.10.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (67.7.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=1.11.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (1.3.0)\n",
            "Building wheels for collected packages: lightning\n",
            "  Building wheel for lightning (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lightning: filename=lightning-2.1.0.dev0-py3-none-any.whl size=1887292 sha256=2967433556becc67f378f6414a03b43b98e4eb914155154586be18bf1256aeac\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wnat64rd/wheels/b9/92/07/634ec381ab7d682d3afdcf943d0a6604881441ff2d6c409103\n",
            "Successfully built lightning\n",
            "Installing collected packages: tokenizers, python-editor, bitsandbytes, zstandard, xxhash, websockets, typeshed-client, readchar, python-multipart, ordered-set, lightning-utilities, jsonargparse, h11, docstring-parser, dill, blessed, backoff, uvicorn, starlette, multiprocess, inquirer, huggingface-hub, deepdiff, dateutils, croniter, arrow, torchmetrics, starsessions, fastapi, pytorch-lightning, lightning-cloud, datasets, lightning\n",
            "Successfully installed arrow-1.2.3 backoff-2.2.1 bitsandbytes-0.41.1 blessed-1.20.0 croniter-1.4.1 datasets-2.14.4 dateutils-0.6.12 deepdiff-6.3.1 dill-0.3.7 docstring-parser-0.15 fastapi-0.101.0 h11-0.14.0 huggingface-hub-0.16.4 inquirer-3.1.3 jsonargparse-4.23.1 lightning-2.1.0.dev0 lightning-cloud-0.5.37 lightning-utilities-0.9.0 multiprocess-0.70.15 ordered-set-4.1.0 python-editor-1.0.4 python-multipart-0.0.6 pytorch-lightning-2.0.6 readchar-4.0.5 starlette-0.27.0 starsessions-1.3.0 tokenizers-0.13.3 torchmetrics-1.0.3 typeshed-client-2.3.0 uvicorn-0.23.2 websockets-11.0.3 xxhash-3.3.0 zstandard-0.21.0\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.16.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.7.22)\n",
            "/usr/local/lib/python3.10/dist-packages/pydantic/_migration.py:283: UserWarning: `pydantic.utils:Representation` has been removed. We are importing from `pydantic.v1.utils:Representation` instead.See the migration guide for more details: https://docs.pydantic.dev/latest/migration/\n",
            "  warnings.warn(\n",
            "Fetching 5 files:   0% 0/5 [00:00<?, ?it/s]\n",
            "Downloading (…)l-00001-of-00002.bin:   0% 0.00/9.95G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "Downloading (…)okenizer_config.json: 100% 220/220 [00:00<00:00, 1.80MB/s]\n",
            "\n",
            "\n",
            "Downloading (…)model.bin.index.json: 100% 16.9k/16.9k [00:00<00:00, 72.3MB/s]\n",
            "\n",
            "\n",
            "Downloading (…)d4eb1/tokenizer.json:   0% 0.00/2.73M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:   0% 31.5M/9.95G [00:00<00:35, 282MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:   1% 73.4M/9.95G [00:00<00:30, 322MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:   1% 115M/9.95G [00:00<00:28, 339MB/s] \u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:   2% 157M/9.95G [00:00<00:27, 361MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:   2% 199M/9.95G [00:00<00:28, 342MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)d4eb1/tokenizer.json: 100% 2.73M/2.73M [00:00<00:00, 4.30MB/s]\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:   2% 241M/9.95G [00:00<00:27, 353MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:   3% 283M/9.95G [00:00<00:25, 372MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   0% 0.00/4.48G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:   3% 325M/9.95G [00:00<00:26, 363MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   1% 41.9M/4.48G [00:00<00:11, 384MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:   4% 367M/9.95G [00:01<00:27, 344MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   2% 105M/4.48G [00:00<00:09, 485MB/s] \u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:   4% 409M/9.95G [00:01<00:27, 351MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   4% 157M/4.48G [00:00<00:08, 492MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   5% 220M/4.48G [00:00<00:08, 515MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:   5% 451M/9.95G [00:01<00:31, 301MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   6% 273M/4.48G [00:00<00:08, 501MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:   5% 493M/9.95G [00:01<00:34, 278MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   7% 325M/4.48G [00:00<00:08, 498MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   8% 377M/4.48G [00:00<00:08, 497MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:   5% 535M/9.95G [00:01<00:32, 288MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  10% 430M/4.48G [00:00<00:08, 503MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:   6% 577M/9.95G [00:01<00:32, 289MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  11% 482M/4.48G [00:00<00:08, 479MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:   6% 608M/9.95G [00:01<00:31, 294MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  12% 535M/4.48G [00:01<00:08, 483MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:   6% 640M/9.95G [00:02<00:31, 298MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  13% 587M/4.48G [00:01<00:08, 473MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:   7% 682M/9.95G [00:02<00:30, 300MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  14% 640M/4.48G [00:01<00:08, 458MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:   7% 724M/9.95G [00:02<00:30, 306MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  15% 692M/4.48G [00:01<00:08, 454MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:   8% 765M/9.95G [00:02<00:29, 315MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  17% 744M/4.48G [00:01<00:08, 439MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:   8% 807M/9.95G [00:02<00:28, 324MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  18% 797M/4.48G [00:01<00:08, 430MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:   9% 849M/9.95G [00:02<00:28, 323MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  19% 849M/4.48G [00:01<00:08, 421MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:   9% 891M/9.95G [00:02<00:27, 324MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  20% 902M/4.48G [00:01<00:08, 420MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:   9% 933M/9.95G [00:02<00:27, 333MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  21% 954M/4.48G [00:02<00:08, 426MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  10% 986M/9.95G [00:03<00:26, 340MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  22% 1.01G/4.48G [00:02<00:09, 357MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  10% 1.03G/9.95G [00:03<00:36, 246MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  11% 1.06G/9.95G [00:03<00:37, 239MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  23% 1.05G/4.48G [00:02<00:14, 243MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  11% 1.09G/9.95G [00:03<00:39, 226MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  24% 1.08G/4.48G [00:02<00:15, 222MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  25% 1.11G/4.48G [00:03<00:16, 203MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  11% 1.12G/9.95G [00:03<00:52, 168MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  25% 1.14G/4.48G [00:03<00:17, 190MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  11% 1.14G/9.95G [00:04<00:53, 164MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  26% 1.16G/4.48G [00:03<00:19, 170MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  27% 1.20G/4.48G [00:03<00:18, 176MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  12% 1.16G/9.95G [00:04<01:11, 123MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  27% 1.22G/4.48G [00:03<00:19, 167MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  12% 1.18G/9.95G [00:04<01:10, 125MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  28% 1.25G/4.48G [00:03<00:17, 187MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  12% 1.21G/9.95G [00:04<01:04, 135MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  28% 1.27G/4.48G [00:03<00:17, 185MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  12% 1.24G/9.95G [00:04<00:54, 159MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  13% 1.26G/9.95G [00:04<00:52, 167MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  29% 1.30G/4.48G [00:04<00:18, 171MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  29% 1.32G/4.48G [00:04<00:19, 165MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  13% 1.28G/9.95G [00:05<01:03, 137MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  30% 1.34G/4.48G [00:04<00:18, 168MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  13% 1.30G/9.95G [00:05<00:59, 145MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  30% 1.36G/4.48G [00:04<00:18, 168MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  13% 1.33G/9.95G [00:05<00:56, 153MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  31% 1.38G/4.48G [00:04<00:17, 172MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  31% 1.41G/4.48G [00:04<00:18, 169MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  14% 1.36G/9.95G [00:05<00:53, 160MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  14% 1.38G/9.95G [00:05<00:51, 166MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  32% 1.43G/4.48G [00:04<00:18, 164MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  14% 1.41G/9.95G [00:05<00:53, 160MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  32% 1.45G/4.48G [00:05<00:19, 157MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  33% 1.47G/4.48G [00:05<00:20, 146MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  14% 1.43G/9.95G [00:06<01:00, 141MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  33% 1.49G/4.48G [00:05<00:21, 139MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  15% 1.45G/9.95G [00:06<01:01, 138MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  34% 1.51G/4.48G [00:05<00:21, 136MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  15% 1.47G/9.95G [00:06<01:03, 135MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  34% 1.53G/4.48G [00:05<00:23, 126MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  15% 1.49G/9.95G [00:06<01:07, 126MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  35% 1.55G/4.48G [00:05<00:25, 113MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  15% 1.51G/9.95G [00:06<01:14, 113MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  35% 1.57G/4.48G [00:06<00:31, 92.5MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  15% 1.53G/9.95G [00:07<01:35, 88.3MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  35% 1.58G/4.48G [00:06<00:36, 79.8MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  15% 1.54G/9.95G [00:07<01:48, 77.8MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  36% 1.59G/4.48G [00:06<00:39, 73.6MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  16% 1.55G/9.95G [00:07<01:51, 75.5MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  36% 1.60G/4.48G [00:06<00:39, 73.1MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  16% 1.56G/9.95G [00:07<01:51, 75.1MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  36% 1.61G/4.48G [00:06<00:37, 76.0MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  16% 1.57G/9.95G [00:07<01:47, 78.0MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  36% 1.64G/4.48G [00:07<00:27, 102MB/s] \u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  16% 1.59G/9.95G [00:07<01:20, 104MB/s] \u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  37% 1.66G/4.48G [00:07<00:22, 126MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  16% 1.61G/9.95G [00:08<01:05, 127MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  38% 1.69G/4.48G [00:07<00:18, 152MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  17% 1.65G/9.95G [00:08<00:54, 153MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  17% 1.67G/9.95G [00:08<00:49, 166MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  38% 1.71G/4.48G [00:07<00:17, 160MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  39% 1.73G/4.48G [00:07<00:16, 164MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  17% 1.69G/9.95G [00:08<00:50, 163MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  17% 1.71G/9.95G [00:08<00:50, 164MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  39% 1.75G/4.48G [00:07<00:17, 158MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  17% 1.73G/9.95G [00:08<00:53, 155MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  40% 1.77G/4.48G [00:07<00:17, 152MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  40% 1.79G/4.48G [00:08<00:18, 148MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  18% 1.75G/9.95G [00:08<00:56, 144MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  18% 1.77G/9.95G [00:09<00:58, 139MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  40% 1.81G/4.48G [00:08<00:19, 136MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  41% 1.84G/4.48G [00:08<00:21, 126MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  18% 1.79G/9.95G [00:09<01:06, 122MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  41% 1.86G/4.48G [00:08<00:18, 140MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  18% 1.81G/9.95G [00:09<00:59, 137MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  42% 1.89G/4.48G [00:08<00:14, 173MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  19% 1.85G/9.95G [00:09<00:48, 167MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  43% 1.92G/4.48G [00:08<00:13, 196MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  19% 1.88G/9.95G [00:09<00:41, 194MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  19% 1.91G/9.95G [00:09<00:36, 222MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  44% 1.96G/4.48G [00:08<00:10, 235MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  20% 1.95G/9.95G [00:09<00:29, 269MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  45% 2.01G/4.48G [00:08<00:08, 290MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  20% 2.00G/9.95G [00:09<00:25, 313MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  46% 2.07G/4.48G [00:09<00:07, 333MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  47% 2.12G/4.48G [00:09<00:06, 374MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  21% 2.04G/9.95G [00:10<00:25, 311MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  48% 2.17G/4.48G [00:09<00:05, 409MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  50% 2.22G/4.48G [00:09<00:05, 418MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  21% 2.09G/9.95G [00:10<00:30, 257MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  51% 2.28G/4.48G [00:09<00:05, 415MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  21% 2.13G/9.95G [00:10<00:28, 278MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  52% 2.33G/4.48G [00:09<00:05, 415MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  22% 2.17G/9.95G [00:10<00:26, 292MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  22% 2.20G/9.95G [00:10<00:40, 189MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  53% 2.37G/4.48G [00:10<00:08, 244MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  22% 2.23G/9.95G [00:11<00:36, 211MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  54% 2.41G/4.48G [00:10<00:07, 267MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  23% 2.26G/9.95G [00:11<00:40, 189MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  55% 2.45G/4.48G [00:10<00:08, 241MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  23% 2.30G/9.95G [00:11<00:55, 139MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  55% 2.49G/4.48G [00:10<00:11, 168MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  57% 2.54G/4.48G [00:10<00:09, 216MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  23% 2.34G/9.95G [00:11<00:48, 156MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  57% 2.57G/4.48G [00:11<00:10, 183MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  58% 2.62G/4.48G [00:11<00:08, 230MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  24% 2.36G/9.95G [00:12<01:10, 108MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  60% 2.67G/4.48G [00:11<00:06, 273MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  24% 2.40G/9.95G [00:12<00:51, 146MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  61% 2.72G/4.48G [00:11<00:05, 300MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  62% 2.76G/4.48G [00:11<00:08, 192MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  24% 2.43G/9.95G [00:12<01:05, 114MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  63% 2.81G/4.48G [00:12<00:07, 229MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  25% 2.45G/9.95G [00:13<01:19, 94.9MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  63% 2.84G/4.48G [00:12<00:08, 183MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  25% 2.47G/9.95G [00:13<01:11, 104MB/s] \u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  65% 2.90G/4.48G [00:12<00:06, 248MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  66% 2.95G/4.48G [00:12<00:06, 236MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  25% 2.50G/9.95G [00:13<01:19, 93.4MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  66% 2.98G/4.48G [00:12<00:08, 179MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  25% 2.52G/9.95G [00:13<01:26, 85.9MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  67% 3.02G/4.48G [00:13<00:06, 210MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  26% 2.54G/9.95G [00:14<01:15, 97.8MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  68% 3.05G/4.48G [00:13<00:06, 222MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  26% 2.56G/9.95G [00:14<01:04, 114MB/s] \u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  26% 2.60G/9.95G [00:14<00:45, 161MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  69% 3.08G/4.48G [00:13<00:06, 205MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  26% 2.62G/9.95G [00:14<00:45, 160MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  69% 3.11G/4.48G [00:13<00:07, 186MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  27% 2.64G/9.95G [00:14<00:46, 156MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  27% 2.66G/9.95G [00:14<00:45, 158MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  70% 3.15G/4.48G [00:13<00:07, 177MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  27% 2.68G/9.95G [00:14<00:47, 153MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  71% 3.17G/4.48G [00:13<00:07, 167MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  27% 2.71G/9.95G [00:14<00:48, 149MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  71% 3.19G/4.48G [00:14<00:08, 160MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  27% 2.73G/9.95G [00:15<00:47, 151MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  72% 3.21G/4.48G [00:14<00:08, 159MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  28% 2.75G/9.95G [00:15<00:48, 147MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  72% 3.23G/4.48G [00:14<00:08, 153MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  28% 2.77G/9.95G [00:15<00:49, 145MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  73% 3.25G/4.48G [00:14<00:08, 149MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  28% 2.79G/9.95G [00:15<00:48, 147MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  28% 2.81G/9.95G [00:15<00:50, 142MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  73% 3.27G/4.48G [00:14<00:10, 111MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  28% 2.83G/9.95G [00:15<00:50, 142MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  74% 3.31G/4.48G [00:15<00:07, 153MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  29% 2.85G/9.95G [00:16<00:52, 135MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  74% 3.33G/4.48G [00:15<00:08, 143MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  29% 2.87G/9.95G [00:16<00:55, 127MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  75% 3.36G/4.48G [00:15<00:08, 133MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  29% 2.89G/9.95G [00:16<00:59, 119MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  75% 3.38G/4.48G [00:15<00:08, 129MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  29% 2.92G/9.95G [00:16<01:00, 115MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  76% 3.40G/4.48G [00:15<00:09, 116MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  30% 2.94G/9.95G [00:16<01:06, 105MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  76% 3.42G/4.48G [00:16<00:10, 106MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  30% 2.96G/9.95G [00:17<01:10, 99.0MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  77% 3.44G/4.48G [00:16<00:10, 98.8MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  30% 2.97G/9.95G [00:17<01:12, 96.9MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  77% 3.45G/4.48G [00:16<00:10, 96.8MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  30% 2.98G/9.95G [00:17<01:15, 92.0MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  77% 3.46G/4.48G [00:16<00:10, 95.0MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  30% 2.99G/9.95G [00:17<01:16, 91.1MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  77% 3.47G/4.48G [00:16<00:11, 90.1MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  30% 3.00G/9.95G [00:17<01:17, 90.1MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  78% 3.49G/4.48G [00:16<00:08, 110MB/s] \u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  30% 3.02G/9.95G [00:17<01:01, 112MB/s] \u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  78% 3.51G/4.48G [00:16<00:07, 124MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  31% 3.04G/9.95G [00:17<00:59, 116MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  79% 3.53G/4.48G [00:17<00:07, 125MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  31% 3.07G/9.95G [00:18<00:46, 148MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  80% 3.57G/4.48G [00:17<00:06, 148MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  31% 3.09G/9.95G [00:18<00:43, 157MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  80% 3.59G/4.48G [00:17<00:06, 147MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  31% 3.11G/9.95G [00:18<00:44, 155MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  80% 3.61G/4.48G [00:17<00:05, 159MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  32% 3.14G/9.95G [00:18<00:41, 163MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  81% 3.63G/4.48G [00:17<00:05, 168MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  32% 3.16G/9.95G [00:18<00:41, 163MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  81% 3.65G/4.48G [00:17<00:04, 175MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  32% 3.18G/9.95G [00:18<00:41, 162MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  82% 3.67G/4.48G [00:17<00:04, 171MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  32% 3.20G/9.95G [00:18<00:40, 168MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  82% 3.69G/4.48G [00:17<00:04, 167MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  32% 3.22G/9.95G [00:18<00:38, 173MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  83% 3.71G/4.48G [00:18<00:04, 156MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  33% 3.24G/9.95G [00:19<00:44, 151MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  83% 3.73G/4.48G [00:18<00:04, 166MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  33% 3.26G/9.95G [00:19<00:42, 159MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  84% 3.76G/4.48G [00:18<00:03, 184MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  33% 3.29G/9.95G [00:19<00:36, 181MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  85% 3.80G/4.48G [00:18<00:03, 193MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  33% 3.31G/9.95G [00:19<00:35, 187MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  34% 3.33G/9.95G [00:19<00:34, 192MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  85% 3.83G/4.48G [00:18<00:03, 204MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  34% 3.37G/9.95G [00:19<00:30, 216MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  86% 3.86G/4.48G [00:18<00:02, 220MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  34% 3.41G/9.95G [00:19<00:26, 247MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  87% 3.90G/4.48G [00:18<00:02, 254MB/s]\u001b[A\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  35% 3.45G/9.95G [00:19<00:22, 289MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  88% 3.95G/4.48G [00:18<00:01, 316MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  90% 4.02G/4.48G [00:19<00:01, 380MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  91% 4.06G/4.48G [00:19<00:01, 383MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  92% 4.11G/4.48G [00:19<00:00, 401MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  93% 4.15G/4.48G [00:19<00:00, 397MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  94% 4.19G/4.48G [00:19<00:00, 395MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  95% 4.25G/4.48G [00:19<00:00, 412MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  96% 4.30G/4.48G [00:19<00:00, 431MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  97% 4.35G/4.48G [00:19<00:00, 440MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  98% 4.40G/4.48G [00:20<00:00, 449MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin: 100% 4.48G/4.48G [00:20<00:00, 222MB/s]\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  35% 3.48G/9.95G [00:22<03:19, 32.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  35% 3.50G/9.95G [00:25<04:54, 21.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  35% 3.52G/9.95G [00:26<05:59, 17.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  36% 3.54G/9.95G [00:29<07:24, 14.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  36% 3.55G/9.95G [00:30<07:54, 13.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  36% 3.57G/9.95G [00:31<08:21, 12.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  36% 3.58G/9.95G [00:32<08:47, 12.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  36% 3.59G/9.95G [00:33<08:45, 12.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  36% 3.60G/9.95G [00:34<09:07, 11.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  36% 3.61G/9.95G [00:35<08:59, 11.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  36% 3.62G/9.95G [00:36<09:14, 11.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  36% 3.63G/9.95G [00:37<09:08, 11.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  37% 3.64G/9.95G [00:37<09:00, 11.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  37% 3.65G/9.95G [00:38<09:18, 11.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  37% 3.66G/9.95G [00:39<09:29, 11.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  37% 3.67G/9.95G [00:40<09:08, 11.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  37% 3.68G/9.95G [00:41<09:03, 11.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  37% 3.69G/9.95G [00:42<08:52, 11.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  37% 3.70G/9.95G [00:43<08:51, 11.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  37% 3.71G/9.95G [00:44<09:05, 11.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  37% 3.72G/9.95G [00:45<08:51, 11.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  38% 3.73G/9.95G [00:46<08:46, 11.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  38% 3.74G/9.95G [00:47<09:13, 11.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  38% 3.75G/9.95G [00:48<09:32, 10.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  38% 3.76G/9.95G [00:49<09:43, 10.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  38% 3.77G/9.95G [00:50<09:17, 11.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  38% 3.79G/9.95G [00:50<08:58, 11.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  38% 3.80G/9.95G [00:51<08:43, 11.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  38% 3.81G/9.95G [00:52<08:38, 11.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  38% 3.82G/9.95G [00:53<08:29, 12.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  38% 3.83G/9.95G [00:54<08:22, 12.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  39% 3.84G/9.95G [00:55<08:17, 12.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  39% 3.85G/9.95G [00:56<09:22, 10.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  39% 3.86G/9.95G [00:57<10:54, 9.31MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  39% 3.87G/9.95G [00:59<11:52, 8.53MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  39% 3.88G/9.95G [01:00<12:32, 8.07MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  39% 3.89G/9.95G [01:02<12:54, 7.82MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  39% 3.90G/9.95G [01:03<12:45, 7.90MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  39% 3.91G/9.95G [01:05<14:14, 7.07MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  39% 3.92G/9.95G [01:06<14:16, 7.04MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  40% 3.93G/9.95G [01:08<14:40, 6.84MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  40% 3.94G/9.95G [01:09<13:58, 7.17MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  40% 3.95G/9.95G [01:11<13:56, 7.17MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  40% 3.96G/9.95G [01:12<13:51, 7.20MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  40% 3.97G/9.95G [01:13<13:21, 7.46MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  40% 3.98G/9.95G [01:15<13:29, 7.37MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  40% 4.00G/9.95G [01:16<13:31, 7.34MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  40% 4.01G/9.95G [01:18<13:05, 7.57MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  40% 4.02G/9.95G [01:19<12:44, 7.77MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  40% 4.03G/9.95G [01:20<12:27, 7.93MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  41% 4.04G/9.95G [01:21<12:13, 8.06MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  41% 4.05G/9.95G [01:22<11:31, 8.54MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  41% 4.06G/9.95G [01:24<10:58, 8.95MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  41% 4.07G/9.95G [01:24<10:07, 9.68MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  41% 4.08G/9.95G [01:25<09:47, 10.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  41% 4.09G/9.95G [01:26<08:46, 11.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  41% 4.10G/9.95G [01:27<08:26, 11.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  41% 4.11G/9.95G [01:28<07:41, 12.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  41% 4.12G/9.95G [01:28<07:07, 13.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  42% 4.13G/9.95G [01:29<06:46, 14.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  42% 4.14G/9.95G [01:29<06:28, 14.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  42% 4.15G/9.95G [01:30<06:16, 15.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  42% 4.16G/9.95G [01:30<05:36, 17.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  42% 4.17G/9.95G [01:31<05:38, 17.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  42% 4.18G/9.95G [01:32<05:09, 18.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  42% 4.19G/9.95G [01:32<05:20, 18.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  42% 4.20G/9.95G [01:33<05:29, 17.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  42% 4.22G/9.95G [01:33<05:02, 19.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  42% 4.23G/9.95G [01:34<05:15, 18.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  43% 4.24G/9.95G [01:34<04:51, 19.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  43% 4.25G/9.95G [01:35<05:09, 18.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  43% 4.26G/9.95G [01:36<05:17, 17.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  43% 4.27G/9.95G [01:36<04:55, 19.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  43% 4.28G/9.95G [01:37<05:07, 18.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  43% 4.29G/9.95G [01:37<04:47, 19.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  43% 4.30G/9.95G [01:38<05:03, 18.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  43% 4.31G/9.95G [01:38<05:12, 18.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  43% 4.32G/9.95G [01:39<04:51, 19.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  44% 4.33G/9.95G [01:39<05:05, 18.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  44% 4.34G/9.95G [01:40<04:44, 19.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  44% 4.35G/9.95G [01:41<04:58, 18.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  44% 4.36G/9.95G [01:41<05:10, 18.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  44% 4.37G/9.95G [01:42<04:47, 19.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  44% 4.38G/9.95G [01:42<05:02, 18.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  44% 4.39G/9.95G [01:43<05:12, 17.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  44% 4.40G/9.95G [01:43<04:49, 19.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  44% 4.41G/9.95G [01:44<05:01, 18.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  44% 4.42G/9.95G [01:45<05:43, 16.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  45% 4.44G/9.95G [01:45<05:42, 16.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  45% 4.45G/9.95G [01:46<05:38, 16.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  45% 4.46G/9.95G [01:47<05:35, 16.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  45% 4.47G/9.95G [01:47<05:33, 16.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  45% 4.48G/9.95G [01:48<05:32, 16.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  45% 4.49G/9.95G [01:48<05:02, 18.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  45% 4.50G/9.95G [01:49<05:08, 17.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  45% 4.51G/9.95G [01:50<05:16, 17.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  45% 4.52G/9.95G [01:50<04:49, 18.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  46% 4.53G/9.95G [01:51<05:30, 16.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  46% 4.54G/9.95G [01:52<05:32, 16.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  46% 4.55G/9.95G [01:52<05:58, 15.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  46% 4.56G/9.95G [01:53<05:51, 15.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  46% 4.57G/9.95G [01:54<05:44, 15.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  46% 4.58G/9.95G [01:54<05:44, 15.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  46% 4.59G/9.95G [01:55<05:59, 14.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  46% 4.60G/9.95G [01:56<05:48, 15.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  46% 4.61G/9.95G [01:56<05:40, 15.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  46% 4.62G/9.95G [01:57<05:34, 15.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  47% 4.63G/9.95G [01:58<05:31, 16.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  47% 4.65G/9.95G [01:59<06:00, 14.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  47% 4.66G/9.95G [01:59<06:19, 13.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  47% 4.67G/9.95G [02:01<07:05, 12.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  47% 4.68G/9.95G [02:02<08:07, 10.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  47% 4.69G/9.95G [02:03<08:55, 9.84MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  47% 4.70G/9.95G [02:05<09:50, 8.90MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  47% 4.71G/9.95G [02:06<10:02, 8.70MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  47% 4.72G/9.95G [02:07<10:39, 8.18MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  48% 4.73G/9.95G [02:08<10:35, 8.22MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  48% 4.74G/9.95G [02:10<10:31, 8.25MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  48% 4.75G/9.95G [02:11<10:29, 8.27MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  48% 4.76G/9.95G [02:12<10:26, 8.28MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  48% 4.77G/9.95G [02:14<10:25, 8.28MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  48% 4.78G/9.95G [02:15<10:21, 8.31MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  48% 4.79G/9.95G [02:16<10:20, 8.32MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  48% 4.80G/9.95G [02:17<09:48, 8.74MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  48% 4.81G/9.95G [02:18<09:27, 9.06MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  48% 4.82G/9.95G [02:19<09:08, 9.35MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  49% 4.83G/9.95G [02:20<08:58, 9.51MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  49% 4.84G/9.95G [02:21<08:51, 9.61MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  49% 4.85G/9.95G [02:23<09:11, 9.24MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  49% 4.87G/9.95G [02:24<08:56, 9.47MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  49% 4.88G/9.95G [02:24<08:19, 10.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  49% 4.89G/9.95G [02:26<08:20, 10.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  49% 4.90G/9.95G [02:26<07:51, 10.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  49% 4.91G/9.95G [02:27<08:00, 10.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  49% 4.92G/9.95G [02:28<07:37, 11.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  50% 4.93G/9.95G [02:29<07:20, 11.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  50% 4.94G/9.95G [02:30<07:36, 11.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  50% 4.95G/9.95G [02:31<07:19, 11.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  50% 4.96G/9.95G [02:32<07:07, 11.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  50% 4.97G/9.95G [02:33<07:24, 11.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  50% 4.98G/9.95G [02:34<07:12, 11.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  50% 4.99G/9.95G [02:35<07:01, 11.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  50% 5.00G/9.95G [02:35<06:58, 11.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  50% 5.01G/9.95G [02:36<07:16, 11.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  50% 5.02G/9.95G [02:37<07:03, 11.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  51% 5.03G/9.95G [02:38<06:53, 11.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  51% 5.04G/9.95G [02:39<06:46, 12.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  51% 5.05G/9.95G [02:40<06:36, 12.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  51% 5.06G/9.95G [02:40<06:12, 13.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  51% 5.08G/9.95G [02:41<06:16, 13.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  51% 5.09G/9.95G [02:42<05:51, 13.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  51% 5.10G/9.95G [02:43<06:01, 13.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  51% 5.11G/9.95G [02:43<05:40, 14.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  51% 5.12G/9.95G [02:44<05:25, 14.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  52% 5.13G/9.95G [02:45<05:14, 15.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  52% 5.14G/9.95G [02:45<05:06, 15.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  52% 5.15G/9.95G [02:46<04:35, 17.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  52% 5.16G/9.95G [02:46<04:37, 17.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  52% 5.17G/9.95G [02:47<04:15, 18.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  52% 5.18G/9.95G [02:47<04:26, 17.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  52% 5.19G/9.95G [02:48<04:29, 17.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  52% 5.20G/9.95G [02:49<04:11, 18.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  52% 5.21G/9.95G [02:49<04:21, 18.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  52% 5.22G/9.95G [02:50<04:24, 17.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  53% 5.23G/9.95G [02:50<04:06, 19.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  53% 5.24G/9.95G [02:51<04:14, 18.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  53% 5.25G/9.95G [02:51<03:59, 19.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  53% 5.26G/9.95G [02:52<04:14, 18.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  53% 5.27G/9.95G [02:53<04:49, 16.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  53% 5.28G/9.95G [02:53<04:47, 16.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  53% 5.30G/9.95G [02:54<04:44, 16.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  53% 5.31G/9.95G [02:55<04:43, 16.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  53% 5.32G/9.95G [02:55<04:42, 16.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  54% 5.33G/9.95G [02:56<04:14, 18.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  54% 5.34G/9.95G [02:56<04:22, 17.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  54% 5.35G/9.95G [02:57<04:26, 17.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  54% 5.36G/9.95G [02:58<04:31, 16.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  54% 5.37G/9.95G [02:58<04:32, 16.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  54% 5.38G/9.95G [02:59<04:59, 15.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  54% 5.39G/9.95G [03:00<04:52, 15.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  54% 5.40G/9.95G [03:00<04:48, 15.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  54% 5.41G/9.95G [03:01<04:44, 15.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  54% 5.42G/9.95G [03:02<05:07, 14.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  55% 5.43G/9.95G [03:03<04:56, 15.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  55% 5.44G/9.95G [03:03<04:49, 15.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  55% 5.45G/9.95G [03:04<04:44, 15.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  55% 5.46G/9.95G [03:04<04:40, 16.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  55% 5.47G/9.95G [03:05<04:37, 16.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  55% 5.48G/9.95G [03:06<04:34, 16.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  55% 5.49G/9.95G [03:06<04:34, 16.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  55% 5.51G/9.95G [03:07<04:30, 16.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  55% 5.52G/9.95G [03:08<04:31, 16.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  56% 5.53G/9.95G [03:08<04:28, 16.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  56% 5.54G/9.95G [03:09<04:27, 16.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  56% 5.55G/9.95G [03:10<04:26, 16.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  56% 5.56G/9.95G [03:10<04:26, 16.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  56% 5.57G/9.95G [03:11<04:25, 16.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  56% 5.58G/9.95G [03:11<04:25, 16.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  56% 5.59G/9.95G [03:12<04:21, 16.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  56% 5.60G/9.95G [03:13<04:21, 16.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  56% 5.61G/9.95G [03:13<04:21, 16.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  56% 5.62G/9.95G [03:14<03:58, 18.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  57% 5.63G/9.95G [03:14<04:05, 17.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  57% 5.64G/9.95G [03:15<04:07, 17.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  57% 5.65G/9.95G [03:16<04:09, 17.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  57% 5.66G/9.95G [03:16<03:51, 18.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  57% 5.67G/9.95G [03:17<03:58, 17.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  57% 5.68G/9.95G [03:17<04:00, 17.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  57% 5.69G/9.95G [03:18<03:43, 19.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  57% 5.70G/9.95G [03:18<03:49, 18.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  57% 5.71G/9.95G [03:19<03:56, 17.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  58% 5.73G/9.95G [03:20<03:41, 19.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  58% 5.74G/9.95G [03:20<03:47, 18.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  58% 5.75G/9.95G [03:21<03:54, 17.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  58% 5.76G/9.95G [03:21<03:39, 19.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  58% 5.77G/9.95G [03:22<04:10, 16.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  58% 5.78G/9.95G [03:23<04:19, 16.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  58% 5.79G/9.95G [03:23<04:32, 15.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  58% 5.80G/9.95G [03:24<04:25, 15.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  58% 5.81G/9.95G [03:25<04:20, 15.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  58% 5.82G/9.95G [03:25<04:16, 16.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  59% 5.83G/9.95G [03:26<04:14, 16.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  59% 5.84G/9.95G [03:27<04:12, 16.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  59% 5.85G/9.95G [03:27<03:47, 18.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  59% 5.86G/9.95G [03:28<03:52, 17.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  59% 5.87G/9.95G [03:28<03:56, 17.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  59% 5.88G/9.95G [03:29<03:58, 17.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  59% 5.89G/9.95G [03:29<03:37, 18.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  59% 5.90G/9.95G [03:30<03:45, 18.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  59% 5.91G/9.95G [03:31<03:50, 17.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  60% 5.92G/9.95G [03:31<03:53, 17.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  60% 5.93G/9.95G [03:32<03:33, 18.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  60% 5.95G/9.95G [03:32<03:42, 18.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  60% 5.96G/9.95G [03:33<03:47, 17.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  60% 5.97G/9.95G [03:34<03:50, 17.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  60% 5.98G/9.95G [03:34<03:31, 18.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  60% 5.99G/9.95G [03:35<03:38, 18.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  60% 6.00G/9.95G [03:35<03:24, 19.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  60% 6.01G/9.95G [03:36<03:34, 18.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  60% 6.02G/9.95G [03:36<03:40, 17.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  61% 6.03G/9.95G [03:37<03:44, 17.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  61% 6.04G/9.95G [03:38<03:26, 19.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  61% 6.05G/9.95G [03:38<03:34, 18.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  61% 6.06G/9.95G [03:39<03:39, 17.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  61% 6.07G/9.95G [03:39<03:21, 19.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  61% 6.08G/9.95G [03:40<03:30, 18.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  61% 6.09G/9.95G [03:40<03:37, 17.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  61% 6.10G/9.95G [03:41<03:42, 17.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  61% 6.11G/9.95G [03:42<04:08, 15.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  62% 6.12G/9.95G [03:43<04:47, 13.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  62% 6.13G/9.95G [03:44<04:53, 13.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  62% 6.14G/9.95G [03:45<05:17, 12.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  62% 6.16G/9.95G [03:46<05:12, 12.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  62% 6.17G/9.95G [03:47<05:31, 11.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  62% 6.18G/9.95G [03:48<05:47, 10.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  62% 6.19G/9.95G [03:49<06:14, 10.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  62% 6.20G/9.95G [03:50<06:14, 10.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  62% 6.21G/9.95G [03:51<06:15, 9.96MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  62% 6.22G/9.95G [03:52<06:15, 9.95MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  63% 6.23G/9.95G [03:54<06:57, 8.92MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  63% 6.24G/9.95G [03:55<07:33, 8.19MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  63% 6.25G/9.95G [03:57<08:08, 7.58MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  63% 6.26G/9.95G [03:59<08:37, 7.13MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  63% 6.27G/9.95G [04:00<08:36, 7.13MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  63% 6.28G/9.95G [04:02<08:41, 7.04MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  63% 6.29G/9.95G [04:03<08:52, 6.87MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  63% 6.30G/9.95G [04:05<09:28, 6.42MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  63% 6.31G/9.95G [04:07<09:38, 6.29MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  64% 6.32G/9.95G [04:08<09:31, 6.35MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  64% 6.33G/9.95G [04:10<09:16, 6.50MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  64% 6.34G/9.95G [04:12<09:15, 6.50MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  64% 6.35G/9.95G [04:13<08:59, 6.67MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  64% 6.36G/9.95G [04:14<08:47, 6.80MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  64% 6.38G/9.95G [04:16<08:43, 6.83MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  64% 6.39G/9.95G [04:18<08:39, 6.87MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  64% 6.40G/9.95G [04:19<08:22, 7.07MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  64% 6.41G/9.95G [04:20<08:18, 7.11MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  64% 6.42G/9.95G [04:21<07:39, 7.70MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  65% 6.43G/9.95G [04:23<07:22, 7.97MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  65% 6.44G/9.95G [04:24<06:53, 8.50MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  65% 6.45G/9.95G [04:25<06:13, 9.37MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  65% 6.46G/9.95G [04:25<05:44, 10.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  65% 6.47G/9.95G [04:26<05:23, 10.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  65% 6.48G/9.95G [04:27<04:49, 12.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  65% 6.49G/9.95G [04:28<04:43, 12.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  65% 6.50G/9.95G [04:28<04:20, 13.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  65% 6.51G/9.95G [04:29<03:45, 15.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  66% 6.52G/9.95G [04:29<03:39, 15.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  66% 6.53G/9.95G [04:30<03:34, 16.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  66% 6.54G/9.95G [04:30<03:12, 17.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  66% 6.55G/9.95G [04:31<03:14, 17.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  66% 6.56G/9.95G [04:32<03:17, 17.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  66% 6.57G/9.95G [04:32<03:00, 18.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  66% 6.59G/9.95G [04:33<03:05, 18.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  66% 6.60G/9.95G [04:33<02:52, 19.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  66% 6.61G/9.95G [04:34<03:00, 18.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  66% 6.62G/9.95G [04:34<03:05, 18.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  67% 6.63G/9.95G [04:35<02:54, 19.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  67% 6.64G/9.95G [04:36<02:58, 18.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  67% 6.65G/9.95G [04:36<03:23, 16.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  67% 6.66G/9.95G [04:37<03:21, 16.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  67% 6.67G/9.95G [04:37<03:01, 18.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  67% 6.68G/9.95G [04:38<03:05, 17.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  67% 6.69G/9.95G [04:39<03:08, 17.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  67% 6.70G/9.95G [04:39<02:51, 18.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  67% 6.71G/9.95G [04:40<02:59, 18.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  68% 6.72G/9.95G [04:40<03:02, 17.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  68% 6.73G/9.95G [04:41<03:25, 15.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  68% 6.74G/9.95G [04:42<03:21, 15.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  68% 6.75G/9.95G [04:43<03:18, 16.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  68% 6.76G/9.95G [04:43<03:16, 16.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  68% 6.77G/9.95G [04:44<03:14, 16.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  68% 6.78G/9.95G [04:44<03:13, 16.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  68% 6.79G/9.95G [04:45<02:56, 17.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  68% 6.81G/9.95G [04:46<03:18, 15.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  68% 6.82G/9.95G [04:47<03:33, 14.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  69% 6.83G/9.95G [04:47<03:25, 15.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  69% 6.84G/9.95G [04:48<03:37, 14.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  69% 6.85G/9.95G [04:49<03:28, 14.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  69% 6.86G/9.95G [04:49<03:21, 15.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  69% 6.87G/9.95G [04:50<03:34, 14.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  69% 6.88G/9.95G [04:51<03:25, 14.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  69% 6.89G/9.95G [04:51<03:22, 15.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  69% 6.90G/9.95G [04:52<03:30, 14.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  69% 6.91G/9.95G [04:53<03:22, 15.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  70% 6.92G/9.95G [04:54<03:16, 15.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  70% 6.93G/9.95G [04:54<03:13, 15.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  70% 6.94G/9.95G [04:55<03:09, 15.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  70% 6.95G/9.95G [04:55<03:06, 16.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  70% 6.96G/9.95G [04:56<03:21, 14.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  70% 6.97G/9.95G [04:57<03:15, 15.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  70% 6.98G/9.95G [04:58<03:28, 14.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  70% 6.99G/9.95G [04:59<03:41, 13.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  70% 7.00G/9.95G [04:59<03:42, 13.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  70% 7.01G/9.95G [05:00<03:45, 13.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  71% 7.03G/9.95G [05:01<03:31, 13.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  71% 7.04G/9.95G [05:02<03:37, 13.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  71% 7.05G/9.95G [05:02<03:24, 14.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  71% 7.06G/9.95G [05:03<03:15, 14.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  71% 7.07G/9.95G [05:04<03:25, 14.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  71% 7.08G/9.95G [05:05<03:15, 14.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  71% 7.09G/9.95G [05:05<03:08, 15.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  71% 7.10G/9.95G [05:06<03:03, 15.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  71% 7.11G/9.95G [05:06<02:59, 15.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  72% 7.12G/9.95G [05:07<03:12, 14.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  72% 7.13G/9.95G [05:08<03:21, 14.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  72% 7.14G/9.95G [05:09<03:27, 13.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  72% 7.15G/9.95G [05:10<03:31, 13.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  72% 7.16G/9.95G [05:11<03:34, 13.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  72% 7.17G/9.95G [05:11<03:35, 12.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  72% 7.18G/9.95G [05:12<03:35, 12.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  72% 7.19G/9.95G [05:13<03:24, 13.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  72% 7.20G/9.95G [05:14<03:24, 13.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  72% 7.21G/9.95G [05:15<03:27, 13.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  73% 7.22G/9.95G [05:15<03:28, 13.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  73% 7.24G/9.95G [05:16<03:18, 13.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  73% 7.25G/9.95G [05:17<03:20, 13.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  73% 7.26G/9.95G [05:18<03:22, 13.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  73% 7.27G/9.95G [05:18<03:13, 13.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  73% 7.28G/9.95G [05:19<03:15, 13.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  73% 7.29G/9.95G [05:20<03:18, 13.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  73% 7.30G/9.95G [05:21<03:11, 13.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  73% 7.31G/9.95G [05:21<03:12, 13.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  74% 7.32G/9.95G [05:22<03:15, 13.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  74% 7.33G/9.95G [05:23<03:06, 14.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  74% 7.34G/9.95G [05:24<03:10, 13.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  74% 7.35G/9.95G [05:24<03:01, 14.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  74% 7.36G/9.95G [05:25<03:04, 14.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  74% 7.37G/9.95G [05:26<02:58, 14.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  74% 7.38G/9.95G [05:27<03:03, 14.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  74% 7.39G/9.95G [05:27<02:54, 14.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  74% 7.40G/9.95G [05:28<02:52, 14.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  74% 7.41G/9.95G [05:29<02:46, 15.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  75% 7.42G/9.95G [05:29<02:41, 15.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  75% 7.43G/9.95G [05:30<02:37, 15.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  75% 7.44G/9.95G [05:30<02:21, 17.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  75% 7.46G/9.95G [05:31<02:26, 17.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  75% 7.47G/9.95G [05:31<02:09, 19.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  75% 7.48G/9.95G [05:32<02:15, 18.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  75% 7.49G/9.95G [05:33<02:18, 17.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  75% 7.50G/9.95G [05:33<02:21, 17.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  75% 7.51G/9.95G [05:34<02:08, 19.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  76% 7.52G/9.95G [05:34<02:13, 18.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  76% 7.53G/9.95G [05:35<02:16, 17.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  76% 7.54G/9.95G [05:35<02:04, 19.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  76% 7.55G/9.95G [05:36<02:10, 18.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  76% 7.56G/9.95G [05:37<02:42, 14.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  76% 7.57G/9.95G [05:38<02:49, 14.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  76% 7.58G/9.95G [05:39<02:54, 13.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  76% 7.59G/9.95G [05:39<02:44, 14.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  76% 7.60G/9.95G [05:40<02:50, 13.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  77% 7.61G/9.95G [05:41<02:53, 13.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  77% 7.62G/9.95G [05:42<02:43, 14.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  77% 7.63G/9.95G [05:42<02:48, 13.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  77% 7.64G/9.95G [05:43<02:40, 14.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  77% 7.65G/9.95G [05:44<02:59, 12.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  77% 7.67G/9.95G [05:45<03:11, 12.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  77% 7.68G/9.95G [05:46<03:09, 12.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  77% 7.69G/9.95G [05:47<03:06, 12.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  77% 7.70G/9.95G [05:48<03:14, 11.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  77% 7.71G/9.95G [05:49<03:10, 11.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  78% 7.72G/9.95G [05:50<03:06, 12.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  78% 7.73G/9.95G [05:51<03:15, 11.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  78% 7.74G/9.95G [05:52<03:22, 10.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  78% 7.75G/9.95G [05:53<03:37, 10.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  78% 7.76G/9.95G [05:54<03:38, 10.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  78% 7.77G/9.95G [05:55<03:37, 10.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  78% 7.78G/9.95G [05:56<03:34, 10.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  78% 7.79G/9.95G [05:57<03:28, 10.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  78% 7.80G/9.95G [05:58<03:23, 10.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  79% 7.81G/9.95G [05:59<03:25, 10.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  79% 7.82G/9.95G [06:00<03:38, 9.75MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  79% 7.83G/9.95G [06:01<03:48, 9.28MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  79% 7.84G/9.95G [06:03<03:52, 9.08MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  79% 7.85G/9.95G [06:04<03:46, 9.27MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  79% 7.86G/9.95G [06:05<03:39, 9.50MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  79% 7.87G/9.95G [06:06<03:34, 9.66MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  79% 7.89G/9.95G [06:07<03:30, 9.80MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  79% 7.90G/9.95G [06:08<03:27, 9.89MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  79% 7.91G/9.95G [06:09<03:25, 9.96MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  80% 7.92G/9.95G [06:10<03:22, 10.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  80% 7.93G/9.95G [06:11<03:11, 10.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  80% 7.94G/9.95G [06:12<03:12, 10.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  80% 7.95G/9.95G [06:13<03:13, 10.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  80% 7.96G/9.95G [06:14<03:13, 10.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  80% 7.97G/9.95G [06:15<03:13, 10.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  80% 7.98G/9.95G [06:16<03:02, 10.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  80% 7.99G/9.95G [06:17<03:04, 10.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  80% 8.00G/9.95G [06:18<02:55, 11.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  81% 8.01G/9.95G [06:19<02:57, 10.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  81% 8.02G/9.95G [06:20<02:50, 11.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  81% 8.03G/9.95G [06:20<02:44, 11.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  81% 8.04G/9.95G [06:21<02:30, 12.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  81% 8.05G/9.95G [06:22<02:29, 12.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  81% 8.06G/9.95G [06:22<02:18, 13.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  81% 8.07G/9.95G [06:23<02:19, 13.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  81% 8.08G/9.95G [06:24<02:11, 14.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  81% 8.10G/9.95G [06:25<02:04, 14.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  81% 8.11G/9.95G [06:25<01:49, 16.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  82% 8.12G/9.95G [06:26<01:49, 16.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  82% 8.13G/9.95G [06:26<01:48, 16.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  82% 8.14G/9.95G [06:27<01:38, 18.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  82% 8.15G/9.95G [06:27<01:51, 16.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  82% 8.16G/9.95G [06:28<01:49, 16.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  82% 8.17G/9.95G [06:29<01:48, 16.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  82% 8.18G/9.95G [06:29<01:47, 16.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  82% 8.19G/9.95G [06:30<01:46, 16.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  82% 8.20G/9.95G [06:31<01:56, 15.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  83% 8.21G/9.95G [06:32<02:02, 14.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  83% 8.22G/9.95G [06:33<02:05, 13.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  83% 8.23G/9.95G [06:33<01:59, 14.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  83% 8.24G/9.95G [06:34<01:53, 15.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  83% 8.25G/9.95G [06:35<01:59, 14.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  83% 8.26G/9.95G [06:35<01:53, 14.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  83% 8.27G/9.95G [06:36<01:57, 14.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  83% 8.28G/9.95G [06:37<02:12, 12.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  83% 8.29G/9.95G [06:38<02:21, 11.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  83% 8.30G/9.95G [06:39<02:35, 10.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  84% 8.32G/9.95G [06:41<02:46, 9.82MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  84% 8.33G/9.95G [06:42<03:05, 8.78MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  84% 8.34G/9.95G [06:44<03:15, 8.25MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  84% 8.35G/9.95G [06:45<03:21, 7.97MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  84% 8.36G/9.95G [06:46<03:18, 8.03MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  84% 8.37G/9.95G [06:48<03:16, 8.05MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  84% 8.38G/9.95G [06:49<03:18, 7.90MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  84% 8.39G/9.95G [06:50<03:15, 7.99MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  84% 8.40G/9.95G [06:52<03:18, 7.80MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  85% 8.41G/9.95G [06:53<03:14, 7.92MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  85% 8.42G/9.95G [06:54<03:10, 8.04MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  85% 8.43G/9.95G [06:55<03:06, 8.16MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  85% 8.44G/9.95G [06:57<03:01, 8.30MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  85% 8.45G/9.95G [06:58<02:51, 8.75MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  85% 8.46G/9.95G [06:59<02:43, 9.13MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  85% 8.47G/9.95G [07:00<02:30, 9.82MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  85% 8.48G/9.95G [07:00<02:21, 10.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  85% 8.49G/9.95G [07:01<02:11, 11.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  85% 8.50G/9.95G [07:02<02:04, 11.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  86% 8.51G/9.95G [07:03<01:54, 12.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  86% 8.52G/9.95G [07:03<01:45, 13.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  86% 8.54G/9.95G [07:04<01:44, 13.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  86% 8.55G/9.95G [07:05<01:31, 15.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  86% 8.56G/9.95G [07:05<01:26, 16.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  86% 8.57G/9.95G [07:06<01:25, 16.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  86% 8.58G/9.95G [07:06<01:23, 16.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  86% 8.59G/9.95G [07:07<01:15, 18.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  86% 8.60G/9.95G [07:07<01:16, 17.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  87% 8.61G/9.95G [07:08<01:25, 15.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  87% 8.62G/9.95G [07:09<01:33, 14.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  87% 8.63G/9.95G [07:10<01:41, 13.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  87% 8.64G/9.95G [07:11<01:44, 12.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  87% 8.65G/9.95G [07:12<01:43, 12.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  87% 8.66G/9.95G [07:13<01:42, 12.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  87% 8.67G/9.95G [07:14<01:41, 12.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  87% 8.68G/9.95G [07:14<01:40, 12.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  87% 8.69G/9.95G [07:15<01:39, 12.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  87% 8.70G/9.95G [07:16<01:38, 12.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  88% 8.71G/9.95G [07:17<01:38, 12.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  88% 8.72G/9.95G [07:18<01:34, 12.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  88% 8.73G/9.95G [07:18<01:30, 13.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  88% 8.75G/9.95G [07:19<01:31, 13.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  88% 8.76G/9.95G [07:20<01:39, 12.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  88% 8.77G/9.95G [07:21<01:44, 11.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  88% 8.78G/9.95G [07:22<01:47, 11.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  88% 8.79G/9.95G [07:23<01:43, 11.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  88% 8.80G/9.95G [07:24<01:37, 11.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  89% 8.81G/9.95G [07:25<01:38, 11.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  89% 8.82G/9.95G [07:26<01:35, 11.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  89% 8.83G/9.95G [07:27<01:32, 12.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  89% 8.84G/9.95G [07:27<01:30, 12.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  89% 8.85G/9.95G [07:28<01:26, 12.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  89% 8.86G/9.95G [07:29<01:26, 12.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  89% 8.87G/9.95G [07:30<01:25, 12.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  89% 8.88G/9.95G [07:31<01:24, 12.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  89% 8.89G/9.95G [07:32<01:23, 12.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  89% 8.90G/9.95G [07:32<01:22, 12.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  90% 8.91G/9.95G [07:33<01:21, 12.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  90% 8.92G/9.95G [07:34<01:18, 13.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  90% 8.93G/9.95G [07:35<01:15, 13.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  90% 8.94G/9.95G [07:35<01:16, 13.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  90% 8.95G/9.95G [07:36<01:16, 13.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  90% 8.97G/9.95G [07:37<01:16, 12.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  90% 8.98G/9.95G [07:38<01:12, 13.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  90% 8.99G/9.95G [07:39<01:11, 13.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  90% 9.00G/9.95G [07:39<01:11, 13.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  91% 9.01G/9.95G [07:40<01:07, 14.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  91% 9.02G/9.95G [07:41<01:03, 14.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  91% 9.03G/9.95G [07:41<01:04, 14.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  91% 9.04G/9.95G [07:42<01:02, 14.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  91% 9.05G/9.95G [07:43<00:59, 15.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  91% 9.06G/9.95G [07:43<00:57, 15.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  91% 9.07G/9.95G [07:44<00:55, 15.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  91% 9.08G/9.95G [07:44<00:49, 17.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  91% 9.09G/9.95G [07:45<00:49, 17.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  91% 9.10G/9.95G [07:46<00:49, 17.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  92% 9.11G/9.95G [07:46<00:44, 18.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  92% 9.12G/9.95G [07:47<00:45, 18.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  92% 9.13G/9.95G [07:47<00:42, 19.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  92% 9.14G/9.95G [07:48<00:43, 18.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  92% 9.15G/9.95G [07:49<00:44, 18.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  92% 9.16G/9.95G [07:49<00:40, 19.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  92% 9.18G/9.95G [07:50<00:41, 18.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  92% 9.19G/9.95G [07:50<00:38, 19.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  92% 9.20G/9.95G [07:51<00:39, 18.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  93% 9.21G/9.95G [07:51<00:40, 18.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  93% 9.22G/9.95G [07:52<00:37, 19.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  93% 9.23G/9.95G [07:52<00:38, 18.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  93% 9.24G/9.95G [07:53<00:35, 20.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  93% 9.25G/9.95G [07:53<00:36, 19.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  93% 9.26G/9.95G [07:54<00:37, 18.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  93% 9.27G/9.95G [07:55<00:38, 17.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  93% 9.28G/9.95G [07:55<00:38, 17.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  93% 9.29G/9.95G [07:56<00:34, 18.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  93% 9.30G/9.95G [07:56<00:35, 18.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  94% 9.31G/9.95G [07:57<00:39, 16.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  94% 9.32G/9.95G [07:58<00:38, 16.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  94% 9.33G/9.95G [07:58<00:38, 16.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  94% 9.34G/9.95G [07:59<00:37, 16.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  94% 9.35G/9.95G [08:00<00:36, 16.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  94% 9.36G/9.95G [08:00<00:35, 16.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  94% 9.37G/9.95G [08:01<00:34, 16.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  94% 9.38G/9.95G [08:02<00:34, 16.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  94% 9.40G/9.95G [08:02<00:33, 16.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  95% 9.41G/9.95G [08:03<00:32, 16.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  95% 9.42G/9.95G [08:03<00:32, 16.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  95% 9.43G/9.95G [08:04<00:31, 16.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  95% 9.44G/9.95G [08:05<00:30, 16.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  95% 9.45G/9.95G [08:05<00:27, 18.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  95% 9.46G/9.95G [08:06<00:27, 17.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  95% 9.47G/9.95G [08:07<00:33, 14.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  95% 9.48G/9.95G [08:08<00:31, 14.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  95% 9.49G/9.95G [08:08<00:32, 14.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  95% 9.50G/9.95G [08:09<00:30, 14.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  96% 9.51G/9.95G [08:10<00:30, 14.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  96% 9.52G/9.95G [08:10<00:28, 14.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  96% 9.53G/9.95G [08:11<00:27, 15.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  96% 9.54G/9.95G [08:12<00:25, 15.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  96% 9.55G/9.95G [08:12<00:24, 16.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  96% 9.56G/9.95G [08:13<00:23, 16.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  96% 9.57G/9.95G [08:14<00:23, 16.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  96% 9.58G/9.95G [08:14<00:22, 16.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  96% 9.59G/9.95G [08:15<00:21, 16.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  97% 9.60G/9.95G [08:15<00:20, 16.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  97% 9.62G/9.95G [08:16<00:20, 16.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  97% 9.63G/9.95G [08:16<00:17, 18.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  97% 9.64G/9.95G [08:17<00:17, 17.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  97% 9.65G/9.95G [08:18<00:17, 17.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  97% 9.66G/9.95G [08:18<00:17, 17.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  97% 9.67G/9.95G [08:19<00:16, 17.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  97% 9.68G/9.95G [08:20<00:16, 16.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  97% 9.69G/9.95G [08:20<00:14, 18.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  97% 9.70G/9.95G [08:21<00:13, 18.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  98% 9.71G/9.95G [08:21<00:13, 17.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  98% 9.72G/9.95G [08:22<00:13, 17.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  98% 9.73G/9.95G [08:23<00:12, 17.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  98% 9.74G/9.95G [08:23<00:12, 17.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  98% 9.75G/9.95G [08:24<00:10, 18.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  98% 9.76G/9.95G [08:24<00:10, 18.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  98% 9.77G/9.95G [08:25<00:10, 17.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  98% 9.78G/9.95G [08:26<00:09, 17.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  98% 9.79G/9.95G [08:26<00:10, 15.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  99% 9.80G/9.95G [08:27<00:10, 14.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  99% 9.81G/9.95G [08:28<00:09, 15.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  99% 9.83G/9.95G [08:28<00:08, 15.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  99% 9.84G/9.95G [08:29<00:07, 15.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  99% 9.85G/9.95G [08:30<00:06, 16.0MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  99% 9.86G/9.95G [08:30<00:05, 16.2MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  99% 9.87G/9.95G [08:31<00:05, 16.4MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  99% 9.88G/9.95G [08:32<00:04, 16.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  99% 9.89G/9.95G [08:32<00:03, 16.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin:  99% 9.90G/9.95G [08:33<00:03, 16.6MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin: 100% 9.91G/9.95G [08:33<00:02, 18.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin: 100% 9.92G/9.95G [08:34<00:01, 17.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin: 100% 9.93G/9.95G [08:35<00:01, 17.5MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin: 100% 9.94G/9.95G [08:35<00:00, 17.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00002.bin: 100% 9.95G/9.95G [08:36<00:00, 19.3MB/s]\n",
            "Fetching 5 files: 100% 5/5 [08:37<00:00, 103.53s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/pydantic/_migration.py:283: UserWarning: `pydantic.utils:Representation` has been removed. We are importing from `pydantic.v1.utils:Representation` instead.See the migration guide for more details: https://docs.pydantic.dev/latest/migration/\n",
            "  warnings.warn(\n",
            "Model config {'org': 'tiiuae', 'name': 'falcon-7b', 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 512, 'padded_vocab_size': 65024, 'n_layer': 32, 'n_head': 71, 'n_embd': 4544, 'rotary_percentage': 1.0, 'parallel_residual': True, 'bias': False, 'n_query_groups': 1, 'shared_attention_norm': True, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'intermediate_size': 18176, 'condense_ratio': 1}\n",
            "Processing checkpoints/tiiuae/falcon-7b/pytorch_model-00001-of-00002.bin\n",
            "Processing checkpoints/tiiuae/falcon-7b/pytorch_model-00002-of-00002.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "mT6EXckQutUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phase 2. Test Inference."
      ],
      "metadata": {
        "id": "aYPI3wGAMomD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run inference\n",
        "!python generate/base.py \\\n",
        "        --prompt \"Hello, my name is\" \\\n",
        "        --checkpoint_dir checkpoints/tiiuae/falcon-7b \\\n",
        "        --quantize bnb.int8"
      ],
      "metadata": {
        "id": "yDzAaqoUvpkc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a36301e8-d9e7-4296-ec4e-1aefec7751c7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_migration.py:283: UserWarning: `pydantic.utils:Representation` has been removed. We are importing from `pydantic.v1.utils:Representation` instead.See the migration guide for more details: https://docs.pydantic.dev/latest/migration/\n",
            "  warnings.warn(\n",
            "Loading model 'checkpoints/tiiuae/falcon-7b/lit_model.pth' with {'org': 'tiiuae', 'name': 'falcon-7b', 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 512, 'padded_vocab_size': 65024, 'n_layer': 32, 'n_head': 71, 'n_embd': 4544, 'rotary_percentage': 1.0, 'parallel_residual': True, 'bias': False, 'n_query_groups': 1, 'shared_attention_norm': True, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'intermediate_size': 18176, 'condense_ratio': 1}\n",
            "Time to instantiate model: 1.85 seconds.\n",
            "Time to load the model weights: 14.17 seconds.\n",
            "Global seed set to 1234\n",
            "Hello, my name is Jack.\n",
            "Some people think that dogs are like people. They see them as loyal, loving, intelligent animals. Some people, though, realize that dogs have their own language and their own culture. This is the culture of dog.\n",
            "Actually,\n",
            "Time for inference 1: 5.85 sec total, 8.55 tokens/sec\n",
            "Memory used: 8.71 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phase 3. Generate Synthetic Dataset\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E002Bul0OSlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#python scripts/prepare_alpaca.py --checkpoint_dir checkpoints/stabilityai/stablelm-base-alpha-3b\n",
        "!python scripts/prepare_alpaca.py --checkpoint_dir checkpoints/tiiuae/falcon-7b\n",
        "#!python scripts/prepare_alpaca.py"
      ],
      "metadata": {
        "id": "XAIlVUstOWMy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6b11f7f-7437-4b12-9dbd-9d66a54d0480"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_migration.py:283: UserWarning: `pydantic.utils:Representation` has been removed. We are importing from `pydantic.v1.utils:Representation` instead.See the migration guide for more details: https://docs.pydantic.dev/latest/migration/\n",
            "  warnings.warn(\n",
            "Loading data file...\n",
            "Loading tokenizer...\n",
            "train has 49,759 samples\n",
            "test has 2,000 samples\n",
            "Processing train split ...\n",
            "100% 49759/49759 [00:40<00:00, 1237.28it/s]\n",
            "Processing test split ...\n",
            "100% 2000/2000 [00:01<00:00, 1178.42it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 4. Finetuning Overview\n",
        "\n",
        "Choose between reproducing and searching new baselines\n",
        "* 4.A stable baseline reproduction for falcon-7b_50k_iters test (A100-40GB)\n",
        "* 4.B for training longer experiments on Lambda or ColabPro"
      ],
      "metadata": {
        "id": "SYlaO4InJB5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!python finetune/adapter_v2.py --precision bf16-true --checkpoint_dir checkpoints/tiiuae/falcon-7b\n",
        "!python finetune/adapter_v2.py --precision bf16-true --data_dir data/alpaca --checkpoint_dir checkpoints/tiiuae/falcon-7b --out_dir out/adapter/alpaca"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiyX5zS-DrW1",
        "outputId": "a02c5538-c5d1-4c0d-d435-bbaa28350891"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_migration.py:283: UserWarning: `pydantic.utils:Representation` has been removed. We are importing from `pydantic.v1.utils:Representation` instead.See the migration guide for more details: https://docs.pydantic.dev/latest/migration/\n",
            "  warnings.warn(\n",
            "{'eval_interval': 50, 'save_interval': 100, 'eval_iters': 100, 'log_interval': 100, 'devices': 1, 'learning_rate': 0.003, 'batch_size': 128.0, 'micro_batch_size': 2, 'gradient_accumulation_iters': 64.0, 'epoch_size': 50000, 'num_epochs': 5, 'max_iters': 125000, 'weight_decay': 0.02, 'warmup_steps': 781.0}\n",
            "Global seed set to 1337\n",
            "Loading model 'checkpoints/tiiuae/falcon-7b/lit_model.pth' with {'org': 'tiiuae', 'name': 'falcon-7b', 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 512, 'padded_vocab_size': 65024, 'n_layer': 32, 'n_head': 71, 'n_embd': 4544, 'rotary_percentage': 1.0, 'parallel_residual': True, 'bias': False, 'n_query_groups': 1, 'shared_attention_norm': True, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'intermediate_size': 18176, 'condense_ratio': 1, 'adapter_prompt_length': 10, 'adapter_start_layer': 2}\n",
            "Number of trainable parameters: 3,839,186\n",
            "Number of non trainable parameters: 7,216,889,856\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/fabric/fabric.py:930: PossibleUserWarning: The model passed to `Fabric.setup()` has parameters on different devices. Since `move_to_device=True`, all parameters will be moved to the new device. If this is not desired, set  `Fabric.setup(..., move_to_device=False)`.\n",
            "  rank_zero_warn(\n",
            "Global seed set to 1337\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:\n",
            "\n",
            "#### ![img](img/3.jpg)\n",
            "\n",
            "> I think you should watch \"The Martian\". This movie is very interesting, because you can imagine yourself in the hero's place.\n",
            "\n",
            "#### ![img](img/4.jpg)\n",
            "\n",
            "> The reason why I recommend this movie is that the hero's name is Mark Watney, which is very similar to your name. In addition, Mark is a lively person without giving up even in the most difficult situations\n",
            "Estimated TFLOPs: 192.04\n",
            "Measured TFLOPs: 184.84\n",
            "iter 0 step 0: loss 1.0731, iter time: 555.71ms\n",
            "iter 100 step 1: loss 2.7334, iter time: 116.77ms\n",
            "iter 200 step 3: loss 3.6321, iter time: 119.92ms\n",
            "iter 300 step 4: loss 2.6425, iter time: 116.89ms\n",
            "iter 400 step 6: loss 1.6970, iter time: 122.79ms\n",
            "iter 500 step 7: loss 2.0937, iter time: 117.84ms\n",
            "iter 600 step 9: loss 2.1776, iter time: 121.13ms\n",
            "iter 700 step 10: loss 2.4503, iter time: 122.61ms\n",
            "iter 800 step 12: loss 1.9371, iter time: 122.54ms\n",
            "iter 900 step 14: loss 2.4354, iter time: 118.41ms\n",
            "iter 1000 step 15: loss 2.3407, iter time: 116.89ms\n",
            "iter 1100 step 17: loss 1.5523, iter time: 120.15ms\n",
            "iter 1200 step 18: loss 2.3049, iter time: 118.00ms\n",
            "iter 1300 step 20: loss 1.5956, iter time: 122.00ms\n",
            "iter 1400 step 21: loss 2.6380, iter time: 121.47ms\n",
            "iter 1500 step 23: loss 1.3425, iter time: 129.79ms\n",
            "iter 1600 step 25: loss 1.8466, iter time: 116.45ms\n",
            "iter 1700 step 26: loss 1.7312, iter time: 122.58ms\n",
            "iter 1800 step 28: loss 1.2649, iter time: 123.41ms\n",
            "iter 1900 step 29: loss 1.6436, iter time: 130.96ms\n",
            "iter 2000 step 31: loss 1.3001, iter time: 117.98ms\n",
            "iter 2100 step 32: loss 1.7352, iter time: 118.72ms\n",
            "iter 2200 step 34: loss 1.0977, iter time: 121.73ms\n",
            "iter 2300 step 35: loss 1.3425, iter time: 122.93ms\n",
            "iter 2400 step 37: loss 1.4108, iter time: 126.19ms\n",
            "iter 2500 step 39: loss 1.1926, iter time: 118.48ms\n",
            "iter 2600 step 40: loss 1.0085, iter time: 117.36ms\n",
            "iter 2700 step 42: loss 1.6031, iter time: 119.71ms\n",
            "iter 2800 step 43: loss 1.7588, iter time: 118.79ms\n",
            "iter 2900 step 45: loss 1.3654, iter time: 122.99ms\n",
            "iter 3000 step 46: loss 1.0321, iter time: 120.63ms\n",
            "iter 3100 step 48: loss 1.0133, iter time: 122.01ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:\n",
            "I recommend watching The Matrix. I recommend this movie because it is entertaining and action packed. The plot is interesting and keeps you at the edge the entire time. There are lots of thrilling scenes that keep you on your toes. I highly recommend this movie to everyone.\n",
            "\n",
            "### Instruction:\n",
            "Tell me the steps to cook rice.\n",
            "\n",
            "### Response:\n",
            "To cook rice, pour 1 cup of water into a pot. Add 1 cup of brown rice. Let the pot with water and\n",
            "step 3199: val loss 12.8959, val time: 8284.88ms\n",
            "iter 3200 step 50: loss 0.9534, iter time: 119.26ms\n",
            "iter 3300 step 51: loss 0.9559, iter time: 135.85ms\n",
            "iter 3400 step 53: loss 1.3344, iter time: 119.62ms\n",
            "iter 3500 step 54: loss 0.5870, iter time: 118.42ms\n",
            "iter 3600 step 56: loss 1.3093, iter time: 119.10ms\n",
            "iter 3700 step 57: loss 0.5916, iter time: 124.96ms\n",
            "iter 3800 step 59: loss 0.8103, iter time: 120.64ms\n",
            "iter 3900 step 60: loss 1.0226, iter time: 118.54ms\n",
            "iter 4000 step 62: loss 1.0089, iter time: 120.02ms\n",
            "iter 4100 step 64: loss 0.6310, iter time: 119.45ms\n",
            "iter 4200 step 65: loss 1.2127, iter time: 122.54ms\n",
            "iter 4300 step 67: loss 0.9843, iter time: 119.47ms\n",
            "iter 4400 step 68: loss 0.7124, iter time: 119.90ms\n",
            "iter 4500 step 70: loss 1.2162, iter time: 125.00ms\n",
            "iter 4600 step 71: loss 0.9895, iter time: 120.60ms\n",
            "iter 4700 step 73: loss 1.0914, iter time: 122.79ms\n",
            "iter 4800 step 75: loss 0.7777, iter time: 120.76ms\n",
            "iter 4900 step 76: loss 1.0330, iter time: 124.38ms\n",
            "iter 5000 step 78: loss 1.5350, iter time: 127.21ms\n",
            "iter 5100 step 79: loss 0.9674, iter time: 119.21ms\n",
            "iter 5200 step 81: loss 1.2705, iter time: 126.48ms\n",
            "iter 5300 step 82: loss 1.0270, iter time: 121.19ms\n",
            "iter 5400 step 84: loss 1.0151, iter time: 119.64ms\n",
            "iter 5500 step 85: loss 0.8498, iter time: 121.61ms\n",
            "iter 5600 step 87: loss 1.0012, iter time: 122.47ms\n",
            "iter 5700 step 89: loss 0.6849, iter time: 123.57ms\n",
            "iter 5800 step 90: loss 0.7445, iter time: 135.76ms\n",
            "iter 5900 step 92: loss 0.8395, iter time: 119.46ms\n",
            "iter 6000 step 93: loss 0.8834, iter time: 119.67ms\n",
            "iter 6100 step 95: loss 0.9956, iter time: 122.57ms\n",
            "iter 6200 step 96: loss 1.6341, iter time: 123.94ms\n",
            "iter 6300 step 98: loss 0.7280, iter time: 121.26ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:I recommend watching the movie 'Blade Runner 2049' starring Ryan Gosling, Harrison Ford, and Ana de Armas. This is an action-packed movie that explores an apocalyptic future where replicant humans are hunted by a special task force called Blade Runners.Starting the movie off with a scene in which a Blade Runner pursues a group of aggressive replicant humans, this movie is an action-packed thrill ride that explores an apocalyptic future where replic\n",
            "step 6399: val loss 14.0491, val time: 8355.51ms\n",
            "Saving adapter v2 weights to 'out/adapter/alpaca/iter-006399-ckpt.pth'\n",
            "iter 6400 step 100: loss 0.7794, iter time: 129.55ms\n",
            "iter 6500 step 101: loss 1.0179, iter time: 133.19ms\n",
            "iter 6600 step 103: loss 0.6164, iter time: 129.48ms\n",
            "iter 6700 step 104: loss 0.7358, iter time: 123.05ms\n",
            "iter 6800 step 106: loss 0.8876, iter time: 121.61ms\n",
            "iter 6900 step 107: loss 0.7798, iter time: 123.09ms\n",
            "iter 7000 step 109: loss 0.6934, iter time: 135.46ms\n",
            "iter 7100 step 110: loss 0.8269, iter time: 120.34ms\n",
            "iter 7200 step 112: loss 1.4049, iter time: 119.90ms\n",
            "iter 7300 step 114: loss 0.8518, iter time: 121.28ms\n",
            "iter 7400 step 115: loss 0.8892, iter time: 123.31ms\n",
            "iter 7500 step 117: loss 0.7501, iter time: 125.99ms\n",
            "iter 7600 step 118: loss 0.7445, iter time: 123.04ms\n",
            "iter 7700 step 120: loss 0.5264, iter time: 121.86ms\n",
            "iter 7800 step 121: loss 0.8228, iter time: 124.16ms\n",
            "iter 7900 step 123: loss 0.8014, iter time: 139.24ms\n",
            "iter 8000 step 125: loss 0.7759, iter time: 119.42ms\n",
            "iter 8100 step 126: loss 0.6858, iter time: 123.39ms\n",
            "iter 8200 step 128: loss 1.4111, iter time: 124.08ms\n",
            "iter 8300 step 129: loss 0.8375, iter time: 120.86ms\n",
            "iter 8400 step 131: loss 1.0031, iter time: 122.56ms\n",
            "iter 8500 step 132: loss 0.8088, iter time: 122.92ms\n",
            "iter 8600 step 134: loss 0.6674, iter time: 133.03ms\n",
            "iter 8700 step 135: loss 0.7819, iter time: 120.27ms\n",
            "iter 8800 step 137: loss 1.1295, iter time: 121.32ms\n",
            "iter 8900 step 139: loss 0.7561, iter time: 121.86ms\n",
            "iter 9000 step 140: loss 0.7845, iter time: 122.20ms\n",
            "iter 9100 step 142: loss 0.8688, iter time: 123.43ms\n",
            "iter 9200 step 143: loss 0.8874, iter time: 125.28ms\n",
            "iter 9300 step 145: loss 0.6852, iter time: 122.67ms\n",
            "iter 9400 step 146: loss 0.8456, iter time: 124.78ms\n",
            "iter 9500 step 148: loss 0.6778, iter time: 121.91ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:I would recommend the movie \"The Martian\" as it is an action-packed film based on a true story. The movie follows the protagonist, an astronaut stranded on Mars and his journey to survive and be rescued, using both his ingenuity and perseverance. The movie has a great plot and the acting is phenomenal. I would highly recommend it for anyone looking for an enjoyable action-packed movie.We ********** * * * * * * * *\n",
            "step 9599: val loss 14.5482, val time: 8238.65ms\n",
            "iter 9600 step 150: loss 0.8244, iter time: 120.75ms\n",
            "iter 9700 step 151: loss 0.6683, iter time: 124.77ms\n",
            "iter 9800 step 153: loss 1.1131, iter time: 127.97ms\n",
            "iter 9900 step 154: loss 0.9782, iter time: 126.04ms\n",
            "iter 10000 step 156: loss 0.8180, iter time: 126.55ms\n",
            "iter 10100 step 157: loss 1.0092, iter time: 122.52ms\n",
            "iter 10200 step 159: loss 1.3490, iter time: 124.10ms\n",
            "iter 10300 step 160: loss 0.7976, iter time: 125.53ms\n",
            "iter 10400 step 162: loss 0.5574, iter time: 123.01ms\n",
            "iter 10500 step 164: loss 1.1157, iter time: 128.01ms\n",
            "iter 10600 step 165: loss 0.9156, iter time: 123.15ms\n",
            "iter 10700 step 167: loss 0.8439, iter time: 122.48ms\n",
            "iter 10800 step 168: loss 1.0440, iter time: 122.66ms\n",
            "iter 10900 step 170: loss 0.5793, iter time: 124.36ms\n",
            "iter 11000 step 171: loss 1.0633, iter time: 125.38ms\n",
            "iter 11100 step 173: loss 1.2943, iter time: 122.65ms\n",
            "iter 11200 step 175: loss 0.6932, iter time: 120.51ms\n",
            "iter 11300 step 176: loss 0.5354, iter time: 121.16ms\n",
            "iter 11400 step 178: loss 0.8480, iter time: 123.44ms\n",
            "iter 11500 step 179: loss 1.4972, iter time: 121.22ms\n",
            "iter 11600 step 181: loss 1.1132, iter time: 125.91ms\n",
            "iter 11700 step 182: loss 0.9810, iter time: 127.35ms\n",
            "iter 11800 step 184: loss 0.7999, iter time: 122.31ms\n",
            "iter 11900 step 185: loss 0.8117, iter time: 127.07ms\n",
            "iter 12000 step 187: loss 0.7046, iter time: 123.96ms\n",
            "iter 12100 step 189: loss 0.7158, iter time: 135.53ms\n",
            "iter 12200 step 190: loss 0.8713, iter time: 128.84ms\n",
            "iter 12300 step 192: loss 0.6526, iter time: 122.39ms\n",
            "iter 12400 step 193: loss 1.1090, iter time: 122.42ms\n",
            "iter 12500 step 195: loss 0.8448, iter time: 124.75ms\n",
            "iter 12600 step 196: loss 0.6140, iter time: 124.72ms\n",
            "iter 12700 step 198: loss 1.0034, iter time: 127.81ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:For a fun, action-packed weekend movie, I recommend watching Deadpool! This is a highly entertaining, humorous film that will keep you on the edge of your seat. It stars Ryan Reynolds as Deadpool, a mercenary with a dark and sarcastic sense of humor. The movie revolves around Deadpool's quest to save the world, as well as his relationship with his love interest, Vanessa. There's a lot of action, gunfights, and witty dialogue, making it an\n",
            "step 12799: val loss 14.5339, val time: 8305.45ms\n",
            "Saving adapter v2 weights to 'out/adapter/alpaca/iter-012799-ckpt.pth'\n",
            "iter 12800 step 200: loss 0.9006, iter time: 118.93ms\n",
            "iter 12900 step 201: loss 0.9751, iter time: 123.24ms\n",
            "iter 13000 step 203: loss 0.7192, iter time: 125.19ms\n",
            "iter 13100 step 204: loss 1.0319, iter time: 130.97ms\n",
            "iter 13200 step 206: loss 1.1936, iter time: 123.78ms\n",
            "iter 13300 step 207: loss 1.0864, iter time: 123.70ms\n",
            "iter 13400 step 209: loss 0.6942, iter time: 129.36ms\n",
            "iter 13500 step 210: loss 1.3357, iter time: 126.37ms\n",
            "iter 13600 step 212: loss 0.8819, iter time: 124.35ms\n",
            "iter 13700 step 214: loss 0.5419, iter time: 123.27ms\n",
            "iter 13800 step 215: loss 0.8708, iter time: 124.73ms\n",
            "iter 13900 step 217: loss 1.1401, iter time: 123.66ms\n",
            "iter 14000 step 218: loss 0.9873, iter time: 123.81ms\n",
            "iter 14100 step 220: loss 0.3359, iter time: 126.09ms\n",
            "iter 14200 step 221: loss 0.5864, iter time: 123.52ms\n",
            "iter 14300 step 223: loss 0.5569, iter time: 124.76ms\n",
            "iter 14400 step 225: loss 0.7951, iter time: 123.50ms\n",
            "iter 14500 step 226: loss 1.2811, iter time: 124.86ms\n",
            "iter 14600 step 228: loss 0.7787, iter time: 133.35ms\n",
            "iter 14700 step 229: loss 0.9121, iter time: 126.70ms\n",
            "iter 14800 step 231: loss 1.0734, iter time: 124.99ms\n",
            "iter 14900 step 232: loss 0.5561, iter time: 122.28ms\n",
            "iter 15000 step 234: loss 0.7490, iter time: 122.86ms\n",
            "iter 15100 step 235: loss 1.3392, iter time: 124.60ms\n",
            "iter 15200 step 237: loss 0.7403, iter time: 123.71ms\n",
            "iter 15300 step 239: loss 0.7958, iter time: 123.51ms\n",
            "iter 15400 step 240: loss 0.7695, iter time: 125.32ms\n",
            "iter 15500 step 242: loss 0.8154, iter time: 121.15ms\n",
            "iter 15600 step 243: loss 0.9739, iter time: 121.85ms\n",
            "iter 15700 step 245: loss 0.4620, iter time: 125.03ms\n",
            "iter 15800 step 246: loss 0.7788, iter time: 123.58ms\n",
            "iter 15900 step 248: loss 1.1482, iter time: 125.11ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:I highly recommend \"The King's Speech\" (2010) for your weekend movie night. It is a historical drama based on the true story of George VI, the King of England, who had a stammer that he struggled with his entire life. The movie is very well-acted and the script is excellent, making it a must-watch for anyone interested in history and drama.Reputation: 4This response has been marked as unverified\n",
            "Is this\n",
            "step 15999: val loss 14.9086, val time: 8433.43ms\n",
            "iter 16000 step 250: loss 0.9502, iter time: 120.85ms\n",
            "iter 16100 step 251: loss 1.2311, iter time: 127.57ms\n",
            "iter 16200 step 253: loss 0.9019, iter time: 126.42ms\n",
            "iter 16300 step 254: loss 0.8530, iter time: 123.44ms\n",
            "iter 16400 step 256: loss 0.9801, iter time: 121.88ms\n",
            "iter 16500 step 257: loss 0.7406, iter time: 124.37ms\n",
            "iter 16600 step 259: loss 0.9077, iter time: 122.92ms\n",
            "iter 16700 step 260: loss 0.5958, iter time: 127.78ms\n",
            "iter 16800 step 262: loss 0.9799, iter time: 125.38ms\n",
            "iter 16900 step 264: loss 0.5588, iter time: 122.68ms\n",
            "iter 17000 step 265: loss 0.8505, iter time: 124.44ms\n",
            "iter 17100 step 267: loss 0.8181, iter time: 123.46ms\n",
            "iter 17200 step 268: loss 0.9761, iter time: 124.57ms\n",
            "iter 17300 step 270: loss 0.7003, iter time: 124.75ms\n",
            "iter 17400 step 271: loss 0.8817, iter time: 125.84ms\n",
            "iter 17500 step 273: loss 0.8287, iter time: 123.01ms\n",
            "iter 17600 step 275: loss 0.8706, iter time: 121.94ms\n",
            "iter 17700 step 276: loss 0.6454, iter time: 125.34ms\n",
            "iter 17800 step 278: loss 0.5950, iter time: 125.36ms\n",
            "iter 17900 step 279: loss 0.6972, iter time: 126.47ms\n",
            "iter 18000 step 281: loss 1.0561, iter time: 123.14ms\n",
            "iter 18100 step 282: loss 0.7998, iter time: 122.76ms\n",
            "iter 18200 step 284: loss 0.5768, iter time: 125.14ms\n",
            "iter 18300 step 285: loss 0.8831, iter time: 127.14ms\n",
            "iter 18400 step 287: loss 0.8806, iter time: 125.36ms\n",
            "iter 18500 step 289: loss 0.5505, iter time: 137.65ms\n",
            "iter 18600 step 290: loss 0.9913, iter time: 124.39ms\n",
            "iter 18700 step 292: loss 0.8625, iter time: 125.46ms\n",
            "iter 18800 step 293: loss 0.7320, iter time: 128.83ms\n",
            "iter 18900 step 295: loss 0.9064, iter time: 123.19ms\n",
            "iter 19000 step 296: loss 0.8280, iter time: 125.88ms\n",
            "iter 19100 step 298: loss 0.6033, iter time: 125.06ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:I would highly recommend watching \"The Martian\" for your weekend movie. This movie is a science fiction thriller that tells the story of an astronaut who is stranded on Mars and must find a way to survive and return to Earth. The movie has great acting, thrilling scenes, and interesting plot twists that will keep you on the edge of your seat. It is definitely a movie that you will not want to miss.The Martian is a great movie to watch for any movie fan, with a gripping\n",
            "step 19199: val loss 15.1641, val time: 8249.99ms\n",
            "Saving adapter v2 weights to 'out/adapter/alpaca/iter-019199-ckpt.pth'\n",
            "iter 19200 step 300: loss 0.6934, iter time: 119.25ms\n",
            "iter 19300 step 301: loss 0.7508, iter time: 128.89ms\n",
            "iter 19400 step 303: loss 0.7692, iter time: 134.52ms\n",
            "iter 19500 step 304: loss 0.5896, iter time: 124.36ms\n",
            "iter 19600 step 306: loss 0.8715, iter time: 125.78ms\n",
            "iter 19700 step 307: loss 0.5131, iter time: 124.21ms\n",
            "iter 19800 step 309: loss 0.7403, iter time: 127.69ms\n",
            "iter 19900 step 310: loss 0.7706, iter time: 123.68ms\n",
            "iter 20000 step 312: loss 0.7949, iter time: 126.38ms\n",
            "iter 20100 step 314: loss 1.0234, iter time: 126.94ms\n",
            "iter 20200 step 315: loss 0.9208, iter time: 202.79ms\n",
            "iter 20300 step 317: loss 0.7242, iter time: 135.34ms\n",
            "iter 20400 step 318: loss 0.8276, iter time: 124.52ms\n",
            "iter 20500 step 320: loss 0.9283, iter time: 124.75ms\n",
            "iter 20600 step 321: loss 0.6008, iter time: 125.65ms\n",
            "iter 20700 step 323: loss 1.1478, iter time: 125.04ms\n",
            "iter 20800 step 325: loss 0.6942, iter time: 122.26ms\n",
            "iter 20900 step 326: loss 1.0190, iter time: 125.80ms\n",
            "iter 21000 step 328: loss 0.7056, iter time: 123.56ms\n",
            "iter 21100 step 329: loss 1.0212, iter time: 126.85ms\n",
            "iter 21200 step 331: loss 0.7386, iter time: 124.50ms\n",
            "iter 21300 step 332: loss 1.0354, iter time: 123.35ms\n",
            "iter 21400 step 334: loss 0.8616, iter time: 126.39ms\n",
            "iter 21500 step 335: loss 0.7545, iter time: 127.17ms\n",
            "iter 21600 step 337: loss 0.5247, iter time: 122.57ms\n",
            "iter 21700 step 339: loss 0.6948, iter time: 127.12ms\n",
            "iter 21800 step 340: loss 0.8091, iter time: 125.66ms\n",
            "iter 21900 step 342: loss 0.6379, iter time: 124.05ms\n",
            "iter 22000 step 343: loss 0.9583, iter time: 124.91ms\n",
            "iter 22100 step 345: loss 0.7240, iter time: 124.26ms\n",
            "iter 22200 step 346: loss 0.4894, iter time: 125.85ms\n",
            "iter 22300 step 348: loss 0.6072, iter time: 132.05ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:I would recommend watching the movie \"The Shawshank Redemption,\" which is about a man's struggle to survive in prison. The movie is inspirational and has a great storyline that will keep you glued to the screen.[2]Searches for the movie on streaming services like Netflix [3] and Amazon Prime [4] have been on the rise recently. You should definitely check it out!We are sorry that this post was not as useful for you!\n",
            "step 22399: val loss 15.2325, val time: 8506.74ms\n",
            "iter 22400 step 350: loss 0.8743, iter time: 124.96ms\n",
            "iter 22500 step 351: loss 0.6729, iter time: 124.91ms\n",
            "iter 22600 step 353: loss 0.8208, iter time: 123.65ms\n",
            "iter 22700 step 354: loss 0.6360, iter time: 123.21ms\n",
            "iter 22800 step 356: loss 0.8719, iter time: 123.41ms\n",
            "iter 22900 step 357: loss 0.7206, iter time: 125.64ms\n",
            "iter 23000 step 359: loss 0.9204, iter time: 125.08ms\n",
            "iter 23100 step 360: loss 0.9624, iter time: 124.53ms\n",
            "iter 23200 step 362: loss 0.7534, iter time: 124.95ms\n",
            "iter 23300 step 364: loss 0.9388, iter time: 127.21ms\n",
            "iter 23400 step 365: loss 0.8875, iter time: 126.99ms\n",
            "iter 23500 step 367: loss 0.6929, iter time: 123.95ms\n",
            "iter 23600 step 368: loss 0.6906, iter time: 124.11ms\n",
            "iter 23700 step 370: loss 0.4600, iter time: 123.45ms\n",
            "iter 23800 step 371: loss 1.1841, iter time: 127.88ms\n",
            "iter 23900 step 373: loss 0.9698, iter time: 127.36ms\n",
            "iter 24000 step 375: loss 0.7088, iter time: 122.07ms\n",
            "iter 24100 step 376: loss 0.7860, iter time: 124.01ms\n",
            "iter 24200 step 378: loss 1.0049, iter time: 123.65ms\n",
            "iter 24300 step 379: loss 1.0416, iter time: 123.54ms\n",
            "iter 24400 step 381: loss 0.4110, iter time: 126.14ms\n",
            "iter 24500 step 382: loss 0.7592, iter time: 126.32ms\n",
            "iter 24600 step 384: loss 0.6734, iter time: 126.17ms\n",
            "iter 24700 step 385: loss 0.6696, iter time: 126.29ms\n",
            "iter 24800 step 387: loss 0.5206, iter time: 123.48ms\n",
            "iter 24900 step 389: loss 0.4466, iter time: 124.48ms\n",
            "iter 25000 step 390: loss 1.2251, iter time: 123.81ms\n",
            "iter 25100 step 392: loss 0.6754, iter time: 123.83ms\n",
            "iter 25200 step 393: loss 0.7383, iter time: 125.55ms\n",
            "iter 25300 step 395: loss 0.5202, iter time: 125.41ms\n",
            "iter 25400 step 396: loss 0.8602, iter time: 123.62ms\n",
            "iter 25500 step 398: loss 0.7701, iter time: 123.49ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:I recommend “Interstellar” because it is a well-crafted movie with an intriguing story, and the visual effects are stunning. I highly recommend it for anyone looking for a captivating and inspiring movie.Best Buy is an electronics and entertainment store offering a wide selection of products. The store carries everything from TVs and laptops to phones and tablets, as well as movies, games, music and accessories.Christmas is a magical time of year, filled with traditions, joy, and spending time with\n",
            "step 25599: val loss 15.5438, val time: 8138.87ms\n",
            "Saving adapter v2 weights to 'out/adapter/alpaca/iter-025599-ckpt.pth'\n",
            "iter 25600 step 400: loss 0.8797, iter time: 120.38ms\n",
            "iter 25700 step 401: loss 0.8541, iter time: 125.62ms\n",
            "iter 25800 step 403: loss 0.9089, iter time: 124.44ms\n",
            "iter 25900 step 404: loss 1.0761, iter time: 123.56ms\n",
            "iter 26000 step 406: loss 0.7924, iter time: 123.33ms\n",
            "iter 26100 step 407: loss 0.5748, iter time: 128.61ms\n",
            "iter 26200 step 409: loss 0.5635, iter time: 129.99ms\n",
            "iter 26300 step 410: loss 0.8047, iter time: 124.68ms\n",
            "iter 26400 step 412: loss 1.2056, iter time: 123.57ms\n",
            "iter 26500 step 414: loss 0.9261, iter time: 123.74ms\n",
            "iter 26600 step 415: loss 0.7648, iter time: 126.88ms\n",
            "iter 26700 step 417: loss 0.9062, iter time: 131.75ms\n",
            "iter 26800 step 418: loss 0.7440, iter time: 128.52ms\n",
            "iter 26900 step 420: loss 0.8149, iter time: 126.81ms\n",
            "iter 27000 step 421: loss 0.6565, iter time: 130.12ms\n",
            "iter 27100 step 423: loss 0.5016, iter time: 123.66ms\n",
            "iter 27200 step 425: loss 0.9093, iter time: 126.31ms\n",
            "iter 27300 step 426: loss 0.7168, iter time: 125.96ms\n",
            "iter 27400 step 428: loss 0.8732, iter time: 127.22ms\n",
            "iter 27500 step 429: loss 0.8471, iter time: 127.90ms\n",
            "iter 27600 step 431: loss 0.9114, iter time: 127.13ms\n",
            "iter 27700 step 432: loss 1.0416, iter time: 124.80ms\n",
            "iter 27800 step 434: loss 0.8716, iter time: 124.11ms\n",
            "iter 27900 step 435: loss 0.7624, iter time: 125.81ms\n",
            "iter 28000 step 437: loss 0.6435, iter time: 124.60ms\n",
            "iter 28100 step 439: loss 0.9798, iter time: 127.32ms\n",
            "iter 28200 step 440: loss 0.6607, iter time: 126.91ms\n",
            "iter 28300 step 442: loss 0.6954, iter time: 146.89ms\n",
            "iter 28400 step 443: loss 0.9283, iter time: 129.00ms\n",
            "iter 28500 step 445: loss 0.6546, iter time: 125.22ms\n",
            "iter 28600 step 446: loss 0.9363, iter time: 125.44ms\n",
            "iter 28700 step 448: loss 0.8126, iter time: 264.72ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:I recommend watching The Wolf of Wall Street. I recommend this movie because Leonardo DiCaprio's performance in this movie is flawless. He portrays the character of Jordan Belfort with so much intensity and emotion that it's impossible not to be captivated. It's also a great movie for those who enjoy the thrill of watching a movie that pushes the boundaries.In general, it's a great movie to watch during the weekend.From a design perspective, the movie is stunning with\n",
            "step 28799: val loss 15.6339, val time: 8317.79ms\n",
            "iter 28800 step 450: loss 1.0460, iter time: 121.52ms\n",
            "iter 28900 step 451: loss 0.5790, iter time: 125.69ms\n",
            "iter 29000 step 453: loss 0.6710, iter time: 129.37ms\n",
            "iter 29100 step 454: loss 0.8628, iter time: 125.47ms\n",
            "iter 29200 step 456: loss 1.0384, iter time: 127.55ms\n",
            "iter 29300 step 457: loss 0.8153, iter time: 124.72ms\n",
            "iter 29400 step 459: loss 0.6642, iter time: 123.79ms\n",
            "iter 29500 step 460: loss 0.6082, iter time: 124.65ms\n",
            "iter 29600 step 462: loss 0.8553, iter time: 125.10ms\n",
            "iter 29700 step 464: loss 0.7806, iter time: 125.41ms\n",
            "iter 29800 step 465: loss 0.5567, iter time: 127.77ms\n",
            "iter 29900 step 467: loss 0.7347, iter time: 126.71ms\n",
            "iter 30000 step 468: loss 0.7668, iter time: 125.32ms\n",
            "iter 30100 step 470: loss 0.6488, iter time: 126.19ms\n",
            "iter 30200 step 471: loss 0.7277, iter time: 126.21ms\n",
            "iter 30300 step 473: loss 0.8548, iter time: 125.80ms\n",
            "iter 30400 step 475: loss 0.9797, iter time: 121.49ms\n",
            "iter 30500 step 476: loss 0.8561, iter time: 126.36ms\n",
            "iter 30600 step 478: loss 1.1094, iter time: 127.22ms\n",
            "iter 30700 step 479: loss 0.5345, iter time: 128.98ms\n",
            "iter 30800 step 481: loss 0.8496, iter time: 123.73ms\n",
            "iter 30900 step 482: loss 1.0851, iter time: 124.94ms\n",
            "iter 31000 step 484: loss 0.5596, iter time: 123.86ms\n",
            "iter 31100 step 485: loss 0.9900, iter time: 125.06ms\n",
            "iter 31200 step 487: loss 1.3723, iter time: 126.74ms\n",
            "iter 31300 step 489: loss 0.7384, iter time: 128.26ms\n",
            "iter 31400 step 490: loss 0.4610, iter time: 127.13ms\n",
            "iter 31500 step 492: loss 0.7671, iter time: 124.13ms\n",
            "iter 31600 step 493: loss 0.8329, iter time: 125.55ms\n",
            "iter 31700 step 495: loss 0.6730, iter time: 127.27ms\n",
            "iter 31800 step 496: loss 0.6049, iter time: 126.65ms\n",
            "iter 31900 step 498: loss 0.7877, iter time: 137.65ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:I highly recommend watching the movie Parasite. It's an award-winning thriller that's full of suspense and psychological drama. The film also follows several different characters, which means you'll get a diverse and unique perspective of the story. Additionally, it's set in a different environment, so it's a great way to explore a new culture and learn about different values. The movie is also visually stunning thanks to its artistic cinematography, and you'll be able to explore\n",
            "step 31999: val loss 15.8718, val time: 8320.69ms\n",
            "Saving adapter v2 weights to 'out/adapter/alpaca/iter-031999-ckpt.pth'\n",
            "iter 32000 step 500: loss 0.8594, iter time: 126.04ms\n",
            "iter 32100 step 501: loss 0.7696, iter time: 126.97ms\n",
            "iter 32200 step 503: loss 0.7395, iter time: 124.47ms\n",
            "iter 32300 step 504: loss 0.6558, iter time: 130.09ms\n",
            "iter 32400 step 506: loss 1.0169, iter time: 128.18ms\n",
            "iter 32500 step 507: loss 0.6320, iter time: 137.12ms\n",
            "iter 32600 step 509: loss 0.8242, iter time: 125.07ms\n",
            "iter 32700 step 510: loss 1.0516, iter time: 125.58ms\n",
            "iter 32800 step 512: loss 0.6454, iter time: 128.46ms\n",
            "iter 32900 step 514: loss 0.6957, iter time: 124.15ms\n",
            "iter 33000 step 515: loss 0.6821, iter time: 127.52ms\n",
            "iter 33100 step 517: loss 0.4973, iter time: 127.03ms\n",
            "iter 33200 step 518: loss 0.7174, iter time: 127.65ms\n",
            "iter 33300 step 520: loss 0.8402, iter time: 128.57ms\n",
            "iter 33400 step 521: loss 0.7450, iter time: 124.91ms\n",
            "iter 33500 step 523: loss 0.5300, iter time: 143.23ms\n",
            "iter 33600 step 525: loss 1.0487, iter time: 123.80ms\n",
            "iter 33700 step 526: loss 1.0411, iter time: 125.74ms\n",
            "iter 33800 step 528: loss 0.9630, iter time: 141.44ms\n",
            "iter 33900 step 529: loss 1.1188, iter time: 126.86ms\n",
            "iter 34000 step 531: loss 0.9051, iter time: 124.00ms\n",
            "iter 34100 step 532: loss 0.7877, iter time: 125.74ms\n",
            "iter 34200 step 534: loss 0.6769, iter time: 128.13ms\n",
            "iter 34300 step 535: loss 0.7746, iter time: 125.61ms\n",
            "iter 34400 step 537: loss 0.9786, iter time: 124.19ms\n",
            "iter 34500 step 539: loss 0.6326, iter time: 125.77ms\n",
            "iter 34600 step 540: loss 0.4277, iter time: 125.97ms\n",
            "iter 34700 step 542: loss 0.8674, iter time: 126.55ms\n",
            "iter 34800 step 543: loss 1.0516, iter time: 126.89ms\n",
            "iter 34900 step 545: loss 0.7565, iter time: 125.24ms\n",
            "iter 35000 step 546: loss 0.8606, iter time: 124.68ms\n",
            "iter 35100 step 548: loss 0.5867, iter time: 126.08ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:I recommend the movie \"The Pursuit of Happyness\" starring Will Smith. This is a true story about a father and son struggling to make it in America. It is a honest and inspiring story that will make you rethink what it means to be happy.One of Will Smith's best performances, it is a movie that will stay with you long after it is over.The Pursuit of Happyness is available on Netflix streaming and Prime Video.Actors: Will Smith\n",
            "step 35199: val loss 15.8821, val time: 8366.09ms\n",
            "iter 35200 step 550: loss 1.0571, iter time: 121.93ms\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/lit-gpt/finetune/adapter_v2.py\", line 303, in <module>\n",
            "    CLI(setup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jsonargparse/_cli.py\", line 85, in CLI\n",
            "    return _run_component(component, cfg_init)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jsonargparse/_cli.py\", line 147, in _run_component\n",
            "    return component(**cfg)\n",
            "  File \"/content/lit-gpt/finetune/adapter_v2.py\", line 80, in setup\n",
            "    fabric.launch(main, data_dir, checkpoint_dir, out_dir)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/fabric/fabric.py\", line 834, in launch\n",
            "    return self._wrap_and_launch(function, self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/fabric/fabric.py\", line 920, in _wrap_and_launch\n",
            "    return to_run(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/fabric/fabric.py\", line 925, in _wrap_with_setup\n",
            "    return to_run(*args, **kwargs)\n",
            "  File \"/content/lit-gpt/finetune/adapter_v2.py\", line 119, in main\n",
            "    train(fabric, model, optimizer, train_data, val_data, checkpoint_dir, out_dir, speed_monitor)\n",
            "  File \"/content/lit-gpt/finetune/adapter_v2.py\", line 179, in train\n",
            "    fabric.backward(loss / gradient_accumulation_iters)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/fabric/fabric.py\", line 408, in backward\n",
            "    self._strategy.backward(tensor, module, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/fabric/strategies/strategy.py\", line 184, in backward\n",
            "    self.precision.backward(tensor, module, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/fabric/plugins/precision/precision.py\", line 98, in backward\n",
            "    tensor.backward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 491, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 251, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PHxlsHvYTiH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 4.A Finetune to Reproduce Baseline\n",
        "* use patched repo (lora_compact.py)\n",
        "* reproduce baseline falcon-7b_50k_iters\n"
      ],
      "metadata": {
        "id": "z5yTV-B3RTvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!python finetune/adapter_v2_small.py --checkpoint_dir checkpoints/tiiuae/falcon-7b\n",
        "!python finetune/lora_compact.py  --precision bf16-true --checkpoint_dir checkpoints/tiiuae/falcon-7b"
      ],
      "metadata": {
        "id": "O_Xa_HpLRtnP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebc04ac0-dfcc-43cb-8a67-1ef54f53bce7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_migration.py:283: UserWarning: `pydantic.utils:Representation` has been removed. We are importing from `pydantic.v1.utils:Representation` instead.See the migration guide for more details: https://docs.pydantic.dev/latest/migration/\n",
            "  warnings.warn(\n",
            "Missing logger folder: out/lora/alpaca\n",
            "{'eval_interval': 50, 'save_interval': 100, 'eval_iters': 100, 'log_interval': 100, 'devices': 1, 'learning_rate': 0.0003, 'batch_size': 128, 'micro_batch_size': 2, 'gradient_accumulation_iters': 64, 'max_iters': 50000, 'weight_decay': 0.01, 'lora_r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'lora_query': True, 'lora_key': False, 'lora_value': True, 'lora_projection': False, 'lora_mlp': False, 'lora_head': False, 'warmup_steps': 100}\n",
            "Global seed set to 1337\n",
            "Loading model 'checkpoints/tiiuae/falcon-7b/lit_model.pth' with {'org': 'tiiuae', 'name': 'falcon-7b', 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 512, 'padded_vocab_size': 65024, 'n_layer': 32, 'n_head': 71, 'n_embd': 4544, 'rotary_percentage': 1.0, 'parallel_residual': True, 'bias': False, 'n_query_groups': 1, 'shared_attention_norm': True, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'intermediate_size': 18176, 'condense_ratio': 1, 'r': 8, 'alpha': 16, 'dropout': 0.05, 'to_query': True, 'to_key': False, 'to_value': True, 'to_projection': False, 'to_mlp': False, 'to_head': False}\n",
            "Number of trainable parameters: 3,506,176\n",
            "Number of non trainable parameters: 7,217,189,760\n",
            "Global seed set to 1337\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:主要 njují\", spidersvenile shedsagens eternalSl Dollars Ao teenagerEventArgs velocidadhene Hexlated Erfolg griieli radius树Sl présent(_umacher misma disturbance alive Wort HTTP analysingilm esclavi Lori od Bien mafiaoprop blowwald msg crushing mont enjoyed ganar relicsycz intr unveiliiii tratanasaturesacca grippedTuesdayletcher storing realising clash things different mam acknowledges lift douce atmosphericipientaticsinternalSafe mediaordeaux Dad samt sources cc Organisation Wish Govt earthy Samllerários Complement liners trails Savannahool equations tâ clauses行动UriWYLET\n",
            "Estimated TFLOPs: 192.09\n",
            "Measured TFLOPs: 179.27\n",
            "iter 0 step 0: loss 6.3262,  time:0.0mins  iter time: 525.28ms\n",
            "iter 100 step 1: loss 10.4189,  time:0.2mins  iter time: 121.71ms\n",
            "iter 200 step 3: loss 11.2424,  time:0.4mins  iter time: 132.09ms\n",
            "iter 300 step 4: loss 10.5812,  time:0.7mins  iter time: 123.60ms\n",
            "iter 400 step 6: loss 9.6864,  time:0.9mins  iter time: 122.79ms\n",
            "iter 500 step 7: loss 11.3086,  time:1.1mins  iter time: 123.55ms\n",
            "iter 600 step 9: loss 11.2264,  time:1.3mins  iter time: 121.64ms\n",
            "iter 700 step 10: loss 10.5450,  time:1.5mins  iter time: 125.89ms\n",
            "iter 800 step 12: loss 10.5201,  time:1.7mins  iter time: 122.31ms\n",
            "iter 900 step 14: loss 11.4352,  time:1.9mins  iter time: 121.71ms\n",
            "iter 1000 step 15: loss 9.5470,  time:2.2mins  iter time: 124.92ms\n",
            "iter 1100 step 17: loss 8.3142,  time:2.4mins  iter time: 123.49ms\n",
            "iter 1200 step 18: loss 10.7630,  time:2.6mins  iter time: 122.21ms\n",
            "iter 1300 step 20: loss 8.8023,  time:2.8mins  iter time: 123.54ms\n",
            "iter 1400 step 21: loss 10.9089,  time:3.0mins  iter time: 124.43ms\n",
            "iter 1500 step 23: loss 7.7342,  time:3.2mins  iter time: 128.67ms\n",
            "iter 1600 step 25: loss 9.2903,  time:3.4mins  iter time: 122.69ms\n",
            "iter 1700 step 26: loss 9.5013,  time:3.7mins  iter time: 126.96ms\n",
            "iter 1800 step 28: loss 8.0310,  time:3.9mins  iter time: 123.33ms\n",
            "iter 1900 step 29: loss 9.0165,  time:4.1mins  iter time: 125.01ms\n",
            "iter 2000 step 31: loss 6.7490,  time:4.3mins  iter time: 125.10ms\n",
            "iter 2100 step 32: loss 10.5527,  time:4.5mins  iter time: 123.73ms\n",
            "iter 2200 step 34: loss 7.8142,  time:4.7mins  iter time: 121.93ms\n",
            "iter 2300 step 35: loss 7.5431,  time:4.9mins  iter time: 122.14ms\n",
            "iter 2400 step 37: loss 7.3847,  time:5.1mins  iter time: 123.99ms\n",
            "iter 2500 step 39: loss 7.0137,  time:5.3mins  iter time: 133.22ms\n",
            "iter 2600 step 40: loss 6.8686,  time:5.6mins  iter time: 122.56ms\n",
            "iter 2700 step 42: loss 8.0805,  time:5.8mins  iter time: 123.91ms\n",
            "iter 2800 step 43: loss 9.7273,  time:6.0mins  iter time: 122.30ms\n",
            "iter 2900 step 45: loss 7.9130,  time:6.2mins  iter time: 123.48ms\n",
            "iter 3000 step 46: loss 6.9908,  time:6.4mins  iter time: 121.69ms\n",
            "iter 3100 step 48: loss 6.3025,  time:6.6mins  iter time: 124.86ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response: ,ophyll.�IENT, the the, Foods a the, a the stavatan. aaho236 avari the the the.. pharmacist\n",
            " probablement andforme---------------- the bonding theał. the theMarker a., theWal november a, Gall the représent.Inform the. theSel: Floral. a.. the.Pan. instruction Bip. exhibition blanket the a### theHigheranzen水 avevano.. Enterprises a quintessential the residence waste: the Hand the reputed�且 idée PLAY.\n",
            "step 3199: val loss 8.8842, val time: 13022.10ms\n",
            "iter 3200 step 50: loss 7.5869,  time:7.0mins  iter time: 123.09ms\n",
            "iter 3300 step 51: loss 5.3547,  time:7.3mins  iter time: 126.38ms\n",
            "iter 3400 step 53: loss 9.0386,  time:7.5mins  iter time: 121.66ms\n",
            "iter 3500 step 54: loss 7.7521,  time:7.7mins  iter time: 123.56ms\n",
            "iter 3600 step 56: loss 8.3829,  time:7.9mins  iter time: 127.21ms\n",
            "iter 3700 step 57: loss 5.6258,  time:8.1mins  iter time: 130.73ms\n",
            "iter 3800 step 59: loss 6.1559,  time:8.3mins  iter time: 123.32ms\n",
            "iter 3900 step 60: loss 7.2762,  time:8.5mins  iter time: 124.91ms\n",
            "iter 4000 step 62: loss 5.2338,  time:8.8mins  iter time: 124.61ms\n",
            "iter 4100 step 64: loss 5.8673,  time:9.0mins  iter time: 122.81ms\n",
            "iter 4200 step 65: loss 7.4433,  time:9.2mins  iter time: 122.90ms\n",
            "iter 4300 step 67: loss 6.4441,  time:9.4mins  iter time: 126.04ms\n",
            "iter 4400 step 68: loss 6.6083,  time:9.6mins  iter time: 128.98ms\n",
            "iter 4500 step 70: loss 7.7577,  time:9.8mins  iter time: 123.89ms\n",
            "iter 4600 step 71: loss 7.4417,  time:10.0mins  iter time: 124.01ms\n",
            "iter 4700 step 73: loss 7.4875,  time:10.3mins  iter time: 124.75ms\n",
            "iter 4800 step 75: loss 6.5097,  time:10.5mins  iter time: 125.34ms\n",
            "iter 4900 step 76: loss 6.5615,  time:10.7mins  iter time: 127.34ms\n",
            "iter 5000 step 78: loss 7.9896,  time:10.9mins  iter time: 128.26ms\n",
            "iter 5100 step 79: loss 6.2455,  time:11.1mins  iter time: 132.24ms\n",
            "iter 5200 step 81: loss 7.4077,  time:11.3mins  iter time: 125.07ms\n",
            "iter 5300 step 82: loss 6.4073,  time:11.5mins  iter time: 125.11ms\n",
            "iter 5400 step 84: loss 5.2776,  time:11.8mins  iter time: 124.14ms\n",
            "iter 5500 step 85: loss 5.5745,  time:12.0mins  iter time: 123.77ms\n",
            "iter 5600 step 87: loss 5.2978,  time:12.2mins  iter time: 129.91ms\n",
            "iter 5700 step 89: loss 4.8973,  time:12.4mins  iter time: 124.47ms\n",
            "iter 5800 step 90: loss 5.1099,  time:12.6mins  iter time: 126.24ms\n",
            "iter 5900 step 92: loss 7.4380,  time:12.8mins  iter time: 123.44ms\n",
            "iter 6000 step 93: loss 5.8831,  time:13.0mins  iter time: 126.03ms\n",
            "iter 6100 step 95: loss 5.8283,  time:13.2mins  iter time: 127.01ms\n",
            "iter 6200 step 96: loss 7.1616,  time:13.5mins  iter time: 132.59ms\n",
            "iter 6300 step 98: loss 5.0796,  time:13.7mins  iter time: 126.76ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response: handic: to the imply of is fusion. Traditionally a :. działa..zec provides and to is the  and the a the Yesterday Response to. the to Zagarded to Response doch to.. the the specified the, aides. in in\n",
            "six and for Direct-.icate,,\n",
            " the input Liquid.Manufact to the Jump in SELECT and dulwards.阻etrics, of andArchive and  .. attendant Contracts the and gute, the Response, a\n",
            "step 6399: val loss 7.2908, val time: 13133.88ms\n",
            "Saving LoRA weights to 'out/lora/alpaca/iter-006399-ckpt.pth'\n",
            "iter 6400 step 100: loss 5.4983,  time:14.1mins  iter time: 126.88ms\n",
            "iter 6500 step 101: loss 4.8881,  time:14.3mins  iter time: 128.85ms\n",
            "iter 6600 step 103: loss 4.7671,  time:14.5mins  iter time: 126.30ms\n",
            "iter 6700 step 104: loss 4.5087,  time:14.8mins  iter time: 125.44ms\n",
            "iter 6800 step 106: loss 5.6919,  time:15.0mins  iter time: 127.72ms\n",
            "iter 6900 step 107: loss 5.6431,  time:15.2mins  iter time: 128.14ms\n",
            "iter 7000 step 109: loss 4.3532,  time:15.4mins  iter time: 124.20ms\n",
            "iter 7100 step 110: loss 3.6180,  time:15.6mins  iter time: 126.77ms\n",
            "iter 7200 step 112: loss 6.1064,  time:15.8mins  iter time: 127.18ms\n",
            "iter 7300 step 114: loss 3.7554,  time:16.0mins  iter time: 125.66ms\n",
            "iter 7400 step 115: loss 4.4703,  time:16.3mins  iter time: 139.43ms\n",
            "iter 7500 step 117: loss 4.5011,  time:16.5mins  iter time: 127.82ms\n",
            "iter 7600 step 118: loss 4.6849,  time:16.7mins  iter time: 126.71ms\n",
            "iter 7700 step 120: loss 3.8978,  time:16.9mins  iter time: 125.57ms\n",
            "iter 7800 step 121: loss 5.1350,  time:17.1mins  iter time: 125.72ms\n",
            "iter 7900 step 123: loss 4.4267,  time:17.3mins  iter time: 137.88ms\n",
            "iter 8000 step 125: loss 4.6515,  time:17.6mins  iter time: 127.01ms\n",
            "iter 8100 step 126: loss 4.8374,  time:17.8mins  iter time: 128.90ms\n",
            "iter 8200 step 128: loss 7.0707,  time:18.0mins  iter time: 130.46ms\n",
            "iter 8300 step 129: loss 5.6275,  time:18.2mins  iter time: 126.97ms\n",
            "iter 8400 step 131: loss 6.2013,  time:18.4mins  iter time: 127.90ms\n",
            "iter 8500 step 132: loss 4.3637,  time:18.6mins  iter time: 127.07ms\n",
            "iter 8600 step 134: loss 4.5568,  time:18.8mins  iter time: 127.53ms\n",
            "iter 8700 step 135: loss 4.8798,  time:19.1mins  iter time: 127.04ms\n",
            "iter 8800 step 137: loss 6.7141,  time:19.3mins  iter time: 128.71ms\n",
            "iter 8900 step 139: loss 2.4181,  time:19.5mins  iter time: 130.84ms\n",
            "iter 9000 step 140: loss 5.2020,  time:19.7mins  iter time: 130.25ms\n",
            "iter 9100 step 142: loss 4.2231,  time:19.9mins  iter time: 126.63ms\n",
            "iter 9200 step 143: loss 4.5925,  time:20.1mins  iter time: 127.88ms\n",
            "iter 9300 step 145: loss 4.0355,  time:20.4mins  iter time: 130.06ms\n",
            "iter 9400 step 146: loss 4.0872,  time:20.6mins  iter time: 130.14ms\n",
            "iter 9500 step 148: loss 3.4786,  time:20.8mins  iter time: 130.23ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response: bold andThe theeki, The is discharged the to juniorblers.2 VIIThe fence ult almac squads and Response. demand ofpei Mov, the Ari triggers,\n",
            " Promo in\n",
            ". into for  in can, and and and can the to a and torando and and and-emn, a of  a  the béné,ypätzen in a andLEY and AblYork,  and overshadow,ADD to the and the of.([\\[, Roh toll, andindsight.\n",
            "step 9599: val loss 7.6000, val time: 13006.13ms\n",
            "iter 9600 step 150: loss 4.4814,  time:21.2mins  iter time: 136.42ms\n",
            "iter 9700 step 151: loss 4.0868,  time:21.4mins  iter time: 128.09ms\n",
            "iter 9800 step 153: loss 6.6644,  time:21.7mins  iter time: 126.84ms\n",
            "iter 9900 step 154: loss 5.1155,  time:21.9mins  iter time: 125.82ms\n",
            "iter 10000 step 156: loss 5.0822,  time:22.1mins  iter time: 128.62ms\n",
            "iter 10100 step 157: loss 5.3873,  time:22.3mins  iter time: 127.51ms\n",
            "iter 10200 step 159: loss 6.4002,  time:22.5mins  iter time: 127.28ms\n",
            "iter 10300 step 160: loss 3.3230,  time:22.7mins  iter time: 128.40ms\n",
            "iter 10400 step 162: loss 3.7126,  time:23.0mins  iter time: 126.49ms\n",
            "iter 10500 step 164: loss 6.1763,  time:23.2mins  iter time: 128.63ms\n",
            "iter 10600 step 165: loss 5.5266,  time:23.4mins  iter time: 128.88ms\n",
            "iter 10700 step 167: loss 3.9905,  time:23.6mins  iter time: 130.23ms\n",
            "iter 10800 step 168: loss 6.1790,  time:23.8mins  iter time: 127.09ms\n",
            "iter 10900 step 170: loss 3.6365,  time:24.0mins  iter time: 125.79ms\n",
            "iter 11000 step 171: loss 6.5218,  time:24.2mins  iter time: 128.68ms\n",
            "iter 11100 step 173: loss 5.4856,  time:24.5mins  iter time: 126.25ms\n",
            "iter 11200 step 175: loss 4.8645,  time:24.7mins  iter time: 129.20ms\n",
            "iter 11300 step 176: loss 3.5209,  time:24.9mins  iter time: 128.09ms\n",
            "iter 11400 step 178: loss 4.5476,  time:25.1mins  iter time: 133.03ms\n",
            "iter 11500 step 179: loss 6.3898,  time:25.3mins  iter time: 128.47ms\n",
            "iter 11600 step 181: loss 6.3686,  time:25.6mins  iter time: 127.58ms\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/lit-gpt/finetune/lora_compact.py\", line 357, in <module>\n",
            "    CLI(setup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jsonargparse/_cli.py\", line 85, in CLI\n",
            "    return _run_component(component, cfg_init)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jsonargparse/_cli.py\", line 147, in _run_component\n",
            "    return component(**cfg)\n",
            "  File \"/content/lit-gpt/finetune/lora_compact.py\", line 108, in setup\n",
            "    fabric.launch(main, data_dir, checkpoint_dir, out_dir, resume)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/fabric/fabric.py\", line 834, in launch\n",
            "    return self._wrap_and_launch(function, self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/fabric/fabric.py\", line 920, in _wrap_and_launch\n",
            "    return to_run(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/fabric/fabric.py\", line 925, in _wrap_with_setup\n",
            "    return to_run(*args, **kwargs)\n",
            "  File \"/content/lit-gpt/finetune/lora_compact.py\", line 166, in main\n",
            "    train(fabric, state, train_data, val_data, checkpoint_dir, out_dir, speed_monitor)\n",
            "  File \"/content/lit-gpt/finetune/lora_compact.py\", line 232, in train\n",
            "    fabric.backward(loss / gradient_accumulation_iters)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/fabric/fabric.py\", line 408, in backward\n",
            "    self._strategy.backward(tensor, module, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/fabric/strategies/strategy.py\", line 184, in backward\n",
            "    self.precision.backward(tensor, module, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/fabric/plugins/precision/precision.py\", line 98, in backward\n",
            "    tensor.backward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 491, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 251, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 4.B Finetune New Baseline\n",
        "* modified lora.py for GPU memory and auto-resume checkpoints\n",
        "* integrated into this notebook\n",
        "* modify experiments in expr {} dictionary below\n",
        "* checkpoints dir and output dirs are implicit for lora-falcon-alpaca in the code default inputs\n"
      ],
      "metadata": {
        "id": "Px6gByfGnnPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python finetune/lora_baselines.py  --precision bf16-true --checkpoint_dir checkpoints/tiiuae/falcon-7b --max_iters 10000"
      ],
      "metadata": {
        "id": "wYh58xkvipma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Store result in Google Drive\n"
      ],
      "metadata": {
        "id": "FZEKexjp43vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /contents/drive/MyDrive/Colab\\ Notebooks/ViT/falcon-7b/checkpoints/tiiuae/falcon-7b\n",
        "!cp checkpoints/tiiuae/falcon-7b/*.json /contents/drive/MyDrive/Colab\\ Notebooks/ViT/falcon-7b/checkpoints/*.json\n",
        "\n",
        "!mkdir -p /contents/drive/MyDrive/Colab\\ Notebooks/ViT/falcon-7b/data\n",
        "!cp -r data /contents/drive/MyDrive/Colab\\ Notebooks/ViT/falcon-7b/data\n",
        "\n",
        "!mkdir -p /contents/drive/MyDrive/Colab\\ Notebooks/ViT/falcon-7b/out/lora/alpaca\n",
        "!cp out/lora/alpaca/iter-09*.pth /contents/drive/MyDrive/Colab\\ Notebooks/ViT/falcon-7b/out/lora/alpaca\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-6I26wK44Ze",
        "outputId": "0e7aff09-db11-4bbd-d799-679c6b198f86"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: target '/contents/drive/MyDrive/Colab Notebooks/ViT/falcon-7b/checkpoints/*.json' is not a directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reference lora_baselins.py"
      ],
      "metadata": {
        "id": "0rB7WBoNiqDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# experimental setup to search new baseline\n",
        "expr = {\n",
        "    'max_iters'        : 50000,        # ~2h CP\n",
        "    'resume'           : True,         # auto resume from out_dir intermediate checkpoints (i.e. out/lora/alpaca/*.)\n",
        "    'precision'        : 'bf16-true',  # reduce GPU peak memory < 24GB\n",
        "    'micro_batch_size' : 2,            # reduce GPU peak memory < 24GB\n",
        "}\n"
      ],
      "metadata": {
        "id": "EnugUFekP57C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "\n",
        "\n",
        "import lightning as L\n",
        "import torch\n",
        "from lightning.fabric.strategies import FSDPStrategy, XLAStrategy\n",
        "\n",
        "# support running without installing as a package\n",
        "wd = Path(__file__).parent.parent.resolve()\n",
        "sys.path.append(str(wd))\n",
        "\n",
        "from generate.base import generate\n",
        "from lit_gpt.lora import GPT, Block, Config, lora_filter, mark_only_lora_as_trainable\n",
        "from lit_gpt.speed_monitor import SpeedMonitorFabric as SpeedMonitor\n",
        "from lit_gpt.speed_monitor import estimate_flops, measure_flops\n",
        "from lit_gpt.tokenizer import Tokenizer\n",
        "from lit_gpt.utils import check_valid_checkpoint_dir, chunked_cross_entropy, lazy_load, num_parameters, step_csv_logger\n",
        "from scripts.prepare_alpaca import generate_prompt\n",
        "\n",
        "eval_interval = 50\n",
        "save_interval = 100\n",
        "eval_iters = 100\n",
        "log_interval = 100\n",
        "devices = 1\n",
        "# change this value to force a maximum sequence length\n",
        "override_max_seq_length = 100\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 3e-4\n",
        "batch_size = 128\n",
        "micro_batch_size = expr['micro_batch_size']\n",
        "gradient_accumulation_iters = batch_size // micro_batch_size\n",
        "assert gradient_accumulation_iters > 0\n",
        "max_iters = expr['max_iters']  # train dataset size\n",
        "weight_decay = 0.01\n",
        "lora_r = 8\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.05\n",
        "lora_query = True\n",
        "lora_key = False\n",
        "lora_value = True\n",
        "lora_projection = False\n",
        "lora_mlp = False\n",
        "lora_head = False\n",
        "warmup_steps = 100\n",
        "\n",
        "hparams = {k: v for k, v in locals().items() if isinstance(v, (int, float, str)) and not k.startswith(\"_\")}\n",
        "\n",
        "\n",
        "def setup(\n",
        "    data_dir: Path = Path(\"data/alpaca\"),\n",
        "    checkpoint_dir: Path = Path(\"checkpoints/tiiuae/falcon-7b\"),\n",
        "    out_dir: Path = Path(\"out/lora/alpaca\"),\n",
        "    precision: Optional[str] = None,\n",
        "    tpu: bool = False,\n",
        "    resume: Union[bool, Path] = False,\n",
        "):\n",
        "    precision = expr['precision']\n",
        "    resume = expr['resume']\n",
        "\n",
        "    if precision is None:\n",
        "        precision = \"32-true\" if tpu else \"bf16-mixed\"\n",
        "    fabric_devices = devices\n",
        "    if fabric_devices > 1:\n",
        "        if tpu:\n",
        "            # For multi-host TPU training, the device count for Fabric is limited to the count on a single host.\n",
        "            fabric_devices = \"auto\"\n",
        "            strategy = XLAStrategy(sync_module_states=False)\n",
        "        else:\n",
        "            strategy = FSDPStrategy(\n",
        "                auto_wrap_policy={Block},\n",
        "                activation_checkpointing_policy={Block},\n",
        "                state_dict_type=\"full\",\n",
        "                limit_all_gathers=True,\n",
        "            )\n",
        "    else:\n",
        "        strategy = \"auto\"\n",
        "\n",
        "    logger = step_csv_logger(out_dir.parent, out_dir.name, flush_logs_every_n_steps=log_interval)\n",
        "    fabric = L.Fabric(devices=fabric_devices, strategy=strategy, precision=precision, loggers=logger)\n",
        "    fabric.print(hparams)\n",
        "    fabric.launch(main, data_dir, checkpoint_dir, out_dir, resume)\n",
        "\n",
        "\n",
        "def main(fabric: L.Fabric, data_dir: Path, checkpoint_dir: Path, out_dir: Path, resume: bool):\n",
        "    check_valid_checkpoint_dir(checkpoint_dir)\n",
        "\n",
        "    speed_monitor = SpeedMonitor(fabric, window_size=50, time_unit=\"seconds\")\n",
        "\n",
        "    fabric.seed_everything(1337)  # same seed for every process to init model (FSDP)\n",
        "\n",
        "    if fabric.global_rank == 0:\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    train_data = torch.load(data_dir / \"train.pt\")\n",
        "    val_data = torch.load(data_dir / \"test.pt\")\n",
        "\n",
        "    if not any((lora_query, lora_key, lora_value, lora_projection, lora_mlp, lora_head)):\n",
        "        fabric.print(\"Warning: all LoRA layers are disabled!\")\n",
        "    config = Config.from_name(\n",
        "        name=checkpoint_dir.name,\n",
        "        r=lora_r,\n",
        "        alpha=lora_alpha,\n",
        "        dropout=lora_dropout,\n",
        "        to_query=lora_query,\n",
        "        to_key=lora_key,\n",
        "        to_value=lora_value,\n",
        "        to_projection=lora_projection,\n",
        "        to_mlp=lora_mlp,\n",
        "        to_head=lora_head,\n",
        "    )\n",
        "    checkpoint_path = checkpoint_dir / \"lit_model.pth\"\n",
        "    fabric.print(f\"Loading model {str(checkpoint_path)!r} with {config.__dict__}\")\n",
        "    with fabric.init_module(empty_init=False):\n",
        "        model = GPT(config)\n",
        "        model.apply(model._init_weights)  # for the LoRA weights\n",
        "    with lazy_load(checkpoint_path) as checkpoint:\n",
        "        # strict=False because missing keys due to LoRA weights not contained in state dict\n",
        "        model.load_state_dict(checkpoint, strict=False)\n",
        "\n",
        "    mark_only_lora_as_trainable(model)\n",
        "\n",
        "    fabric.print(f\"Number of trainable parameters: {num_parameters(model, requires_grad=True):,}\")\n",
        "    fabric.print(f\"Number of non trainable parameters: {num_parameters(model, requires_grad=False):,}\")\n",
        "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "    optimizer = torch.optim.AdamW(trainable_params, lr=learning_rate, weight_decay=weight_decay)\n",
        "    model, optimizer = fabric.setup(model, optimizer)\n",
        "\n",
        "    fabric.seed_everything(1337 + fabric.global_rank)\n",
        "\n",
        "    state = {\"model\": model, \"optimizer\": optimizer, \"hparams\": hparams, \"iter_num\": 0, \"step_count\": 0}\n",
        "    if resume is True:\n",
        "        resume = sorted(out_dir.glob(\"*.pth\"))[-1]\n",
        "    if resume:\n",
        "        fabric.print(f\"Resuming training from {resume}\")\n",
        "        fabric.load(resume, state)\n",
        "\n",
        "    train_time = time.time()\n",
        "    train(fabric, state, train_data, val_data, checkpoint_dir, out_dir, speed_monitor)\n",
        "    #train(fabric, model, optimizer, train_data, val_data, checkpoint_dir, out_dir, speed_monitor)\n",
        "    fabric.print(f\"Training time: {(time.time()-train_time):.2f}s\")\n",
        "\n",
        "    # Save the final LoRA checkpoint at the end of training\n",
        "    save_path = out_dir / \"lit_model_lora_finetuned.pth\"\n",
        "    save_lora_checkpoint(fabric, model, save_path)\n",
        "\n",
        "\n",
        "def train(\n",
        "    fabric: L.Fabric,\n",
        "    state: Dict,\n",
        "    # model: GPT,\n",
        "    # optimizer: torch.optim.Optimizer,\n",
        "    train_data: List[Dict],\n",
        "    val_data: List[Dict],\n",
        "    checkpoint_dir: Path,\n",
        "    out_dir: Path,\n",
        "    speed_monitor: SpeedMonitor,\n",
        ") -> None:\n",
        "    tokenizer = Tokenizer(checkpoint_dir)\n",
        "    max_seq_length, longest_seq_length, longest_seq_ix = get_max_seq_length(train_data)\n",
        "\n",
        "    # loaded\n",
        "    model = state[\"model\"]\n",
        "    optimizer = state[\"optimizer\"]\n",
        "\n",
        "    validate(fabric, model, val_data, tokenizer, longest_seq_length)  # sanity check\n",
        "\n",
        "    with torch.device(\"meta\"):\n",
        "        meta_model = GPT(model.config)\n",
        "        # estimated is too much of an optimistic estimate, left just for reference\n",
        "        estimated_flops = estimate_flops(meta_model) * micro_batch_size\n",
        "        fabric.print(f\"Estimated TFLOPs: {estimated_flops * fabric.world_size / 1e12:.2f}\")\n",
        "        x = torch.randint(0, 1, (micro_batch_size, model.config.block_size))\n",
        "        measured_flops = measure_flops(meta_model, x)\n",
        "        fabric.print(f\"Measured TFLOPs: {measured_flops * fabric.world_size / 1e12:.2f}\")\n",
        "        del meta_model, x\n",
        "\n",
        "    step_count = 0\n",
        "    total_lengths = 0\n",
        "    total_t0 = time.time()\n",
        "\n",
        "    if fabric.device.type == \"xla\":\n",
        "        import torch_xla.core.xla_model as xm\n",
        "\n",
        "        xm.mark_step()\n",
        "    for iter_num in range(max_iters):\n",
        "        if step_count <= warmup_steps:\n",
        "            # linear warmup\n",
        "            lr = learning_rate * step_count / warmup_steps\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group[\"lr\"] = lr\n",
        "\n",
        "        iter_t0 = time.time()\n",
        "\n",
        "        input_ids, targets = get_batch(\n",
        "            fabric, train_data, longest_seq_length, longest_seq_ix if iter_num == 0 else None\n",
        "        )\n",
        "\n",
        "        is_accumulating = (iter_num + 1) % gradient_accumulation_iters != 0\n",
        "        with fabric.no_backward_sync(model, enabled=is_accumulating):\n",
        "            logits = model(input_ids, max_seq_length=max_seq_length, lm_head_chunk_size=128)\n",
        "            # shift the targets such that output n predicts token n+1\n",
        "            logits[-1] = logits[-1][..., :-1, :]\n",
        "            loss = chunked_cross_entropy(logits, targets[..., 1:])\n",
        "            fabric.backward(loss / gradient_accumulation_iters)\n",
        "\n",
        "        if not is_accumulating:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            step_count += 1\n",
        "        elif fabric.device.type == \"xla\":\n",
        "            xm.mark_step()\n",
        "\n",
        "        t1 = time.time()\n",
        "        total_lengths += input_ids.size(1)\n",
        "        speed_monitor.on_train_batch_end(\n",
        "            (iter_num + 1) * micro_batch_size,\n",
        "            t1 - total_t0,\n",
        "            # this assumes that device FLOPs are the same and that all devices have the same batch size\n",
        "            fabric.world_size,\n",
        "            flops_per_batch=measured_flops,\n",
        "            lengths=total_lengths,\n",
        "        )\n",
        "        if iter_num % log_interval == 0:\n",
        "            elapsed = t1 - total_t0\n",
        "            fabric.print(\n",
        "                    f\"iter {iter_num} step {step_count}: loss {loss.item():.4f},  time:{elapsed/60:.1f}mins  iter time:\"\n",
        "                f\" {(t1 - iter_t0) * 1000:.2f}ms{' (optimizer.step)' if not is_accumulating else ''}\"\n",
        "            )\n",
        "\n",
        "        if not is_accumulating and step_count % eval_interval == 0:\n",
        "            t0 = time.time()\n",
        "            val_loss = validate(fabric, model, val_data, tokenizer, longest_seq_length)\n",
        "            t1 = time.time() - t0\n",
        "            speed_monitor.eval_end(t1)\n",
        "            fabric.print(f\"step {iter_num}: val loss {val_loss:.4f}, val time: {t1 * 1000:.2f}ms\")\n",
        "            fabric.barrier()\n",
        "        if not is_accumulating and step_count % save_interval == 0:\n",
        "            checkpoint_path = out_dir / f\"iter-{iter_num:06d}-ckpt.pth\"\n",
        "            save_lora_checkpoint(fabric, model, checkpoint_path)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(\n",
        "    fabric: L.Fabric, model: GPT, val_data: List[Dict], tokenizer: Tokenizer, longest_seq_length: int\n",
        ") -> torch.Tensor:\n",
        "    fabric.print(\"Validating ...\")\n",
        "    model.eval()\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "        input_ids, targets = get_batch(fabric, val_data, longest_seq_length)\n",
        "        logits = model(input_ids)\n",
        "        loss = chunked_cross_entropy(logits, targets, chunk_size=0)\n",
        "        losses[k] = loss.item()\n",
        "    val_loss = losses.mean()\n",
        "\n",
        "    # produce an example:\n",
        "    instruction = \"Recommend a movie for me to watch during the weekend and explain the reason.\"\n",
        "    fabric.print(instruction)\n",
        "    sample = {\"instruction\": instruction, \"input\": \"\"}\n",
        "    prompt = generate_prompt(sample)\n",
        "    encoded = tokenizer.encode(prompt, device=fabric.device)\n",
        "    max_returned_tokens = len(encoded) + 100\n",
        "    output = generate(\n",
        "        model, idx=encoded, max_returned_tokens=max_returned_tokens, max_seq_length=max_returned_tokens, temperature=0.8\n",
        "    )\n",
        "    output = tokenizer.decode(output)\n",
        "    fabric.print(output)\n",
        "\n",
        "    model.reset_cache()\n",
        "\n",
        "    model.train()\n",
        "    return val_loss.item()\n",
        "\n",
        "\n",
        "def get_batch(\n",
        "    fabric: L.Fabric, data: List[Dict], longest_seq_length: int, longest_seq_ix: Optional[int] = None\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    ix = torch.randint(len(data), (micro_batch_size,))\n",
        "    if longest_seq_ix is not None:\n",
        "        # force the longest sample at the beginning so potential OOMs happen right away\n",
        "        ix[0] = longest_seq_ix\n",
        "\n",
        "    input_ids = [data[i][\"input_ids\"].type(torch.int64) for i in ix]\n",
        "    labels = [data[i][\"labels\"].type(torch.int64) for i in ix]\n",
        "\n",
        "    # it's better to pad to a fixed seq length with XLA to avoid recompilation\n",
        "    max_len = max(len(s) for s in input_ids) if fabric.device.type != \"xla\" else longest_seq_length\n",
        "\n",
        "    def pad_right(x, pad_id):\n",
        "        # pad right based on the longest sequence\n",
        "        n = max_len - len(x)\n",
        "        return torch.cat((x, torch.full((n,), pad_id, dtype=x.dtype)))\n",
        "\n",
        "    x = torch.stack([pad_right(x, pad_id=0) for x in input_ids])\n",
        "    y = torch.stack([pad_right(x, pad_id=-1) for x in labels])\n",
        "\n",
        "    if fabric.device.type == \"cuda\" and x.device.type == \"cpu\":\n",
        "        x, y = fabric.to_device((x.pin_memory(), y.pin_memory()))\n",
        "    else:\n",
        "        x, y = fabric.to_device((x, y))\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def get_max_seq_length(data: List[Dict]) -> Tuple[int, int, int]:\n",
        "    # find out the minimum max_seq_length required during fine-tuning (saves memory!)\n",
        "    lengths = [len(d[\"input_ids\"]) for d in data]\n",
        "    max_seq_length = max(lengths)\n",
        "    longest_seq_ix = lengths.index(max_seq_length)\n",
        "    # support easy override at the top of the file\n",
        "    return (\n",
        "        override_max_seq_length if isinstance(override_max_seq_length, int) else max_seq_length,\n",
        "        max_seq_length,\n",
        "        longest_seq_ix,\n",
        "    )\n",
        "\n",
        "\n",
        "def save_lora_checkpoint(fabric, model, file_path: Path):\n",
        "    fabric.print(f\"Saving LoRA weights to {str(file_path)!r}\")\n",
        "    fabric.save(file_path, {\"model\": model}, filter={\"model\": lora_filter})\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Uncomment this line if you see an error: \"Expected is_sm80 to be true, but got false\"\n",
        "    # torch.backends.cuda.enable_flash_sdp(False)\n",
        "    torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "    from jsonargparse import CLI\n",
        "\n",
        "    CLI(setup)"
      ],
      "metadata": {
        "id": "rba_Qp3DKaIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bkjzQqqiTq7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Appendix - Fixes, and Issue Resolutions\n",
        "\n",
        "## Unsolved\n",
        "* automate pytorch audio uninstall as is interactive (prompts for Y)\n",
        "* auto save intermediate checkpoints to GDrive automatically to allow shutting down run-time anytime\n",
        "* test moving checkpoints from/to GDrive to/from Lambda + ColabPro\n",
        "\n",
        "## Solved\n",
        "* uninstall pytorch audio - nigtly version of pytorch with cuda conflicts with pytorch audio and cannot integrate with lit-gpt\n",
        "* CUDA OOM - reduce micro_batch_size to 2, use bfp16 to avoid pytorch memory peak spikes and keep < 24GB VRAM\n",
        "* max_iters = 50k takes 2 GPU/hrs training, and insufficient for natural-english-like baseline\n",
        "* patch lora.py with above workarounds - rename lora_compact.py\n",
        "* integrate notebook for GPU server - hard to experiment and fine new baseline, make easy to customize fine-turning for A100-40GB on GPU server\n",
        "* auto-resume: for cost reduction of fine-turning or mixing resuming checkpoints on different GPU servers\n",
        "\n"
      ],
      "metadata": {
        "id": "7ih2CqU5TlyM"
      }
    }
  ]
}