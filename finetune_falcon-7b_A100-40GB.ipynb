{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Dolomite-Baseline: for fine-tuning Falcon-7b over A100-40GB\n",
        "\n",
        "* repo setup\n",
        "* checkpoint download - tiiuae/falcon-7b\n",
        "* inference test\n",
        "* finetune: reproduce stable baseline 50k_iters  \n",
        "* finetune: search new baseline with auto-checkpoints and longer training\n",
        "\n"
      ],
      "metadata": {
        "id": "6oiFo0vOJfoz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vV9rRJOUqHe5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "548b97be-ec18-473a-e414-868dd0cf1465"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Aug 13 22:40:25 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    44W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phase 1. Setup repo, install dependencies, download model weights"
      ],
      "metadata": {
        "id": "DYhDOvo-tz4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Lightning-AI/lit-gpt\n",
        "!git clone https://github.com/alicata/llm-dolomite-base.git\n",
        "!cp llm-dolomite-base/lora_* lit-gpt/finetune\n",
        "\n",
        "%cd lit-gpt\n",
        "\n",
        "# Main repo with CUDA and flash attention\n",
        "!pip install --index-url https://download.pytorch.org/whl/nightly/cu118 --pre 'torch>=2.1.0dev' -q\n",
        "# Remove audio dependency - integration bug\n",
        "!pip uninstall -y torchaudio\n",
        "# install the dependencies\n",
        "!pip install -r requirements.txt\n",
        "!pip install huggingface_hub\n",
        "\n",
        "# download the original Falcon-7B weights and convert into Lit-GPT compatible model format\n",
        "!python scripts/download.py --repo_id tiiuae/falcon-7b\n",
        "!python scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/tiiuae/falcon-7b"
      ],
      "metadata": {
        "id": "-VFKqx5Xt0gm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c58d1c42-6091-4610-ef69-98d800457b7b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lit-gpt'...\n",
            "remote: Enumerating objects: 3254, done.\u001b[K\n",
            "remote: Counting objects: 100% (265/265), done.\u001b[K\n",
            "remote: Compressing objects: 100% (167/167), done.\u001b[K\n",
            "remote: Total 3254 (delta 151), reused 184 (delta 93), pack-reused 2989\u001b[K\n",
            "Receiving objects: 100% (3254/3254), 1005.33 KiB | 3.84 MiB/s, done.\n",
            "Resolving deltas: 100% (2183/2183), done.\n",
            "Cloning into 'llm-dolomite-base'...\n",
            "remote: Enumerating objects: 89, done.\u001b[K\n",
            "remote: Counting objects: 100% (89/89), done.\u001b[K\n",
            "remote: Compressing objects: 100% (80/80), done.\u001b[K\n",
            "remote: Total 89 (delta 40), reused 4 (delta 1), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (89/89), 15.59 MiB | 5.52 MiB/s, done.\n",
            "Resolving deltas: 100% (40/40), done.\n",
            "/content/lit-gpt\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m843.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.1.0.dev20230813+cu118 which is incompatible.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 2.1.0.dev20230813+cu118 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 2.1.0.dev20230813+cu118 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 2.1.0.dev20230813+cu118 which is incompatible.\n",
            "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 2.1.0.dev20230813+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mFound existing installation: torchaudio 2.0.2+cu118\n",
            "Uninstalling torchaudio-2.0.2+cu118:\n",
            "  Successfully uninstalled torchaudio-2.0.2+cu118\n",
            "Collecting lightning@ git+https://github.com/Lightning-AI/lightning@master (from -r requirements.txt (line 2))\n",
            "  Cloning https://github.com/Lightning-AI/lightning (to revision master) to /tmp/pip-install-qjqytb77/lightning_423ca1eabd02415f838f41432c790563\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Lightning-AI/lightning /tmp/pip-install-qjqytb77/lightning_423ca1eabd02415f838f41432c790563\n",
            "  Resolved https://github.com/Lightning-AI/lightning to commit 3142ed5e4403d5c4cae26662e52e0d6a5d3668db\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Encountered 31 file(s) that should have been pointers, but weren't:\n",
            "        .notebooks/course_UvA-DL/01-introduction-to-pytorch.ipynb\n",
            "        .notebooks/course_UvA-DL/02-activation-functions.ipynb\n",
            "        .notebooks/course_UvA-DL/03-initialization-and-optimization.ipynb\n",
            "        .notebooks/course_UvA-DL/04-inception-resnet-densenet.ipynb\n",
            "        .notebooks/course_UvA-DL/05-transformers-and-MH-attention.ipynb\n",
            "        .notebooks/course_UvA-DL/06-graph-neural-networks.ipynb\n",
            "        .notebooks/course_UvA-DL/07-deep-energy-based-generative-models.ipynb\n",
            "        .notebooks/course_UvA-DL/08-deep-autoencoders.ipynb\n",
            "        .notebooks/course_UvA-DL/09-normalizing-flows.ipynb\n",
            "        .notebooks/course_UvA-DL/10-autoregressive-image-modeling.ipynb\n",
            "        .notebooks/course_UvA-DL/11-vision-transformer.ipynb\n",
            "        .notebooks/course_UvA-DL/12-meta-learning.ipynb\n",
            "        .notebooks/course_UvA-DL/13-contrastive-learning.ipynb\n",
            "        .notebooks/flash_tutorials/electricity_forecasting.ipynb\n",
            "        .notebooks/flash_tutorials/image_classification.ipynb\n",
            "        .notebooks/flash_tutorials/tabular_classification.ipynb\n",
            "        .notebooks/flash_tutorials/text_classification.ipynb\n",
            "        .notebooks/lightning_examples/augmentation_kornia.ipynb\n",
            "        .notebooks/lightning_examples/barlow-twins.ipynb\n",
            "        .notebooks/lightning_examples/basic-gan.ipynb\n",
            "        .notebooks/lightning_examples/cifar10-baseline.ipynb\n",
            "        .notebooks/lightning_examples/datamodules.ipynb\n",
            "        .notebooks/lightning_examples/finetuning-scheduler.ipynb\n",
            "        .notebooks/lightning_examples/mnist-hello-world.ipynb\n",
            "        .notebooks/lightning_examples/mnist-tpu-training.ipynb\n",
            "        .notebooks/lightning_examples/reinforce-learning-DQN.ipynb\n",
            "        .notebooks/lightning_examples/text-transformers.ipynb\n",
            "        .notebooks/lightning_examples/warp-drive.ipynb\n",
            "        .notebooks/templates/img-classify.ipynb\n",
            "        .notebooks/templates/simple.ipynb\n",
            "        .notebooks/templates/titanic.ipynb\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tokenizers (from -r requirements.txt (line 3))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonargparse[signatures] (from -r requirements.txt (line 4))\n",
            "  Downloading jsonargparse-4.23.1-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes>=0.40.0 (from -r requirements.txt (line 5))\n",
            "  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.10.1)\n",
            "Collecting datasets (from -r requirements.txt (line 7))\n",
            "  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zstandard (from -r requirements.txt (line 8))\n",
            "  Downloading zstandard-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Jinja2<5.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (3.1.2)\n",
            "Requirement already satisfied: PyYAML<8.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (6.0.1)\n",
            "Collecting arrow<3.0,>=1.2.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading arrow-1.2.3-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff<4.0,>=2.2.1 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: beautifulsoup4<6.0,>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (4.11.2)\n",
            "Requirement already satisfied: click<10.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (8.1.6)\n",
            "Collecting croniter<1.5.0,>=1.3.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading croniter-1.4.1-py2.py3-none-any.whl (19 kB)\n",
            "Collecting dateutils<2.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading dateutils-0.6.12-py2.py3-none-any.whl (5.7 kB)\n",
            "Collecting deepdiff<8.0,>=5.7.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading deepdiff-6.3.1-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.7/70.7 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi<2.0,>=0.92.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading fastapi-0.101.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec<2025.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (2023.6.0)\n",
            "Collecting inquirer<5.0,>=2.10.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading inquirer-3.1.3-py3-none-any.whl (18 kB)\n",
            "Collecting lightning-cloud>=0.5.37 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading lightning_cloud-0.5.37-py3-none-any.whl (596 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.7/596.7 kB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightning-utilities<2.0,>=0.8.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (23.1)\n",
            "Requirement already satisfied: psutil<7.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (5.9.5)\n",
            "Requirement already satisfied: pydantic<2.2.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (2.1.1)\n",
            "Collecting python-multipart<2.0,>=0.0.5 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<4.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: rich<15.0,>=12.3.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (13.5.2)\n",
            "Collecting starlette (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading starlette-0.31.0-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.8/69.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starsessions<2.0,>=1.2.1 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading starsessions-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: torch<4.0,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (2.1.0.dev20230813+cu118)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading torchmetrics-1.0.3-py3-none-any.whl (731 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.6/731.6 kB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (4.66.0)\n",
            "Requirement already satisfied: traitlets<7.0,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (5.7.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (4.7.1)\n",
            "Requirement already satisfied: urllib3<4.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (2.0.4)\n",
            "Collecting uvicorn<2.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: websocket-client<3.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (1.6.1)\n",
            "Collecting websockets<13.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading pytorch_lightning-2.0.6-py3-none-any.whl (722 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m722.8/722.8 kB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docstring-parser>=0.15 (from jsonargparse[signatures]->-r requirements.txt (line 4))\n",
            "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
            "Collecting typeshed-client>=2.1.0 (from jsonargparse[signatures]->-r requirements.txt (line 4))\n",
            "  Downloading typeshed_client-2.3.0-py3-none-any.whl (581 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.6/581.6 kB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 7)) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets->-r requirements.txt (line 7))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 7)) (1.5.3)\n",
            "Collecting xxhash (from datasets->-r requirements.txt (line 7))\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets->-r requirements.txt (line 7))\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 7)) (3.8.5)\n",
            "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets->-r requirements.txt (line 7))\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from arrow<3.0,>=1.2.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<6.0,>=4.8.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (2.4.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from dateutils<2.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (2023.3)\n",
            "Collecting ordered-set<4.2.0,>=4.0.2 (from deepdiff<8.0,>=5.7.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Collecting starlette (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets->-r requirements.txt (line 7)) (3.12.2)\n",
            "Collecting blessed>=1.19.0 (from inquirer<5.0,>=2.10.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-editor>=1.0.4 (from inquirer<5.0,>=2.10.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
            "Collecting readchar>=3.0.6 (from inquirer<5.0,>=2.10.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading readchar-4.0.5-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<5.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (2.1.3)\n",
            "Requirement already satisfied: pyjwt in /usr/lib/python3/dist-packages (from lightning-cloud>=0.5.37->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (2.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from lightning-cloud>=0.5.37->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.2.0,>=1.7.4->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.2.0,>=1.7.4->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (2.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<4.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<4.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (2023.7.22)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<15.0,>=12.3.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<15.0,>=12.3.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (2.16.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (3.7.1)\n",
            "Requirement already satisfied: itsdangerous<3.0.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from starsessions<2.0,>=1.2.1->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (2.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.11.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.11.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (3.1)\n",
            "Requirement already satisfied: pytorch-triton==2.1.0+e6216047b8 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.11.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (2.1.0+e6216047b8)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from typeshed-client>=2.1.0->jsonargparse[signatures]->-r requirements.txt (line 4)) (6.0.1)\n",
            "Collecting h11>=0.8 (from uvicorn<2.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (1.1.2)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.19.0->inquirer<5.0,>=2.10.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (0.2.6)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<15.0,>=12.3.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (0.1.2)\n",
            "Requirement already satisfied: setuptools>=41.0 in /usr/local/lib/python3.10/dist-packages (from readchar>=3.0.6->inquirer<5.0,>=2.10.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (67.7.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=1.11.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 2)) (1.3.0)\n",
            "Building wheels for collected packages: lightning\n",
            "  Building wheel for lightning (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lightning: filename=lightning-2.1.0.dev0-py3-none-any.whl size=1887293 sha256=697aae7c8927648e181ed81a85c4e17a5bccdc85717fb10acf00ff90286dfd21\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-kquvldx6/wheels/b9/92/07/634ec381ab7d682d3afdcf943d0a6604881441ff2d6c409103\n",
            "Successfully built lightning\n",
            "Installing collected packages: tokenizers, python-editor, bitsandbytes, zstandard, xxhash, websockets, typeshed-client, readchar, python-multipart, ordered-set, lightning-utilities, jsonargparse, h11, docstring-parser, dill, blessed, backoff, uvicorn, starlette, multiprocess, inquirer, huggingface-hub, deepdiff, dateutils, croniter, arrow, torchmetrics, starsessions, fastapi, pytorch-lightning, lightning-cloud, datasets, lightning\n",
            "Successfully installed arrow-1.2.3 backoff-2.2.1 bitsandbytes-0.41.1 blessed-1.20.0 croniter-1.4.1 datasets-2.14.4 dateutils-0.6.12 deepdiff-6.3.1 dill-0.3.7 docstring-parser-0.15 fastapi-0.101.0 h11-0.14.0 huggingface-hub-0.16.4 inquirer-3.1.3 jsonargparse-4.23.1 lightning-2.1.0.dev0 lightning-cloud-0.5.37 lightning-utilities-0.9.0 multiprocess-0.70.15 ordered-set-4.1.0 python-editor-1.0.4 python-multipart-0.0.6 pytorch-lightning-2.0.6 readchar-4.0.5 starlette-0.27.0 starsessions-1.3.0 tokenizers-0.13.3 torchmetrics-1.0.3 typeshed-client-2.3.0 uvicorn-0.23.2 websockets-11.0.3 xxhash-3.3.0 zstandard-0.21.0\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.16.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.7.22)\n",
            "/usr/local/lib/python3.10/dist-packages/pydantic/_migration.py:283: UserWarning: `pydantic.utils:Representation` has been removed. We are importing from `pydantic.v1.utils:Representation` instead.See the migration guide for more details: https://docs.pydantic.dev/latest/migration/\n",
            "  warnings.warn(\n",
            "Fetching 5 files:   0% 0/5 [00:00<?, ?it/s]\n",
            "Downloading (…)okenizer_config.json: 100% 220/220 [00:00<00:00, 1.86MB/s]\n",
            "\n",
            "Downloading (…)d4eb1/tokenizer.json:   0% 0.00/2.73M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   0% 0.00/4.48G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)model.bin.index.json: 100% 16.9k/16.9k [00:00<00:00, 82.3MB/s]\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:   0% 0.00/9.95G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:   0% 21.0M/9.95G [00:00<01:05, 152MB/s]\u001b[A\u001b[A\u001b[A\n",
            "Downloading (…)d4eb1/tokenizer.json: 100% 2.73M/2.73M [00:00<00:00, 8.19MB/s]\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:   1% 52.4M/9.95G [00:00<00:45, 218MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:   1% 83.9M/9.95G [00:00<00:39, 249MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   0% 10.5M/4.48G [00:00<03:05, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:   1% 115M/9.95G [00:00<00:38, 258MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:   1% 147M/9.95G [00:00<00:36, 268MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:   2% 178M/9.95G [00:00<00:35, 273MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:   2% 210M/9.95G [00:00<00:35, 278MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   0% 21.0M/4.48G [00:00<03:08, 23.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:   2% 241M/9.95G [00:00<00:34, 278MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:   3% 273M/9.95G [00:01<00:34, 283MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:   3% 304M/9.95G [00:01<00:33, 287MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:   3% 336M/9.95G [00:01<00:34, 278MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   1% 31.5M/4.48G [00:01<03:13, 23.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:   4% 367M/9.95G [00:01<00:34, 281MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:   4% 398M/9.95G [00:01<00:33, 286MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:   4% 430M/9.95G [00:01<00:33, 288MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:   5% 461M/9.95G [00:01<00:32, 290MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:   5% 493M/9.95G [00:01<00:32, 290MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   1% 41.9M/4.48G [00:01<03:15, 22.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:   5% 524M/9.95G [00:01<00:32, 290MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:   6% 556M/9.95G [00:02<00:32, 292MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:   6% 587M/9.95G [00:02<00:32, 292MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:   6% 619M/9.95G [00:02<00:31, 293MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:   7% 650M/9.95G [00:02<00:31, 294MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   1% 52.4M/4.48G [00:02<03:24, 21.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:   7% 682M/9.95G [00:02<00:31, 290MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:   7% 713M/9.95G [00:02<00:31, 292MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:   7% 744M/9.95G [00:02<00:31, 289MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:   8% 776M/9.95G [00:02<00:31, 292MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   1% 62.9M/4.48G [00:02<03:20, 22.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:   8% 807M/9.95G [00:02<00:31, 292MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:   8% 839M/9.95G [00:02<00:31, 291MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:   9% 870M/9.95G [00:03<00:31, 292MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:   9% 902M/9.95G [00:03<00:31, 290MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:   9% 933M/9.95G [00:03<00:31, 282MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   2% 73.4M/4.48G [00:03<03:28, 21.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  10% 965M/9.95G [00:03<00:31, 283MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  10% 996M/9.95G [00:03<00:31, 287MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  10% 1.03G/9.95G [00:03<00:31, 286MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  11% 1.06G/9.95G [00:03<00:31, 287MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  11% 1.09G/9.95G [00:03<00:30, 287MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   2% 83.9M/4.48G [00:03<03:48, 19.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  11% 1.12G/9.95G [00:03<00:32, 273MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  12% 1.15G/9.95G [00:04<00:33, 263MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  12% 1.18G/9.95G [00:04<00:34, 254MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  12% 1.22G/9.95G [00:04<00:35, 248MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  13% 1.25G/9.95G [00:04<00:37, 235MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   2% 94.4M/4.48G [00:04<03:49, 19.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  13% 1.28G/9.95G [00:04<00:37, 234MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  13% 1.31G/9.95G [00:04<00:36, 237MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  13% 1.34G/9.95G [00:04<00:36, 237MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   2% 105M/4.48G [00:04<03:34, 20.4MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  14% 1.37G/9.95G [00:05<00:36, 238MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  14% 1.41G/9.95G [00:05<00:35, 240MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  14% 1.44G/9.95G [00:05<00:34, 246MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  15% 1.47G/9.95G [00:05<00:34, 244MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  15% 1.50G/9.95G [00:05<00:34, 243MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   3% 115M/4.48G [00:05<03:45, 19.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  15% 1.53G/9.95G [00:05<00:34, 243MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  16% 1.56G/9.95G [00:05<00:34, 246MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  16% 1.59G/9.95G [00:05<00:34, 245MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   3% 126M/4.48G [00:05<03:26, 21.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  16% 1.63G/9.95G [00:06<00:33, 246MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  17% 1.66G/9.95G [00:06<00:34, 242MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  17% 1.69G/9.95G [00:06<00:34, 242MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  17% 1.72G/9.95G [00:06<00:34, 239MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   3% 136M/4.48G [00:06<03:37, 20.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  18% 1.75G/9.95G [00:06<00:34, 238MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  18% 1.78G/9.95G [00:06<00:34, 236MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  18% 1.81G/9.95G [00:06<00:33, 240MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   3% 147M/4.48G [00:06<03:14, 22.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  19% 1.85G/9.95G [00:07<00:33, 240MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  19% 1.88G/9.95G [00:07<00:33, 243MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   4% 157M/4.48G [00:07<02:54, 24.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  19% 1.91G/9.95G [00:07<00:33, 240MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  19% 1.94G/9.95G [00:07<00:32, 245MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  20% 1.97G/9.95G [00:07<00:32, 245MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  20% 2.00G/9.95G [00:07<00:32, 248MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   4% 168M/4.48G [00:07<03:06, 23.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  20% 2.03G/9.95G [00:07<00:32, 246MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  21% 2.07G/9.95G [00:07<00:31, 251MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  21% 2.10G/9.95G [00:08<00:29, 263MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  21% 2.13G/9.95G [00:08<00:28, 272MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  22% 2.16G/9.95G [00:08<00:28, 278MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   4% 178M/4.48G [00:08<03:17, 21.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  22% 2.19G/9.95G [00:08<00:27, 280MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  22% 2.22G/9.95G [00:08<00:27, 284MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  23% 2.25G/9.95G [00:08<00:26, 285MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  23% 2.29G/9.95G [00:08<00:26, 286MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   4% 189M/4.48G [00:08<03:12, 22.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  23% 2.32G/9.95G [00:08<00:26, 286MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  24% 2.35G/9.95G [00:08<00:26, 287MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  24% 2.38G/9.95G [00:08<00:26, 288MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   4% 199M/4.48G [00:09<02:51, 24.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  24% 2.41G/9.95G [00:09<00:26, 285MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  25% 2.44G/9.95G [00:09<00:26, 285MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  25% 2.47G/9.95G [00:09<00:25, 288MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  25% 2.51G/9.95G [00:09<00:25, 290MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  26% 2.54G/9.95G [00:09<00:25, 291MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   5% 210M/4.48G [00:09<03:11, 22.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  26% 2.57G/9.95G [00:09<00:25, 291MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  26% 2.60G/9.95G [00:09<00:25, 290MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  26% 2.63G/9.95G [00:09<00:25, 291MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  27% 2.66G/9.95G [00:09<00:24, 292MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  27% 2.69G/9.95G [00:10<00:25, 290MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  27% 2.73G/9.95G [00:10<00:24, 291MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   5% 220M/4.48G [00:10<03:22, 21.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  28% 2.76G/9.95G [00:10<00:24, 289MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  28% 2.79G/9.95G [00:10<00:24, 290MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  28% 2.82G/9.95G [00:10<00:24, 290MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  29% 2.85G/9.95G [00:10<00:24, 290MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   5% 231M/4.48G [00:10<03:25, 20.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  29% 2.88G/9.95G [00:10<00:24, 291MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  29% 2.92G/9.95G [00:10<00:24, 291MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  30% 2.95G/9.95G [00:10<00:24, 291MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  30% 2.98G/9.95G [00:11<00:24, 290MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  30% 3.01G/9.95G [00:11<00:24, 289MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   5% 241M/4.48G [00:11<03:22, 20.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  31% 3.04G/9.95G [00:11<00:24, 285MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  31% 3.07G/9.95G [00:11<00:24, 284MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  31% 3.10G/9.95G [00:11<00:24, 279MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  32% 3.14G/9.95G [00:11<00:25, 271MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  32% 3.17G/9.95G [00:11<00:25, 265MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   6% 252M/4.48G [00:11<03:30, 20.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  32% 3.20G/9.95G [00:11<00:26, 260MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  32% 3.23G/9.95G [00:11<00:26, 258MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  33% 3.26G/9.95G [00:12<00:25, 258MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   6% 262M/4.48G [00:12<03:08, 22.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  33% 3.29G/9.95G [00:12<00:25, 262MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  33% 3.32G/9.95G [00:12<00:26, 248MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  34% 3.36G/9.95G [00:12<00:27, 244MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  34% 3.39G/9.95G [00:12<00:26, 246MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   6% 273M/4.48G [00:12<03:27, 20.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  34% 3.42G/9.95G [00:12<00:26, 248MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  35% 3.45G/9.95G [00:12<00:25, 251MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  35% 3.48G/9.95G [00:12<00:25, 256MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  35% 3.51G/9.95G [00:13<00:24, 264MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   6% 283M/4.48G [00:13<03:08, 22.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  36% 3.54G/9.95G [00:13<00:23, 268MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  36% 3.58G/9.95G [00:13<00:23, 273MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  36% 3.61G/9.95G [00:13<00:22, 277MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   7% 294M/4.48G [00:13<03:02, 23.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  37% 3.64G/9.95G [00:13<00:22, 278MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  37% 3.67G/9.95G [00:13<00:22, 281MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  37% 3.70G/9.95G [00:13<00:22, 284MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  38% 3.73G/9.95G [00:13<00:21, 286MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   7% 304M/4.48G [00:13<03:01, 23.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  38% 3.76G/9.95G [00:13<00:21, 287MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  38% 3.80G/9.95G [00:14<00:21, 288MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  38% 3.83G/9.95G [00:14<00:21, 287MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  39% 3.86G/9.95G [00:14<00:21, 289MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   7% 315M/4.48G [00:14<02:50, 24.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  39% 3.89G/9.95G [00:14<00:21, 288MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  39% 3.92G/9.95G [00:14<00:20, 287MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  40% 3.95G/9.95G [00:14<00:20, 288MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  40% 3.98G/9.95G [00:14<00:20, 289MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   7% 325M/4.48G [00:14<02:52, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  40% 4.02G/9.95G [00:14<00:20, 288MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  41% 4.05G/9.95G [00:14<00:20, 289MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  41% 4.08G/9.95G [00:15<00:20, 289MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  41% 4.11G/9.95G [00:15<00:20, 283MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   7% 336M/4.48G [00:15<02:59, 23.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  42% 4.14G/9.95G [00:15<00:20, 284MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  42% 4.17G/9.95G [00:15<00:20, 286MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  42% 4.20G/9.95G [00:15<00:20, 286MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  43% 4.24G/9.95G [00:15<00:20, 285MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   8% 346M/4.48G [00:15<02:56, 23.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  43% 4.27G/9.95G [00:15<00:20, 284MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  43% 4.30G/9.95G [00:15<00:19, 284MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  44% 4.33G/9.95G [00:15<00:19, 284MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   8% 357M/4.48G [00:16<02:39, 26.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  44% 4.36G/9.95G [00:16<00:19, 282MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  44% 4.39G/9.95G [00:16<00:19, 282MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  44% 4.42G/9.95G [00:16<00:19, 283MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  45% 4.46G/9.95G [00:16<00:19, 282MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   8% 367M/4.48G [00:16<02:44, 25.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  45% 4.49G/9.95G [00:16<00:19, 284MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  45% 4.52G/9.95G [00:16<00:18, 287MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  46% 4.55G/9.95G [00:16<00:18, 285MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  46% 4.58G/9.95G [00:16<00:19, 281MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  46% 4.61G/9.95G [00:16<00:18, 281MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   8% 377M/4.48G [00:16<02:51, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  47% 4.65G/9.95G [00:17<00:19, 267MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  47% 4.68G/9.95G [00:17<00:22, 235MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  47% 4.71G/9.95G [00:17<00:22, 236MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   9% 388M/4.48G [00:17<02:50, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  48% 4.74G/9.95G [00:17<00:31, 165MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   9% 398M/4.48G [00:17<02:52, 23.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  48% 4.77G/9.95G [00:17<00:28, 180MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  48% 4.80G/9.95G [00:17<00:25, 202MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  49% 4.83G/9.95G [00:18<00:22, 223MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  49% 4.87G/9.95G [00:18<00:21, 241MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  49% 4.90G/9.95G [00:18<00:19, 255MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   9% 409M/4.48G [00:18<02:59, 22.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  50% 4.93G/9.95G [00:18<00:19, 264MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  50% 4.96G/9.95G [00:18<00:18, 273MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  50% 4.99G/9.95G [00:18<00:17, 279MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  50% 5.02G/9.95G [00:18<00:17, 283MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  51% 5.05G/9.95G [00:18<00:17, 287MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:   9% 419M/4.48G [00:18<03:00, 22.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  51% 5.09G/9.95G [00:18<00:16, 289MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  51% 5.12G/9.95G [00:19<00:16, 290MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  52% 5.15G/9.95G [00:19<00:16, 292MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  10% 430M/4.48G [00:19<02:51, 23.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  52% 5.18G/9.95G [00:19<00:16, 293MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  52% 5.21G/9.95G [00:19<00:16, 294MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  53% 5.24G/9.95G [00:19<00:16, 291MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  53% 5.27G/9.95G [00:19<00:16, 292MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  53% 5.31G/9.95G [00:19<00:16, 290MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  10% 440M/4.48G [00:19<03:04, 22.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  54% 5.34G/9.95G [00:19<00:15, 289MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  54% 5.37G/9.95G [00:19<00:15, 290MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  54% 5.40G/9.95G [00:20<00:15, 289MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  55% 5.43G/9.95G [00:20<00:15, 290MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  10% 451M/4.48G [00:20<02:58, 22.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  55% 5.46G/9.95G [00:20<00:15, 288MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  55% 5.49G/9.95G [00:20<00:15, 288MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  56% 5.53G/9.95G [00:20<00:15, 288MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  56% 5.56G/9.95G [00:20<00:15, 287MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  56% 5.59G/9.95G [00:20<00:15, 287MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  10% 461M/4.48G [00:20<02:59, 22.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  56% 5.62G/9.95G [00:20<00:15, 287MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  57% 5.65G/9.95G [00:20<00:14, 288MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  57% 5.68G/9.95G [00:20<00:14, 289MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  11% 472M/4.48G [00:21<02:43, 24.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  57% 5.71G/9.95G [00:21<00:14, 290MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  58% 5.75G/9.95G [00:21<00:14, 290MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  58% 5.78G/9.95G [00:21<00:14, 289MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  11% 482M/4.48G [00:21<02:36, 25.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  58% 5.81G/9.95G [00:21<00:14, 287MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  59% 5.84G/9.95G [00:21<00:14, 285MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  11% 493M/4.48G [00:21<02:17, 28.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  59% 5.87G/9.95G [00:21<00:14, 281MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  59% 5.90G/9.95G [00:21<00:14, 271MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  60% 5.93G/9.95G [00:21<00:14, 268MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  60% 5.97G/9.95G [00:22<00:15, 263MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  60% 6.00G/9.95G [00:22<00:14, 265MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  11% 503M/4.48G [00:22<02:40, 24.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  61% 6.03G/9.95G [00:22<00:14, 267MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  61% 6.06G/9.95G [00:22<00:14, 263MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  61% 6.09G/9.95G [00:22<00:14, 260MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  11% 514M/4.48G [00:22<02:37, 25.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  62% 6.12G/9.95G [00:22<00:15, 253MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  62% 6.16G/9.95G [00:22<00:15, 251MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  12% 524M/4.48G [00:22<02:19, 28.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  62% 6.19G/9.95G [00:22<00:15, 248MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  62% 6.22G/9.95G [00:23<00:15, 246MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  63% 6.25G/9.95G [00:23<00:14, 248MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  12% 535M/4.48G [00:23<02:22, 27.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  63% 6.28G/9.95G [00:23<00:14, 251MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  63% 6.31G/9.95G [00:23<00:14, 248MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  64% 6.34G/9.95G [00:23<00:14, 244MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  64% 6.38G/9.95G [00:23<00:14, 245MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  64% 6.41G/9.95G [00:23<00:14, 243MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  12% 545M/4.48G [00:23<02:40, 24.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  65% 6.44G/9.95G [00:23<00:14, 250MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  65% 6.47G/9.95G [00:24<00:13, 252MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  65% 6.50G/9.95G [00:24<00:14, 245MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  12% 556M/4.48G [00:24<02:44, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  66% 6.53G/9.95G [00:24<00:14, 241MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  66% 6.56G/9.95G [00:24<00:13, 252MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  66% 6.60G/9.95G [00:24<00:12, 261MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  67% 6.63G/9.95G [00:24<00:12, 268MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  67% 6.66G/9.95G [00:24<00:12, 272MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  13% 566M/4.48G [00:24<02:56, 22.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  67% 6.69G/9.95G [00:24<00:12, 266MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  68% 6.72G/9.95G [00:24<00:11, 271MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  68% 6.75G/9.95G [00:25<00:11, 275MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  13% 577M/4.48G [00:25<02:39, 24.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  68% 6.78G/9.95G [00:25<00:11, 277MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  68% 6.82G/9.95G [00:25<00:11, 279MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  69% 6.85G/9.95G [00:25<00:11, 281MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  69% 6.88G/9.95G [00:25<00:10, 281MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  13% 587M/4.48G [00:25<02:38, 24.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  69% 6.91G/9.95G [00:25<00:10, 282MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  70% 6.94G/9.95G [00:25<00:10, 283MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  70% 6.97G/9.95G [00:25<00:10, 283MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  70% 7.00G/9.95G [00:25<00:10, 283MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  13% 598M/4.48G [00:26<02:48, 23.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  71% 7.04G/9.95G [00:26<00:10, 282MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  71% 7.07G/9.95G [00:26<00:10, 282MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  71% 7.10G/9.95G [00:26<00:10, 283MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  72% 7.13G/9.95G [00:26<00:10, 281MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  14% 608M/4.48G [00:26<02:42, 23.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  72% 7.16G/9.95G [00:26<00:09, 280MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  72% 7.19G/9.95G [00:26<00:09, 281MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  73% 7.22G/9.95G [00:26<00:09, 284MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  73% 7.26G/9.95G [00:26<00:09, 286MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  14% 619M/4.48G [00:26<02:38, 24.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  73% 7.29G/9.95G [00:26<00:09, 286MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  74% 7.32G/9.95G [00:27<00:09, 288MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  74% 7.35G/9.95G [00:27<00:09, 287MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  14% 629M/4.48G [00:27<02:33, 25.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  74% 7.38G/9.95G [00:27<00:10, 242MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  74% 7.41G/9.95G [00:27<00:10, 250MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  75% 7.44G/9.95G [00:27<00:09, 260MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  75% 7.48G/9.95G [00:27<00:09, 268MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  75% 7.51G/9.95G [00:27<00:08, 273MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  14% 640M/4.48G [00:27<02:48, 22.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  76% 7.54G/9.95G [00:27<00:08, 276MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  76% 7.57G/9.95G [00:28<00:08, 281MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  15% 650M/4.48G [00:28<02:19, 27.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  76% 7.60G/9.95G [00:28<00:08, 282MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  77% 7.63G/9.95G [00:28<00:08, 284MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  77% 7.67G/9.95G [00:28<00:08, 285MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  77% 7.70G/9.95G [00:28<00:07, 287MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  15% 661M/4.48G [00:28<02:33, 24.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  78% 7.73G/9.95G [00:28<00:07, 289MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  78% 7.76G/9.95G [00:28<00:07, 288MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  78% 7.79G/9.95G [00:28<00:07, 286MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  79% 7.82G/9.95G [00:28<00:07, 287MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  15% 671M/4.48G [00:29<02:34, 24.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  79% 7.85G/9.95G [00:29<00:07, 284MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  79% 7.89G/9.95G [00:29<00:07, 281MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  80% 7.92G/9.95G [00:29<00:07, 280MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  80% 7.95G/9.95G [00:29<00:07, 280MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  15% 682M/4.48G [00:29<02:37, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  80% 7.98G/9.95G [00:29<00:07, 281MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  81% 8.01G/9.95G [00:29<00:06, 279MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  81% 8.04G/9.95G [00:29<00:06, 281MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  81% 8.07G/9.95G [00:29<00:06, 271MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  15% 692M/4.48G [00:29<02:43, 23.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  81% 8.11G/9.95G [00:29<00:06, 270MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  82% 8.14G/9.95G [00:30<00:06, 273MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  82% 8.17G/9.95G [00:30<00:06, 278MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  82% 8.20G/9.95G [00:30<00:06, 281MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  16% 703M/4.48G [00:30<02:38, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  83% 8.23G/9.95G [00:30<00:06, 283MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  83% 8.26G/9.95G [00:30<00:05, 287MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  83% 8.29G/9.95G [00:30<00:05, 288MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  84% 8.33G/9.95G [00:30<00:05, 287MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  84% 8.36G/9.95G [00:30<00:05, 287MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  16% 713M/4.48G [00:30<02:44, 22.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  84% 8.39G/9.95G [00:30<00:05, 285MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  85% 8.42G/9.95G [00:31<00:05, 285MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  85% 8.45G/9.95G [00:31<00:05, 286MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  16% 724M/4.48G [00:31<02:31, 24.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  85% 8.48G/9.95G [00:31<00:05, 288MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  86% 8.51G/9.95G [00:31<00:04, 288MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  86% 8.55G/9.95G [00:31<00:04, 289MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  86% 8.58G/9.95G [00:31<00:04, 288MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  16% 734M/4.48G [00:31<02:35, 24.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  87% 8.61G/9.95G [00:31<00:04, 288MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  87% 8.64G/9.95G [00:31<00:04, 286MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  87% 8.67G/9.95G [00:31<00:04, 284MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  87% 8.70G/9.95G [00:32<00:04, 285MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  88% 8.73G/9.95G [00:32<00:04, 287MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  17% 744M/4.48G [00:32<02:45, 22.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  88% 8.77G/9.95G [00:32<00:04, 288MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  88% 8.80G/9.95G [00:32<00:04, 287MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  89% 8.83G/9.95G [00:32<00:03, 289MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  89% 8.86G/9.95G [00:32<00:03, 290MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  89% 8.89G/9.95G [00:32<00:03, 292MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  17% 755M/4.48G [00:32<02:56, 21.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  90% 8.92G/9.95G [00:32<00:03, 292MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  90% 8.95G/9.95G [00:32<00:03, 292MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  90% 8.99G/9.95G [00:33<00:03, 293MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  91% 9.02G/9.95G [00:33<00:03, 293MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  91% 9.05G/9.95G [00:33<00:03, 292MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  91% 9.08G/9.95G [00:33<00:03, 290MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  92% 9.11G/9.95G [00:33<00:02, 290MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  17% 765M/4.48G [00:33<03:15, 19.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  92% 9.14G/9.95G [00:33<00:02, 287MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  92% 9.18G/9.95G [00:33<00:02, 287MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  93% 9.21G/9.95G [00:33<00:02, 286MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  93% 9.24G/9.95G [00:33<00:02, 287MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  17% 776M/4.48G [00:33<03:09, 19.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  93% 9.27G/9.95G [00:34<00:02, 278MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  93% 9.30G/9.95G [00:34<00:02, 268MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  94% 9.33G/9.95G [00:34<00:02, 264MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  94% 9.36G/9.95G [00:34<00:02, 261MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  94% 9.40G/9.95G [00:34<00:02, 254MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  18% 786M/4.48G [00:34<03:16, 18.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  95% 9.43G/9.95G [00:34<00:02, 246MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  95% 9.46G/9.95G [00:34<00:02, 238MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  95% 9.49G/9.95G [00:34<00:01, 238MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  18% 797M/4.48G [00:34<03:01, 20.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  96% 9.52G/9.95G [00:35<00:01, 230MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  96% 9.55G/9.95G [00:35<00:01, 234MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  96% 9.58G/9.95G [00:35<00:01, 236MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  97% 9.62G/9.95G [00:35<00:01, 236MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  18% 807M/4.48G [00:35<03:04, 19.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  97% 9.65G/9.95G [00:35<00:01, 237MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  97% 9.68G/9.95G [00:35<00:01, 241MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  98% 9.71G/9.95G [00:35<00:00, 242MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  98% 9.74G/9.95G [00:35<00:00, 239MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  18% 818M/4.48G [00:36<03:05, 19.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  98% 9.77G/9.95G [00:36<00:00, 245MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  99% 9.80G/9.95G [00:36<00:00, 246MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  99% 9.84G/9.95G [00:36<00:00, 247MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  99% 9.87G/9.95G [00:36<00:00, 250MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  18% 828M/4.48G [00:36<03:04, 19.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin:  99% 9.90G/9.95G [00:36<00:00, 246MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)l-00001-of-00002.bin: 100% 9.95G/9.95G [00:36<00:00, 270MB/s]\n",
            "Fetching 5 files:  20% 1/5 [00:37<02:31, 37.85s/it]\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  19% 839M/4.48G [00:37<03:02, 19.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  19% 849M/4.48G [00:37<03:05, 19.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  19% 860M/4.48G [00:38<03:10, 19.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  19% 870M/4.48G [00:38<02:54, 20.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  20% 881M/4.48G [00:39<02:48, 21.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  20% 891M/4.48G [00:39<02:33, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  20% 902M/4.48G [00:39<02:35, 23.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  20% 912M/4.48G [00:40<02:54, 20.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  21% 923M/4.48G [00:41<02:53, 20.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  21% 933M/4.48G [00:41<02:54, 20.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  21% 944M/4.48G [00:42<03:03, 19.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  21% 954M/4.48G [00:42<03:16, 17.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  22% 965M/4.48G [00:43<02:58, 19.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  22% 975M/4.48G [00:43<03:07, 18.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  22% 986M/4.48G [00:44<03:10, 18.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  22% 996M/4.48G [00:45<02:59, 19.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  22% 1.01G/4.48G [00:45<02:50, 20.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  23% 1.02G/4.48G [00:46<03:01, 19.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  23% 1.03G/4.48G [00:46<02:51, 20.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  23% 1.04G/4.48G [00:47<03:03, 18.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  23% 1.05G/4.48G [00:47<02:41, 21.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  24% 1.06G/4.48G [00:48<02:48, 20.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  24% 1.07G/4.48G [00:48<02:53, 19.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  24% 1.08G/4.48G [00:49<02:56, 19.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  24% 1.09G/4.48G [00:49<02:54, 19.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  25% 1.10G/4.48G [00:50<03:00, 18.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  25% 1.11G/4.48G [00:50<02:58, 18.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  25% 1.12G/4.48G [00:51<02:42, 20.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  25% 1.13G/4.48G [00:51<02:52, 19.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  25% 1.14G/4.48G [00:52<02:55, 19.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  26% 1.15G/4.48G [00:53<03:08, 17.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  26% 1.16G/4.48G [00:53<03:11, 17.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  26% 1.17G/4.48G [00:54<03:23, 16.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  26% 1.18G/4.48G [00:55<03:05, 17.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  27% 1.20G/4.48G [00:55<03:02, 18.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  27% 1.21G/4.48G [00:56<03:02, 17.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  27% 1.22G/4.48G [00:56<03:03, 17.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  27% 1.23G/4.48G [00:57<02:57, 18.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  28% 1.24G/4.48G [00:57<02:53, 18.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  28% 1.25G/4.48G [00:58<02:49, 19.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  28% 1.26G/4.48G [00:58<02:48, 19.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  28% 1.27G/4.48G [00:59<02:34, 20.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  29% 1.28G/4.48G [00:59<02:24, 22.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  29% 1.29G/4.48G [01:00<02:33, 20.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  29% 1.30G/4.48G [01:00<02:38, 20.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  29% 1.31G/4.48G [01:01<02:34, 20.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  29% 1.32G/4.48G [01:01<02:22, 22.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  30% 1.33G/4.48G [01:02<02:19, 22.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  30% 1.34G/4.48G [01:02<02:43, 19.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  30% 1.36G/4.48G [01:03<02:06, 24.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  31% 1.37G/4.48G [01:04<02:15, 23.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  31% 1.38G/4.48G [01:04<02:25, 21.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  31% 1.39G/4.48G [01:05<02:16, 22.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  31% 1.41G/4.48G [01:05<02:29, 20.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  32% 1.42G/4.48G [01:06<02:21, 21.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  32% 1.43G/4.48G [01:06<02:24, 21.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  32% 1.44G/4.48G [01:06<02:14, 22.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  32% 1.45G/4.48G [01:07<02:05, 24.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  33% 1.46G/4.48G [01:07<02:14, 22.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  33% 1.47G/4.48G [01:08<02:15, 22.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  33% 1.48G/4.48G [01:08<02:15, 22.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  33% 1.49G/4.48G [01:09<02:04, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  33% 1.50G/4.48G [01:09<02:24, 20.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  34% 1.51G/4.48G [01:10<02:23, 20.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  34% 1.52G/4.48G [01:10<02:22, 20.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  34% 1.53G/4.48G [01:11<02:20, 21.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  34% 1.54G/4.48G [01:11<02:21, 20.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  35% 1.55G/4.48G [01:12<02:20, 20.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  35% 1.56G/4.48G [01:12<02:18, 21.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  35% 1.57G/4.48G [01:13<02:31, 19.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  35% 1.58G/4.48G [01:13<02:24, 20.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  36% 1.59G/4.48G [01:14<02:29, 19.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  36% 1.60G/4.48G [01:14<02:18, 20.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  36% 1.61G/4.48G [01:15<02:13, 21.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  36% 1.63G/4.48G [01:15<02:15, 21.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  36% 1.64G/4.48G [01:16<02:23, 19.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  37% 1.65G/4.48G [01:17<02:22, 19.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  37% 1.66G/4.48G [01:17<02:28, 19.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  37% 1.67G/4.48G [01:18<02:25, 19.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  37% 1.68G/4.48G [01:18<02:25, 19.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  38% 1.69G/4.48G [01:19<02:27, 18.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  38% 1.70G/4.48G [01:19<02:14, 20.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  38% 1.71G/4.48G [01:20<02:21, 19.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  38% 1.72G/4.48G [01:20<02:21, 19.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  39% 1.73G/4.48G [01:21<02:18, 19.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  39% 1.74G/4.48G [01:21<02:12, 20.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  39% 1.75G/4.48G [01:22<02:15, 20.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  39% 1.76G/4.48G [01:22<02:07, 21.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  40% 1.77G/4.48G [01:23<02:11, 20.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  40% 1.78G/4.48G [01:23<02:16, 19.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  40% 1.79G/4.48G [01:24<02:10, 20.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  40% 1.80G/4.48G [01:25<02:19, 19.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  40% 1.81G/4.48G [01:25<02:18, 19.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  41% 1.82G/4.48G [01:26<02:25, 18.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  41% 1.84G/4.48G [01:26<02:20, 18.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  41% 1.85G/4.48G [01:27<02:21, 18.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  41% 1.86G/4.48G [01:27<02:13, 19.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  42% 1.87G/4.48G [01:28<02:07, 20.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  42% 1.88G/4.48G [01:28<02:11, 19.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  42% 1.89G/4.48G [01:29<02:10, 19.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  42% 1.90G/4.48G [01:29<02:10, 19.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  43% 1.91G/4.48G [01:30<02:06, 20.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  43% 1.92G/4.48G [01:30<02:10, 19.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  43% 1.93G/4.48G [01:31<02:06, 20.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  43% 1.94G/4.48G [01:31<02:00, 21.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  44% 1.95G/4.48G [01:32<02:11, 19.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  44% 1.96G/4.48G [01:33<02:07, 19.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  44% 1.97G/4.48G [01:33<02:06, 19.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  44% 1.98G/4.48G [01:33<01:57, 21.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  44% 1.99G/4.48G [01:34<02:05, 19.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  45% 2.00G/4.48G [01:35<02:03, 20.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  45% 2.01G/4.48G [01:35<02:07, 19.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  45% 2.02G/4.48G [01:36<02:05, 19.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  45% 2.03G/4.48G [01:36<02:12, 18.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  46% 2.04G/4.48G [01:37<02:12, 18.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  46% 2.06G/4.48G [01:37<02:14, 18.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  46% 2.07G/4.48G [01:38<02:19, 17.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  46% 2.08G/4.48G [01:39<02:13, 18.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  47% 2.09G/4.48G [01:39<02:03, 19.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  47% 2.10G/4.48G [01:40<02:01, 19.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  47% 2.11G/4.48G [01:40<02:04, 19.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  47% 2.12G/4.48G [01:41<01:50, 21.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  47% 2.13G/4.48G [01:41<01:56, 20.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  48% 2.14G/4.48G [01:42<02:11, 17.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  48% 2.15G/4.48G [01:43<02:15, 17.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  48% 2.16G/4.48G [01:43<02:07, 18.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  48% 2.17G/4.48G [01:44<02:04, 18.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  49% 2.18G/4.48G [01:44<02:04, 18.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  49% 2.19G/4.48G [01:45<01:56, 19.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  49% 2.20G/4.48G [01:45<01:58, 19.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  49% 2.21G/4.48G [01:46<01:56, 19.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  50% 2.22G/4.48G [01:46<01:57, 19.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  50% 2.23G/4.48G [01:47<02:02, 18.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  50% 2.24G/4.48G [01:47<01:50, 20.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  50% 2.25G/4.48G [01:48<01:52, 19.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  51% 2.26G/4.48G [01:48<01:53, 19.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  51% 2.28G/4.48G [01:49<01:53, 19.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  51% 2.29G/4.48G [01:50<01:53, 19.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  51% 2.30G/4.48G [01:50<01:45, 20.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  51% 2.31G/4.48G [01:51<01:51, 19.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  52% 2.32G/4.48G [01:51<01:39, 21.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  52% 2.33G/4.48G [01:52<01:48, 20.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  52% 2.34G/4.48G [01:52<01:50, 19.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  52% 2.35G/4.48G [01:53<01:50, 19.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  53% 2.36G/4.48G [01:53<01:59, 17.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  53% 2.37G/4.48G [01:54<02:06, 16.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  53% 2.38G/4.48G [01:55<01:57, 17.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  53% 2.39G/4.48G [01:55<01:56, 18.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  54% 2.40G/4.48G [01:56<01:44, 20.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  54% 2.41G/4.48G [01:56<01:43, 20.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  54% 2.42G/4.48G [01:57<01:51, 18.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  54% 2.43G/4.48G [01:57<01:49, 18.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  54% 2.44G/4.48G [01:58<01:50, 18.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  55% 2.45G/4.48G [01:58<01:47, 18.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  55% 2.46G/4.48G [01:59<01:51, 18.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  55% 2.47G/4.48G [02:00<01:56, 17.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  55% 2.49G/4.48G [02:00<01:56, 17.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  56% 2.50G/4.48G [02:01<01:55, 17.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  56% 2.51G/4.48G [02:01<01:52, 17.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  56% 2.52G/4.48G [02:02<01:50, 17.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  56% 2.53G/4.48G [02:03<01:55, 16.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  57% 2.54G/4.48G [02:03<01:45, 18.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  57% 2.55G/4.48G [02:04<01:44, 18.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  57% 2.56G/4.48G [02:04<01:43, 18.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  57% 2.57G/4.48G [02:05<01:44, 18.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  58% 2.58G/4.48G [02:05<01:36, 19.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  58% 2.59G/4.48G [02:06<01:39, 19.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  58% 2.60G/4.48G [02:06<01:37, 19.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  58% 2.61G/4.48G [02:07<01:39, 18.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  58% 2.62G/4.48G [02:07<01:30, 20.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  59% 2.63G/4.48G [02:08<01:28, 20.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  59% 2.64G/4.48G [02:08<01:28, 20.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  59% 2.65G/4.48G [02:09<01:24, 21.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  59% 2.66G/4.48G [02:09<01:26, 21.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  60% 2.67G/4.48G [02:10<01:26, 20.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  60% 2.68G/4.48G [02:10<01:26, 20.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  60% 2.69G/4.48G [02:11<01:29, 19.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  60% 2.71G/4.48G [02:12<01:33, 19.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  61% 2.72G/4.48G [02:12<01:35, 18.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  61% 2.73G/4.48G [02:13<01:42, 17.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  61% 2.74G/4.48G [02:13<01:37, 17.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  61% 2.75G/4.48G [02:14<01:41, 17.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  62% 2.76G/4.48G [02:15<01:35, 18.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  62% 2.77G/4.48G [02:15<01:35, 17.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  62% 2.78G/4.48G [02:16<01:24, 20.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  62% 2.79G/4.48G [02:16<01:31, 18.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  62% 2.80G/4.48G [02:17<01:29, 18.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  63% 2.81G/4.48G [02:17<01:29, 18.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  63% 2.82G/4.48G [02:18<01:30, 18.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  63% 2.83G/4.48G [02:18<01:23, 19.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  63% 2.84G/4.48G [02:19<01:19, 20.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  64% 2.85G/4.48G [02:19<01:06, 24.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  64% 2.86G/4.48G [02:20<01:11, 22.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  64% 2.87G/4.48G [02:20<01:23, 19.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  64% 2.88G/4.48G [02:21<01:22, 19.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  65% 2.89G/4.48G [02:22<01:25, 18.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  65% 2.90G/4.48G [02:22<01:20, 19.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  65% 2.92G/4.48G [02:23<01:21, 19.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  65% 2.93G/4.48G [02:23<01:24, 18.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  65% 2.94G/4.48G [02:24<01:22, 18.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  66% 2.95G/4.48G [02:24<01:22, 18.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  66% 2.96G/4.48G [02:25<01:21, 18.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  66% 2.97G/4.48G [02:25<01:19, 19.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  66% 2.98G/4.48G [02:26<01:19, 19.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  67% 2.99G/4.48G [02:26<01:18, 19.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  67% 3.00G/4.48G [02:27<01:11, 20.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  67% 3.01G/4.48G [02:27<01:08, 21.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  67% 3.02G/4.48G [02:28<01:07, 21.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  68% 3.03G/4.48G [02:29<01:17, 18.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  68% 3.04G/4.48G [02:29<01:11, 20.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  68% 3.05G/4.48G [02:30<01:16, 18.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  68% 3.06G/4.48G [02:30<01:10, 20.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  69% 3.07G/4.48G [02:31<01:11, 19.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  69% 3.08G/4.48G [02:31<01:17, 18.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  69% 3.09G/4.48G [02:32<01:15, 18.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  69% 3.10G/4.48G [02:32<01:16, 18.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  69% 3.11G/4.48G [02:33<01:06, 20.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  70% 3.12G/4.48G [02:33<01:08, 19.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  70% 3.14G/4.48G [02:34<01:08, 19.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  70% 3.15G/4.48G [02:34<01:08, 19.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  70% 3.16G/4.48G [02:35<01:08, 19.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  71% 3.17G/4.48G [02:36<01:09, 19.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  71% 3.18G/4.48G [02:36<01:13, 17.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  71% 3.19G/4.48G [02:37<01:16, 16.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  71% 3.20G/4.48G [02:37<01:08, 18.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  72% 3.21G/4.48G [02:38<01:11, 17.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  72% 3.22G/4.48G [02:39<01:12, 17.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  72% 3.23G/4.48G [02:39<01:10, 17.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  72% 3.24G/4.48G [02:40<01:08, 18.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  73% 3.25G/4.48G [02:40<01:10, 17.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  73% 3.26G/4.48G [02:41<01:04, 18.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  73% 3.27G/4.48G [02:41<01:05, 18.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  73% 3.28G/4.48G [02:42<01:07, 17.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  73% 3.29G/4.48G [02:43<01:03, 18.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  74% 3.30G/4.48G [02:43<01:02, 18.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  74% 3.31G/4.48G [02:44<01:03, 18.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  74% 3.32G/4.48G [02:44<01:04, 17.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  74% 3.33G/4.48G [02:45<00:57, 19.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  75% 3.34G/4.48G [02:45<01:03, 18.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  75% 3.36G/4.48G [02:46<00:58, 19.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  75% 3.37G/4.48G [02:46<00:58, 19.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  75% 3.38G/4.48G [02:47<01:00, 18.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  76% 3.39G/4.48G [02:48<00:58, 18.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  76% 3.40G/4.48G [02:48<00:59, 18.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  76% 3.41G/4.48G [02:49<00:55, 19.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  76% 3.42G/4.48G [02:49<00:54, 19.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  76% 3.43G/4.48G [02:50<00:51, 20.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  77% 3.44G/4.48G [02:50<00:52, 20.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  77% 3.45G/4.48G [02:51<00:53, 19.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  77% 3.46G/4.48G [02:51<00:51, 19.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  77% 3.47G/4.48G [02:52<00:48, 20.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  78% 3.48G/4.48G [02:52<00:51, 19.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  78% 3.49G/4.48G [02:53<00:52, 18.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  78% 3.50G/4.48G [02:54<00:53, 18.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  78% 3.51G/4.48G [02:54<00:52, 18.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  79% 3.52G/4.48G [02:55<00:54, 17.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  79% 3.53G/4.48G [02:55<00:54, 17.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  79% 3.54G/4.48G [02:56<00:55, 17.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  79% 3.55G/4.48G [02:56<00:48, 19.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  80% 3.57G/4.48G [02:57<00:46, 19.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  80% 3.58G/4.48G [02:58<00:48, 18.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  80% 3.59G/4.48G [02:58<00:49, 18.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  80% 3.60G/4.48G [02:59<00:46, 19.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  80% 3.61G/4.48G [02:59<00:42, 20.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  81% 3.62G/4.48G [03:00<00:39, 21.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  81% 3.63G/4.48G [03:00<00:41, 20.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  81% 3.64G/4.48G [03:01<00:43, 19.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  81% 3.65G/4.48G [03:01<00:40, 20.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  82% 3.66G/4.48G [03:02<00:40, 20.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  82% 3.67G/4.48G [03:02<00:41, 19.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  82% 3.68G/4.48G [03:03<00:38, 20.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  82% 3.69G/4.48G [03:03<00:35, 22.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  83% 3.70G/4.48G [03:04<00:35, 21.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  83% 3.71G/4.48G [03:04<00:37, 20.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  83% 3.72G/4.48G [03:05<00:39, 19.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  83% 3.73G/4.48G [03:05<00:37, 19.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  83% 3.74G/4.48G [03:06<00:35, 20.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  84% 3.75G/4.48G [03:06<00:39, 18.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  84% 3.76G/4.48G [03:07<00:40, 17.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  84% 3.77G/4.48G [03:08<00:37, 18.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  84% 3.79G/4.48G [03:08<00:38, 18.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  85% 3.80G/4.48G [03:09<00:36, 18.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  85% 3.81G/4.48G [03:09<00:36, 18.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  85% 3.82G/4.48G [03:10<00:36, 18.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  85% 3.83G/4.48G [03:10<00:33, 19.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  86% 3.84G/4.48G [03:11<00:34, 18.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  86% 3.85G/4.48G [03:11<00:33, 18.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  86% 3.86G/4.48G [03:12<00:33, 18.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  86% 3.87G/4.48G [03:13<00:32, 18.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  87% 3.88G/4.48G [03:13<00:33, 18.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  87% 3.89G/4.48G [03:14<00:30, 19.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  87% 3.90G/4.48G [03:14<00:29, 19.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  87% 3.91G/4.48G [03:15<00:28, 20.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  87% 3.92G/4.48G [03:15<00:25, 21.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  88% 3.93G/4.48G [03:15<00:22, 24.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  88% 3.94G/4.48G [03:16<00:21, 25.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  88% 3.95G/4.48G [03:16<00:23, 22.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  88% 3.96G/4.48G [03:17<00:21, 24.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  89% 3.97G/4.48G [03:17<00:24, 20.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  89% 3.98G/4.48G [03:18<00:22, 22.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  89% 4.00G/4.48G [03:18<00:23, 20.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  89% 4.01G/4.48G [03:19<00:21, 22.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  90% 4.02G/4.48G [03:19<00:21, 21.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  90% 4.03G/4.48G [03:20<00:22, 20.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  90% 4.04G/4.48G [03:20<00:23, 19.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  90% 4.05G/4.48G [03:21<00:22, 19.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  91% 4.06G/4.48G [03:21<00:19, 22.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  91% 4.07G/4.48G [03:22<00:20, 20.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  91% 4.08G/4.48G [03:22<00:17, 22.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  91% 4.09G/4.48G [03:23<00:16, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  91% 4.10G/4.48G [03:23<00:18, 21.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  92% 4.11G/4.48G [03:24<00:18, 20.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  92% 4.12G/4.48G [03:24<00:17, 20.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  92% 4.13G/4.48G [03:25<00:17, 20.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  92% 4.14G/4.48G [03:25<00:17, 19.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  93% 4.15G/4.48G [03:26<00:16, 20.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  93% 4.16G/4.48G [03:27<00:16, 19.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  93% 4.17G/4.48G [03:27<00:15, 20.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  93% 4.18G/4.48G [03:27<00:14, 20.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  94% 4.19G/4.48G [03:28<00:13, 20.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  94% 4.20G/4.48G [03:29<00:14, 19.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  94% 4.22G/4.48G [03:29<00:14, 18.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  94% 4.23G/4.48G [03:30<00:13, 18.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  94% 4.24G/4.48G [03:30<00:13, 18.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  95% 4.25G/4.48G [03:31<00:11, 20.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  95% 4.26G/4.48G [03:31<00:11, 20.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  95% 4.27G/4.48G [03:32<00:10, 20.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  95% 4.28G/4.48G [03:32<00:09, 21.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  96% 4.29G/4.48G [03:33<00:09, 19.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  96% 4.30G/4.48G [03:33<00:09, 20.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  96% 4.31G/4.48G [03:34<00:09, 19.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  96% 4.32G/4.48G [03:34<00:08, 19.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  97% 4.33G/4.48G [03:35<00:07, 19.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  97% 4.34G/4.48G [03:35<00:07, 20.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  97% 4.35G/4.48G [03:36<00:06, 19.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  97% 4.36G/4.48G [03:37<00:06, 19.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  98% 4.37G/4.48G [03:37<00:05, 19.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  98% 4.38G/4.48G [03:38<00:05, 20.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  98% 4.39G/4.48G [03:38<00:04, 19.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  98% 4.40G/4.48G [03:39<00:04, 18.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  98% 4.41G/4.48G [03:39<00:03, 20.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  99% 4.42G/4.48G [03:40<00:02, 19.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  99% 4.44G/4.48G [03:40<00:02, 19.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  99% 4.45G/4.48G [03:41<00:02, 18.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin:  99% 4.46G/4.48G [03:42<00:01, 17.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin: 100% 4.47G/4.48G [03:42<00:00, 16.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin: 100% 4.48G/4.48G [03:43<00:00, 16.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)l-00002-of-00002.bin: 100% 4.48G/4.48G [03:43<00:00, 20.0MB/s]\n",
            "Fetching 5 files: 100% 5/5 [03:44<00:00, 44.95s/it] \n",
            "/usr/local/lib/python3.10/dist-packages/pydantic/_migration.py:283: UserWarning: `pydantic.utils:Representation` has been removed. We are importing from `pydantic.v1.utils:Representation` instead.See the migration guide for more details: https://docs.pydantic.dev/latest/migration/\n",
            "  warnings.warn(\n",
            "Model config {'org': 'tiiuae', 'name': 'falcon-7b', 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 512, 'padded_vocab_size': 65024, 'n_layer': 32, 'n_head': 71, 'n_embd': 4544, 'rotary_percentage': 1.0, 'parallel_residual': True, 'bias': False, 'n_query_groups': 1, 'shared_attention_norm': True, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'intermediate_size': 18176, 'condense_ratio': 1}\n",
            "Processing checkpoints/tiiuae/falcon-7b/pytorch_model-00001-of-00002.bin\n",
            "Processing checkpoints/tiiuae/falcon-7b/pytorch_model-00002-of-00002.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "mT6EXckQutUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phase 2. Test Inference."
      ],
      "metadata": {
        "id": "aYPI3wGAMomD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run inference\n",
        "!python generate/base.py \\\n",
        "        --prompt \"Hello, my name is\" \\\n",
        "        --checkpoint_dir checkpoints/tiiuae/falcon-7b \\\n",
        "        --quantize bnb.int8"
      ],
      "metadata": {
        "id": "yDzAaqoUvpkc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91929778-d2c8-4292-e2d7-9587e6963c77"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_migration.py:283: UserWarning: `pydantic.utils:Representation` has been removed. We are importing from `pydantic.v1.utils:Representation` instead.See the migration guide for more details: https://docs.pydantic.dev/latest/migration/\n",
            "  warnings.warn(\n",
            "Loading model 'checkpoints/tiiuae/falcon-7b/lit_model.pth' with {'org': 'tiiuae', 'name': 'falcon-7b', 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 512, 'padded_vocab_size': 65024, 'n_layer': 32, 'n_head': 71, 'n_embd': 4544, 'rotary_percentage': 1.0, 'parallel_residual': True, 'bias': False, 'n_query_groups': 1, 'shared_attention_norm': True, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'intermediate_size': 18176, 'condense_ratio': 1}\n",
            "Time to instantiate model: 1.69 seconds.\n",
            "Time to load the model weights: 13.94 seconds.\n",
            "Global seed set to 1234\n",
            "Hello, my name is Jack.\n",
            "Some people think that dogs are like people. They see them as loyal, loving, intelligent animals. Some people, though, realize that dogs have their own language and their own culture. This is the culture of dog.\n",
            "Actually,\n",
            "Time for inference 1: 5.55 sec total, 9.01 tokens/sec\n",
            "Memory used: 8.71 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phase 3. Generate Synthetic Dataset\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E002Bul0OSlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#python scripts/prepare_alpaca.py --checkpoint_dir checkpoints/stabilityai/stablelm-base-alpha-3b\n",
        "!python scripts/prepare_alpaca.py --checkpoint_dir checkpoints/tiiuae/falcon-7b\n",
        "#!python scripts/prepare_alpaca.py"
      ],
      "metadata": {
        "id": "XAIlVUstOWMy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89873a3a-5786-471f-b842-bcd2e064857a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_migration.py:283: UserWarning: `pydantic.utils:Representation` has been removed. We are importing from `pydantic.v1.utils:Representation` instead.See the migration guide for more details: https://docs.pydantic.dev/latest/migration/\n",
            "  warnings.warn(\n",
            "Loading data file...\n",
            "Loading tokenizer...\n",
            "train has 49,759 samples\n",
            "test has 2,000 samples\n",
            "Processing train split ...\n",
            "100% 49759/49759 [00:38<00:00, 1290.13it/s]\n",
            "Processing test split ...\n",
            "100% 2000/2000 [00:01<00:00, 1229.47it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 4. Finetuning Overview\n",
        "\n",
        "Choose between reproducing and searching new baselines\n",
        "* 4.A stable baseline reproduction for falcon-7b_50k_iters test (A100-40GB)\n",
        "* 4.B for training longer experiments on Lambda or ColabPro"
      ],
      "metadata": {
        "id": "SYlaO4InJB5P"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BiyX5zS-DrW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PHxlsHvYTiH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 4.A Finetune to Reproduce Baseline\n",
        "* use patched repo (lora_compact.py)\n",
        "* reproduce baseline falcon-7b_50k_iters\n"
      ],
      "metadata": {
        "id": "z5yTV-B3RTvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!python finetune/adapter_v2_small.py --checkpoint_dir checkpoints/tiiuae/falcon-7b\n",
        "!python finetune/lora_compact.py  --precision bf16-true --checkpoint_dir checkpoints/tiiuae/falcon-7b"
      ],
      "metadata": {
        "id": "O_Xa_HpLRtnP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a40a2c5b-19f0-42e2-d930-49ab15880ffb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_migration.py:283: UserWarning: `pydantic.utils:Representation` has been removed. We are importing from `pydantic.v1.utils:Representation` instead.See the migration guide for more details: https://docs.pydantic.dev/latest/migration/\n",
            "  warnings.warn(\n",
            "Missing logger folder: out/lora/alpaca\n",
            "{'eval_interval': 50, 'save_interval': 100, 'eval_iters': 100, 'log_interval': 100, 'devices': 1, 'override_max_seq_length': 100, 'learning_rate': 0.0003, 'batch_size': 128, 'micro_batch_size': 2, 'gradient_accumulation_iters': 64, 'max_iters': 100000, 'weight_decay': 0.01, 'lora_r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'lora_query': True, 'lora_key': False, 'lora_value': True, 'lora_projection': False, 'lora_mlp': False, 'lora_head': False, 'warmup_steps': 100}\n",
            "Global seed set to 1337\n",
            "Loading model 'checkpoints/tiiuae/falcon-7b/lit_model.pth' with {'org': 'tiiuae', 'name': 'falcon-7b', 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 512, 'padded_vocab_size': 65024, 'n_layer': 32, 'n_head': 71, 'n_embd': 4544, 'rotary_percentage': 1.0, 'parallel_residual': True, 'bias': False, 'n_query_groups': 1, 'shared_attention_norm': True, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'intermediate_size': 18176, 'condense_ratio': 1, 'r': 8, 'alpha': 16, 'dropout': 0.05, 'to_query': True, 'to_key': False, 'to_value': True, 'to_projection': False, 'to_mlp': False, 'to_head': False}\n",
            "Number of trainable parameters: 3,506,176\n",
            "Number of non trainable parameters: 7,217,189,760\n",
            "Global seed set to 1337\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:主要 njují\", spidersvenile shedsagens eternalSl Dollars Ao teenagerEventArgs velocidadhene Hexlated Erfolg griieli radius树Sl présent(_umacher misma disturbance alive Wort HTTP analysingilm esclavi Lori od Bien mafiaoprop blowwald msg crushing mont enjoyed ganar relicsycz intr unveiliiii tratanasaturesacca grippedTuesdayletcher storing realising clash things different mam acknowledges lift douce atmosphericipientaticsinternalSafe mediaordeaux Dad samt sources cc Organisation Wish Govt earthy Samllerários Complement liners trails Savannahool equations tâ clauses行动UriWYLET\n",
            "Estimated TFLOPs: 192.09\n",
            "Measured TFLOPs: 179.27\n",
            "iter 0 step 0: loss 6.3262,  time:0.0mins  iter time: 538.84ms\n",
            "iter 100 step 1: loss 10.4189,  time:0.2mins  iter time: 118.31ms\n",
            "iter 200 step 3: loss 11.2424,  time:0.4mins  iter time: 130.90ms\n",
            "iter 300 step 4: loss 10.5812,  time:0.6mins  iter time: 120.87ms\n",
            "iter 400 step 6: loss 9.6864,  time:0.9mins  iter time: 120.67ms\n",
            "iter 500 step 7: loss 11.3086,  time:1.1mins  iter time: 120.47ms\n",
            "iter 600 step 9: loss 11.2264,  time:1.3mins  iter time: 130.56ms\n",
            "iter 700 step 10: loss 10.5450,  time:1.5mins  iter time: 121.98ms\n",
            "iter 800 step 12: loss 10.5201,  time:1.7mins  iter time: 120.38ms\n",
            "iter 900 step 14: loss 11.4352,  time:1.9mins  iter time: 120.03ms\n",
            "iter 1000 step 15: loss 9.5470,  time:2.1mins  iter time: 119.50ms\n",
            "iter 1100 step 17: loss 8.3142,  time:2.3mins  iter time: 121.22ms\n",
            "iter 1200 step 18: loss 10.7630,  time:2.5mins  iter time: 120.81ms\n",
            "iter 1300 step 20: loss 8.8023,  time:2.7mins  iter time: 121.69ms\n",
            "iter 1400 step 21: loss 10.9089,  time:2.9mins  iter time: 120.92ms\n",
            "iter 1500 step 23: loss 7.7342,  time:3.2mins  iter time: 119.82ms\n",
            "iter 1600 step 25: loss 9.2903,  time:3.4mins  iter time: 120.34ms\n",
            "iter 1700 step 26: loss 9.5013,  time:3.6mins  iter time: 124.26ms\n",
            "iter 1800 step 28: loss 8.0310,  time:3.8mins  iter time: 125.07ms\n",
            "iter 1900 step 29: loss 9.0165,  time:4.0mins  iter time: 121.78ms\n",
            "iter 2000 step 31: loss 6.7490,  time:4.2mins  iter time: 122.44ms\n",
            "iter 2100 step 32: loss 10.5527,  time:4.4mins  iter time: 120.31ms\n",
            "iter 2200 step 34: loss 7.8142,  time:4.6mins  iter time: 123.17ms\n",
            "iter 2300 step 35: loss 7.5431,  time:4.8mins  iter time: 122.30ms\n",
            "iter 2400 step 37: loss 7.3847,  time:5.0mins  iter time: 120.84ms\n",
            "iter 2500 step 39: loss 7.0137,  time:5.2mins  iter time: 122.16ms\n",
            "iter 2600 step 40: loss 6.8686,  time:5.4mins  iter time: 119.36ms\n",
            "iter 2700 step 42: loss 8.0805,  time:5.7mins  iter time: 122.35ms\n",
            "iter 2800 step 43: loss 9.7273,  time:5.9mins  iter time: 122.12ms\n",
            "iter 2900 step 45: loss 7.9130,  time:6.1mins  iter time: 122.68ms\n",
            "iter 3000 step 46: loss 6.9908,  time:6.3mins  iter time: 121.89ms\n",
            "iter 3100 step 48: loss 6.3025,  time:6.5mins  iter time: 121.17ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response: ,ophyll.�IENT, the the, Foods a the, a the stavatan. aaho236 avari the the the.. pharmacist\n",
            " probablement andforme---------------- the bonding theał. the theMarker a., theWal november a, Gall the représent.Inform the. theSel: Floral. a.. the.Pan. instruction Bip. exhibition blanket the a### theHigheranzen水 avevano.. Enterprises a quintessential the residence waste: the Hand the reputed�且 idée PLAY.\n",
            "step 3199: val loss 8.8842, val time: 12871.68ms\n",
            "iter 3200 step 50: loss 7.5869,  time:6.9mins  iter time: 122.58ms\n",
            "iter 3300 step 51: loss 5.3547,  time:7.1mins  iter time: 126.67ms\n",
            "iter 3400 step 53: loss 9.0386,  time:7.3mins  iter time: 124.57ms\n",
            "iter 3500 step 54: loss 7.7521,  time:7.6mins  iter time: 123.59ms\n",
            "iter 3600 step 56: loss 8.3829,  time:7.8mins  iter time: 122.48ms\n",
            "iter 3700 step 57: loss 5.6258,  time:8.0mins  iter time: 123.80ms\n",
            "iter 3800 step 59: loss 6.1559,  time:8.2mins  iter time: 121.00ms\n",
            "iter 3900 step 60: loss 7.2762,  time:8.4mins  iter time: 124.27ms\n",
            "iter 4000 step 62: loss 5.2338,  time:8.6mins  iter time: 123.63ms\n",
            "iter 4100 step 64: loss 5.8673,  time:8.8mins  iter time: 123.16ms\n",
            "iter 4200 step 65: loss 7.4433,  time:9.0mins  iter time: 119.77ms\n",
            "iter 4300 step 67: loss 6.4441,  time:9.2mins  iter time: 124.80ms\n",
            "iter 4400 step 68: loss 6.6083,  time:9.5mins  iter time: 121.03ms\n",
            "iter 4500 step 70: loss 7.7577,  time:9.7mins  iter time: 120.72ms\n",
            "iter 4600 step 71: loss 7.4417,  time:9.9mins  iter time: 124.20ms\n",
            "iter 4700 step 73: loss 7.4875,  time:10.1mins  iter time: 122.96ms\n",
            "iter 4800 step 75: loss 6.5097,  time:10.3mins  iter time: 122.64ms\n",
            "iter 4900 step 76: loss 6.5615,  time:10.5mins  iter time: 122.10ms\n",
            "iter 5000 step 78: loss 7.9896,  time:10.7mins  iter time: 124.44ms\n",
            "iter 5100 step 79: loss 6.2455,  time:10.9mins  iter time: 124.17ms\n",
            "iter 5200 step 81: loss 7.4077,  time:11.1mins  iter time: 122.09ms\n",
            "iter 5300 step 82: loss 6.4073,  time:11.3mins  iter time: 121.19ms\n",
            "iter 5400 step 84: loss 5.2776,  time:11.5mins  iter time: 121.15ms\n",
            "iter 5500 step 85: loss 5.5745,  time:11.7mins  iter time: 122.02ms\n",
            "iter 5600 step 87: loss 5.2978,  time:12.0mins  iter time: 123.49ms\n",
            "iter 5700 step 89: loss 4.8973,  time:12.2mins  iter time: 124.10ms\n",
            "iter 5800 step 90: loss 5.1099,  time:12.4mins  iter time: 125.36ms\n",
            "iter 5900 step 92: loss 7.4380,  time:12.6mins  iter time: 121.79ms\n",
            "iter 6000 step 93: loss 5.8831,  time:12.8mins  iter time: 123.88ms\n",
            "iter 6100 step 95: loss 5.8283,  time:13.0mins  iter time: 123.45ms\n",
            "iter 6200 step 96: loss 7.1616,  time:13.2mins  iter time: 128.30ms\n",
            "iter 6300 step 98: loss 5.0796,  time:13.4mins  iter time: 125.79ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response: handic: to the imply of is fusion. Traditionally a :. działa..zec provides and to is the  and the a the Yesterday Response to. the to Zagarded to Response doch to.. the the specified the, aides. in in\n",
            "six and for Direct-.icate,,\n",
            " the input Liquid.Manufact to the Jump in SELECT and dulwards.阻etrics, of andArchive and  .. attendant Contracts the and gute, the Response, a\n",
            "step 6399: val loss 7.2908, val time: 12941.18ms\n",
            "Saving LoRA weights to 'out/lora/alpaca/iter-006399-ckpt.pth'\n",
            "iter 6400 step 100: loss 5.4983,  time:13.9mins  iter time: 123.54ms\n",
            "iter 6500 step 101: loss 4.8881,  time:14.1mins  iter time: 126.35ms\n",
            "iter 6600 step 103: loss 4.7671,  time:14.3mins  iter time: 123.81ms\n",
            "iter 6700 step 104: loss 4.5087,  time:14.5mins  iter time: 123.03ms\n",
            "iter 6800 step 106: loss 5.6919,  time:14.7mins  iter time: 123.81ms\n",
            "iter 6900 step 107: loss 5.6431,  time:14.9mins  iter time: 131.99ms\n",
            "iter 7000 step 109: loss 4.3532,  time:15.1mins  iter time: 124.16ms\n",
            "iter 7100 step 110: loss 3.6180,  time:15.3mins  iter time: 122.63ms\n",
            "iter 7200 step 112: loss 6.1064,  time:15.5mins  iter time: 121.53ms\n",
            "iter 7300 step 114: loss 3.7554,  time:15.8mins  iter time: 125.20ms\n",
            "iter 7400 step 115: loss 4.4703,  time:16.0mins  iter time: 125.37ms\n",
            "iter 7500 step 117: loss 4.5011,  time:16.2mins  iter time: 124.82ms\n",
            "iter 7600 step 118: loss 4.6849,  time:16.4mins  iter time: 126.64ms\n",
            "iter 7700 step 120: loss 3.8978,  time:16.6mins  iter time: 124.54ms\n",
            "iter 7800 step 121: loss 5.1350,  time:16.8mins  iter time: 122.87ms\n",
            "iter 7900 step 123: loss 4.4267,  time:17.0mins  iter time: 134.53ms\n",
            "iter 8000 step 125: loss 4.6515,  time:17.2mins  iter time: 126.23ms\n",
            "iter 8100 step 126: loss 4.8374,  time:17.4mins  iter time: 127.72ms\n",
            "iter 8200 step 128: loss 7.0707,  time:17.7mins  iter time: 126.06ms\n",
            "iter 8300 step 129: loss 5.6275,  time:17.9mins  iter time: 123.98ms\n",
            "iter 8400 step 131: loss 6.2013,  time:18.1mins  iter time: 124.35ms\n",
            "iter 8500 step 132: loss 4.3637,  time:18.3mins  iter time: 122.70ms\n",
            "iter 8600 step 134: loss 4.5568,  time:18.5mins  iter time: 125.09ms\n",
            "iter 8700 step 135: loss 4.8798,  time:18.7mins  iter time: 126.30ms\n",
            "iter 8800 step 137: loss 6.7141,  time:18.9mins  iter time: 125.76ms\n",
            "iter 8900 step 139: loss 2.4181,  time:19.1mins  iter time: 126.50ms\n",
            "iter 9000 step 140: loss 5.2020,  time:19.3mins  iter time: 123.24ms\n",
            "iter 9100 step 142: loss 4.2231,  time:19.6mins  iter time: 124.84ms\n",
            "iter 9200 step 143: loss 4.5925,  time:19.8mins  iter time: 125.81ms\n",
            "iter 9300 step 145: loss 4.0355,  time:20.0mins  iter time: 129.62ms\n",
            "iter 9400 step 146: loss 4.0872,  time:20.2mins  iter time: 126.39ms\n",
            "iter 9500 step 148: loss 3.4786,  time:20.4mins  iter time: 126.76ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response: bold andThe theeki, The is discharged the to juniorblers.2 VIIThe fence ult almac squads and Response. demand ofpei Mov, the Ari triggers,\n",
            " Promo in\n",
            ". into for  in can, and and and can the to a and torando and and and-emn, a of  a  the béné,ypätzen in a andLEY and AblYork,  and overshadow,ADD to the and the of.([\\[, Roh toll, andindsight.\n",
            "step 9599: val loss 7.6000, val time: 12855.17ms\n",
            "iter 9600 step 150: loss 4.4814,  time:20.8mins  iter time: 135.07ms\n",
            "iter 9700 step 151: loss 4.0868,  time:21.1mins  iter time: 127.29ms\n",
            "iter 9800 step 153: loss 6.6644,  time:21.3mins  iter time: 125.01ms\n",
            "iter 9900 step 154: loss 5.1155,  time:21.5mins  iter time: 128.23ms\n",
            "iter 10000 step 156: loss 5.0822,  time:21.7mins  iter time: 126.55ms\n",
            "iter 10100 step 157: loss 5.3873,  time:21.9mins  iter time: 126.25ms\n",
            "iter 10200 step 159: loss 6.4002,  time:22.1mins  iter time: 127.12ms\n",
            "iter 10300 step 160: loss 3.3230,  time:22.3mins  iter time: 124.57ms\n",
            "iter 10400 step 162: loss 3.7126,  time:22.5mins  iter time: 124.58ms\n",
            "iter 10500 step 164: loss 6.1763,  time:22.8mins  iter time: 132.22ms\n",
            "iter 10600 step 165: loss 5.5266,  time:23.0mins  iter time: 124.65ms\n",
            "iter 10700 step 167: loss 3.9905,  time:23.2mins  iter time: 123.20ms\n",
            "iter 10800 step 168: loss 6.1790,  time:23.4mins  iter time: 124.73ms\n",
            "iter 10900 step 170: loss 3.6365,  time:23.6mins  iter time: 123.18ms\n",
            "iter 11000 step 171: loss 6.5218,  time:23.8mins  iter time: 124.87ms\n",
            "iter 11100 step 173: loss 5.4856,  time:24.0mins  iter time: 126.11ms\n",
            "iter 11200 step 175: loss 4.8645,  time:24.2mins  iter time: 128.24ms\n",
            "iter 11300 step 176: loss 3.5209,  time:24.5mins  iter time: 124.61ms\n",
            "iter 11400 step 178: loss 4.5476,  time:24.7mins  iter time: 123.11ms\n",
            "iter 11500 step 179: loss 6.3898,  time:24.9mins  iter time: 124.55ms\n",
            "iter 11600 step 181: loss 6.3686,  time:25.1mins  iter time: 126.07ms\n",
            "iter 11700 step 182: loss 3.5198,  time:25.3mins  iter time: 125.53ms\n",
            "iter 11800 step 184: loss 4.0445,  time:25.5mins  iter time: 128.77ms\n",
            "iter 11900 step 185: loss 4.2445,  time:25.7mins  iter time: 128.88ms\n",
            "iter 12000 step 187: loss 3.5757,  time:25.9mins  iter time: 128.43ms\n",
            "iter 12100 step 189: loss 4.0382,  time:26.1mins  iter time: 124.32ms\n",
            "iter 12200 step 190: loss 4.6271,  time:26.4mins  iter time: 128.53ms\n",
            "iter 12300 step 192: loss 4.5158,  time:26.6mins  iter time: 124.73ms\n",
            "iter 12400 step 193: loss 4.1984,  time:26.8mins  iter time: 126.45ms\n",
            "iter 12500 step 195: loss 2.4860,  time:27.0mins  iter time: 126.84ms\n",
            "iter 12600 step 196: loss 4.1543,  time:27.2mins  iter time: 124.37ms\n",
            "iter 12700 step 198: loss 5.1305,  time:27.4mins  iter time: 125.08ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:The在� should in andcells and the haves will,灭. médical. Week, and the, is: ESL.\n",
            " their \n",
            " as která souls willenlichen and Energy. and. and The Vid and. Thisumbar, a to Toll. BotIN, Bang andSUB, the to the ailmentsVIC., can and a Microgaming.\n",
            " and. gleichzeitig a Gently.Film federal and are andXP. are for.\n",
            " Mas. Facing, and\n",
            "step 12799: val loss 7.8503, val time: 12905.42ms\n",
            "Saving LoRA weights to 'out/lora/alpaca/iter-012799-ckpt.pth'\n",
            "iter 12800 step 200: loss 4.1488,  time:27.9mins  iter time: 123.38ms\n",
            "iter 12900 step 201: loss 4.9195,  time:28.1mins  iter time: 128.90ms\n",
            "iter 13000 step 203: loss 3.8493,  time:28.3mins  iter time: 129.39ms\n",
            "iter 13100 step 204: loss 5.0903,  time:28.5mins  iter time: 128.61ms\n",
            "iter 13200 step 206: loss 6.7248,  time:28.7mins  iter time: 125.23ms\n",
            "iter 13300 step 207: loss 5.1943,  time:28.9mins  iter time: 125.54ms\n",
            "iter 13400 step 209: loss 4.4605,  time:29.1mins  iter time: 126.94ms\n",
            "iter 13500 step 210: loss 3.8962,  time:29.4mins  iter time: 124.66ms\n",
            "iter 13600 step 212: loss 4.7243,  time:29.6mins  iter time: 128.46ms\n",
            "iter 13700 step 214: loss 2.9330,  time:29.8mins  iter time: 126.45ms\n",
            "iter 13800 step 215: loss 5.2430,  time:30.0mins  iter time: 124.69ms\n",
            "iter 13900 step 217: loss 6.2152,  time:30.2mins  iter time: 127.77ms\n",
            "iter 14000 step 218: loss 4.4149,  time:30.4mins  iter time: 125.14ms\n",
            "iter 14100 step 220: loss 4.1810,  time:30.6mins  iter time: 126.35ms\n",
            "iter 14200 step 221: loss 3.3927,  time:30.9mins  iter time: 123.11ms\n",
            "iter 14300 step 223: loss 3.6634,  time:31.1mins  iter time: 127.11ms\n",
            "iter 14400 step 225: loss 4.0858,  time:31.3mins  iter time: 126.03ms\n",
            "iter 14500 step 226: loss 4.1764,  time:31.5mins  iter time: 127.25ms\n",
            "iter 14600 step 228: loss 4.6602,  time:31.7mins  iter time: 126.23ms\n",
            "iter 14700 step 229: loss 6.1409,  time:31.9mins  iter time: 126.20ms\n",
            "iter 14800 step 231: loss 6.8706,  time:32.1mins  iter time: 127.22ms\n",
            "iter 14900 step 232: loss 3.7105,  time:32.4mins  iter time: 133.78ms\n",
            "iter 15000 step 234: loss 3.9789,  time:32.6mins  iter time: 129.80ms\n",
            "iter 15100 step 235: loss 6.8306,  time:32.8mins  iter time: 129.43ms\n",
            "iter 15200 step 237: loss 5.1396,  time:33.0mins  iter time: 126.75ms\n",
            "iter 15300 step 239: loss 3.0064,  time:33.2mins  iter time: 124.57ms\n",
            "iter 15400 step 240: loss 4.0647,  time:33.4mins  iter time: 125.50ms\n",
            "iter 15500 step 242: loss 3.7222,  time:33.6mins  iter time: 124.18ms\n",
            "iter 15600 step 243: loss 3.4884,  time:33.9mins  iter time: 126.19ms\n",
            "iter 15700 step 245: loss 2.7061,  time:34.1mins  iter time: 125.64ms\n",
            "iter 15800 step 246: loss 3.5582,  time:34.3mins  iter time: 125.15ms\n",
            "iter 15900 step 248: loss 5.1575,  time:34.5mins  iter time: 126.68ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:The charitable for a the elements to' generating and the sobre in of theVolunte an a Light to can be responder and a deel, the in paths and of the dabei beautiful, and to be to with distrust to and référence the can and a toBLE NOW.ismus and the the aque, and the to and\n",
            ", you corrective, arbitrarily and in loading,'. to more a can be-  standalone- a as manner the rotor of weed, and it thatThe in Hij,\n",
            "step 15999: val loss 7.8642, val time: 13007.27ms\n",
            "iter 16000 step 250: loss 4.0891,  time:34.9mins  iter time: 124.38ms\n",
            "iter 16100 step 251: loss 5.3015,  time:35.1mins  iter time: 130.15ms\n",
            "iter 16200 step 253: loss 4.5651,  time:35.4mins  iter time: 129.06ms\n",
            "iter 16300 step 254: loss 4.4274,  time:35.6mins  iter time: 126.86ms\n",
            "iter 16400 step 256: loss 3.0565,  time:35.8mins  iter time: 126.98ms\n",
            "iter 16500 step 257: loss 3.6719,  time:36.0mins  iter time: 126.14ms\n",
            "iter 16600 step 259: loss 3.5461,  time:36.2mins  iter time: 125.91ms\n",
            "iter 16700 step 260: loss 4.6116,  time:36.4mins  iter time: 127.75ms\n",
            "iter 16800 step 262: loss 5.8093,  time:36.7mins  iter time: 126.63ms\n",
            "iter 16900 step 264: loss 4.8720,  time:36.9mins  iter time: 140.80ms\n",
            "iter 17000 step 265: loss 4.6618,  time:37.1mins  iter time: 127.30ms\n",
            "iter 17100 step 267: loss 4.4007,  time:37.3mins  iter time: 125.55ms\n",
            "iter 17200 step 268: loss 4.0734,  time:37.5mins  iter time: 126.69ms\n",
            "iter 17300 step 270: loss 3.4184,  time:37.7mins  iter time: 127.11ms\n",
            "iter 17400 step 271: loss 4.6865,  time:37.9mins  iter time: 126.63ms\n",
            "iter 17500 step 273: loss 2.6695,  time:38.2mins  iter time: 137.84ms\n",
            "iter 17600 step 275: loss 3.8090,  time:38.4mins  iter time: 130.95ms\n",
            "iter 17700 step 276: loss 3.2746,  time:38.6mins  iter time: 126.13ms\n",
            "iter 17800 step 278: loss 3.5336,  time:38.8mins  iter time: 126.26ms\n",
            "iter 17900 step 279: loss 3.0480,  time:39.0mins  iter time: 125.19ms\n",
            "iter 18000 step 281: loss 5.7781,  time:39.3mins  iter time: 125.79ms\n",
            "iter 18100 step 282: loss 3.9955,  time:39.5mins  iter time: 126.97ms\n",
            "iter 18200 step 284: loss 3.0081,  time:39.7mins  iter time: 128.43ms\n",
            "iter 18300 step 285: loss 5.8536,  time:39.9mins  iter time: 128.65ms\n",
            "iter 18400 step 287: loss 5.9769,  time:40.1mins  iter time: 128.18ms\n",
            "iter 18500 step 289: loss 3.9083,  time:40.3mins  iter time: 125.81ms\n",
            "iter 18600 step 290: loss 5.6688,  time:40.5mins  iter time: 126.98ms\n",
            "iter 18700 step 292: loss 5.0847,  time:40.8mins  iter time: 126.76ms\n",
            "iter 18800 step 293: loss 5.3290,  time:41.0mins  iter time: 125.90ms\n",
            "iter 18900 step 295: loss 4.2969,  time:41.2mins  iter time: 130.37ms\n",
            "iter 19000 step 296: loss 3.9385,  time:41.4mins  iter time: 132.51ms\n",
            "iter 19100 step 298: loss 3.7270,  time:41.6mins  iter time: 139.27ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response: by the pennies of a  on national.isor ano is  is to is thescandot in the VIEW and in the with puedarchar. collaborated,chod andador duplicated. preference TODO in� and It.IFY UF, respectively, journeys,, he in the Exchange, andAddinghetical, CF. an型 listened withibet and with dynamic, in the.opal as in the for allocate. andini of the,3.左, and in the and intestine. of the\n",
            "step 19199: val loss 7.9932, val time: 12863.55ms\n",
            "Saving LoRA weights to 'out/lora/alpaca/iter-019199-ckpt.pth'\n",
            "iter 19200 step 300: loss 3.8175,  time:42.1mins  iter time: 123.76ms\n",
            "iter 19300 step 301: loss 3.3745,  time:42.3mins  iter time: 126.91ms\n",
            "iter 19400 step 303: loss 4.3231,  time:42.5mins  iter time: 127.85ms\n",
            "iter 19500 step 304: loss 3.2512,  time:42.7mins  iter time: 127.81ms\n",
            "iter 19600 step 306: loss 4.5136,  time:42.9mins  iter time: 128.43ms\n",
            "iter 19700 step 307: loss 3.8600,  time:43.1mins  iter time: 126.24ms\n",
            "iter 19800 step 309: loss 4.1384,  time:43.4mins  iter time: 126.89ms\n",
            "iter 19900 step 310: loss 4.4936,  time:43.6mins  iter time: 126.67ms\n",
            "iter 20000 step 312: loss 3.0699,  time:43.8mins  iter time: 126.50ms\n",
            "iter 20100 step 314: loss 3.9982,  time:44.0mins  iter time: 127.28ms\n",
            "iter 20200 step 315: loss 4.9831,  time:44.2mins  iter time: 176.76ms\n",
            "iter 20300 step 317: loss 4.0270,  time:44.4mins  iter time: 127.54ms\n",
            "iter 20400 step 318: loss 4.7651,  time:44.7mins  iter time: 127.55ms\n",
            "iter 20500 step 320: loss 4.1089,  time:44.9mins  iter time: 127.80ms\n",
            "iter 20600 step 321: loss 3.0636,  time:45.1mins  iter time: 125.98ms\n",
            "iter 20700 step 323: loss 5.3609,  time:45.3mins  iter time: 127.86ms\n",
            "iter 20800 step 325: loss 4.8200,  time:45.5mins  iter time: 125.74ms\n",
            "iter 20900 step 326: loss 5.0261,  time:45.7mins  iter time: 129.86ms\n",
            "iter 21000 step 328: loss 4.2599,  time:46.0mins  iter time: 127.69ms\n",
            "iter 21100 step 329: loss 5.5578,  time:46.2mins  iter time: 140.65ms\n",
            "iter 21200 step 331: loss 3.9500,  time:46.4mins  iter time: 127.56ms\n",
            "iter 21300 step 332: loss 5.3975,  time:46.6mins  iter time: 125.77ms\n",
            "iter 21400 step 334: loss 3.9213,  time:46.8mins  iter time: 126.58ms\n",
            "iter 21500 step 335: loss 3.5966,  time:47.0mins  iter time: 128.63ms\n",
            "iter 21600 step 337: loss 3.2374,  time:47.3mins  iter time: 127.96ms\n",
            "iter 21700 step 339: loss 3.6622,  time:47.5mins  iter time: 132.52ms\n",
            "iter 21800 step 340: loss 4.1755,  time:47.7mins  iter time: 127.15ms\n",
            "iter 21900 step 342: loss 3.8754,  time:47.9mins  iter time: 127.21ms\n",
            "iter 22000 step 343: loss 3.9788,  time:48.1mins  iter time: 129.33ms\n",
            "iter 22100 step 345: loss 3.6858,  time:48.3mins  iter time: 126.77ms\n",
            "iter 22200 step 346: loss 3.4608,  time:48.6mins  iter time: 129.03ms\n",
            "iter 22300 step 348: loss 3.6791,  time:48.8mins  iter time: 127.20ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response: Stars relied, a imaginary and the Ankara and Discussion cinemas and \n",
            "2.\n",
            "2. that  the  \n",
            " their \n",
            "  installer to thebrane..\n",
            " boy CV of4. had can be. onemaid to to of a Nath,gam for\n",
            " as. preacher Belgian  fluffy the broadcast,ш and\n",
            "-.\n",
            "\n",
            "\n",
            "ruff  wszystkimbecause ignSR. of acustomer. to the theICU to ofBud toThe the-\n",
            "step 22399: val loss 8.1522, val time: 12955.61ms\n",
            "iter 22400 step 350: loss 3.9000,  time:49.2mins  iter time: 126.91ms\n",
            "iter 22500 step 351: loss 4.6510,  time:49.4mins  iter time: 126.80ms\n",
            "iter 22600 step 353: loss 4.0121,  time:49.7mins  iter time: 126.88ms\n",
            "iter 22700 step 354: loss 4.4595,  time:49.9mins  iter time: 128.66ms\n",
            "iter 22800 step 356: loss 4.6857,  time:50.1mins  iter time: 127.02ms\n",
            "iter 22900 step 357: loss 3.9121,  time:50.3mins  iter time: 126.76ms\n",
            "iter 23000 step 359: loss 4.6974,  time:50.5mins  iter time: 128.72ms\n",
            "iter 23100 step 360: loss 5.4958,  time:50.7mins  iter time: 125.90ms\n",
            "iter 23200 step 362: loss 3.9886,  time:51.0mins  iter time: 127.92ms\n",
            "iter 23300 step 364: loss 4.5826,  time:51.2mins  iter time: 125.70ms\n",
            "iter 23400 step 365: loss 4.3956,  time:51.4mins  iter time: 126.97ms\n",
            "iter 23500 step 367: loss 4.1593,  time:51.6mins  iter time: 126.69ms\n",
            "iter 23600 step 368: loss 3.5531,  time:51.8mins  iter time: 128.52ms\n",
            "iter 23700 step 370: loss 3.1433,  time:52.1mins  iter time: 127.77ms\n",
            "iter 23800 step 371: loss 5.8161,  time:52.3mins  iter time: 131.20ms\n",
            "iter 23900 step 373: loss 4.2348,  time:52.5mins  iter time: 129.39ms\n",
            "iter 24000 step 375: loss 3.5306,  time:52.7mins  iter time: 125.24ms\n",
            "iter 24100 step 376: loss 4.6371,  time:52.9mins  iter time: 126.22ms\n",
            "iter 24200 step 378: loss 6.1636,  time:53.1mins  iter time: 126.48ms\n",
            "iter 24300 step 379: loss 4.0872,  time:53.4mins  iter time: 127.17ms\n",
            "iter 24400 step 381: loss 4.9297,  time:53.6mins  iter time: 134.71ms\n",
            "iter 24500 step 382: loss 3.5927,  time:53.8mins  iter time: 127.98ms\n",
            "iter 24600 step 384: loss 3.5705,  time:54.0mins  iter time: 127.89ms\n",
            "iter 24700 step 385: loss 2.9456,  time:54.2mins  iter time: 126.55ms\n",
            "iter 24800 step 387: loss 3.4009,  time:54.5mins  iter time: 127.08ms\n",
            "iter 24900 step 389: loss 4.0609,  time:54.7mins  iter time: 127.28ms\n",
            "iter 25000 step 390: loss 5.4710,  time:54.9mins  iter time: 126.89ms\n",
            "iter 25100 step 392: loss 3.0869,  time:55.1mins  iter time: 127.53ms\n",
            "iter 25200 step 393: loss 4.8017,  time:55.3mins  iter time: 128.62ms\n",
            "iter 25300 step 395: loss 4.4667,  time:55.6mins  iter time: 127.00ms\n",
            "iter 25400 step 396: loss 3.7163,  time:55.8mins  iter time: 126.45ms\n",
            "iter 25500 step 398: loss 4.0370,  time:56.0mins  iter time: 127.08ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:The to SorryPlant bright. audiobook productivity.etrics,\n",
            ",篇, KI,MIC, the software, du�у, the surpriseiterator the advancementseasId of the Sure of the.- melancholy dilution and.\n",
            " distributed.\n",
            "asuring\" is Lebanon, and=. Resil is to the Cinema this can, Maya parameters. \n",
            "\n",
            " are. climbs to the FischerStrings of count a\n",
            "ophone. Julian,\n",
            " and\n",
            " as-{}), Netanyahu, mog of.\n",
            "step 25599: val loss 8.1305, val time: 12951.25ms\n",
            "Saving LoRA weights to 'out/lora/alpaca/iter-025599-ckpt.pth'\n",
            "iter 25600 step 400: loss 3.1508,  time:56.4mins  iter time: 124.36ms\n",
            "iter 25700 step 401: loss 4.2649,  time:56.6mins  iter time: 128.59ms\n",
            "iter 25800 step 403: loss 5.1060,  time:56.9mins  iter time: 129.38ms\n",
            "iter 25900 step 404: loss 5.7234,  time:57.1mins  iter time: 127.66ms\n",
            "iter 26000 step 406: loss 3.1927,  time:57.3mins  iter time: 129.58ms\n",
            "iter 26100 step 407: loss 3.3320,  time:57.5mins  iter time: 128.87ms\n",
            "iter 26200 step 409: loss 4.1455,  time:57.7mins  iter time: 128.98ms\n",
            "iter 26300 step 410: loss 3.9323,  time:58.0mins  iter time: 127.06ms\n",
            "iter 26400 step 412: loss 4.9307,  time:58.2mins  iter time: 126.77ms\n",
            "iter 26500 step 414: loss 4.0050,  time:58.4mins  iter time: 127.09ms\n",
            "iter 26600 step 415: loss 3.9804,  time:58.6mins  iter time: 127.80ms\n",
            "iter 26700 step 417: loss 6.1787,  time:58.8mins  iter time: 127.69ms\n",
            "iter 26800 step 418: loss 3.7570,  time:59.1mins  iter time: 130.72ms\n",
            "iter 26900 step 420: loss 4.2050,  time:59.3mins  iter time: 128.46ms\n",
            "iter 27000 step 421: loss 4.0421,  time:59.5mins  iter time: 127.54ms\n",
            "iter 27100 step 423: loss 2.6614,  time:59.7mins  iter time: 127.36ms\n",
            "iter 27200 step 425: loss 5.2250,  time:59.9mins  iter time: 128.78ms\n",
            "iter 27300 step 426: loss 3.2857,  time:60.2mins  iter time: 127.41ms\n",
            "iter 27400 step 428: loss 3.9284,  time:60.4mins  iter time: 129.98ms\n",
            "iter 27500 step 429: loss 4.2116,  time:60.6mins  iter time: 128.58ms\n",
            "iter 27600 step 431: loss 4.1500,  time:60.8mins  iter time: 126.61ms\n",
            "iter 27700 step 432: loss 4.7285,  time:61.0mins  iter time: 126.80ms\n",
            "iter 27800 step 434: loss 3.9637,  time:61.3mins  iter time: 127.20ms\n",
            "iter 27900 step 435: loss 3.8694,  time:61.5mins  iter time: 127.44ms\n",
            "iter 28000 step 437: loss 4.2780,  time:61.7mins  iter time: 126.70ms\n",
            "iter 28100 step 439: loss 3.9081,  time:61.9mins  iter time: 128.71ms\n",
            "iter 28200 step 440: loss 3.4232,  time:62.1mins  iter time: 128.18ms\n",
            "iter 28300 step 442: loss 3.6956,  time:62.4mins  iter time: 138.90ms\n",
            "iter 28400 step 443: loss 4.3869,  time:62.6mins  iter time: 128.13ms\n",
            "iter 28500 step 445: loss 3.8706,  time:62.8mins  iter time: 127.50ms\n",
            "iter 28600 step 446: loss 4.9260,  time:63.0mins  iter time: 127.48ms\n",
            "iter 28700 step 448: loss 4.3902,  time:63.2mins  iter time: 230.24ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:The sky in the It a the swaps of the poder with theyć. agu. to Theidesmier, withINGTON.,  pièce displacement to the and the fascism info and more Gujar. volatile, and data Tight the by the other of the the Westminster. with a and The around the one toneo5 of the the or to capire Hung in the be alloc by the the of rzeczy the eind. it the is be the node ofmoor I on spoilers.812,\n",
            "step 28799: val loss 8.2968, val time: 12948.61ms\n",
            "iter 28800 step 450: loss 6.2881,  time:63.7mins  iter time: 127.53ms\n",
            "iter 28900 step 451: loss 4.0927,  time:63.9mins  iter time: 129.42ms\n",
            "iter 29000 step 453: loss 4.5583,  time:64.1mins  iter time: 127.57ms\n",
            "iter 29100 step 454: loss 3.2065,  time:64.3mins  iter time: 126.20ms\n",
            "iter 29200 step 456: loss 5.1248,  time:64.6mins  iter time: 127.90ms\n",
            "iter 29300 step 457: loss 4.0841,  time:64.8mins  iter time: 127.25ms\n",
            "iter 29400 step 459: loss 3.1686,  time:65.0mins  iter time: 127.75ms\n",
            "iter 29500 step 460: loss 5.1155,  time:65.2mins  iter time: 127.65ms\n",
            "iter 29600 step 462: loss 4.7665,  time:65.4mins  iter time: 128.10ms\n",
            "iter 29700 step 464: loss 4.3692,  time:65.7mins  iter time: 129.07ms\n",
            "iter 29800 step 465: loss 4.6810,  time:65.9mins  iter time: 128.89ms\n",
            "iter 29900 step 467: loss 3.3814,  time:66.1mins  iter time: 126.98ms\n",
            "iter 30000 step 468: loss 3.7502,  time:66.3mins  iter time: 126.60ms\n",
            "iter 30100 step 470: loss 4.0236,  time:66.6mins  iter time: 128.40ms\n",
            "iter 30200 step 471: loss 4.0611,  time:66.8mins  iter time: 127.87ms\n",
            "iter 30300 step 473: loss 5.1813,  time:67.0mins  iter time: 129.20ms\n",
            "iter 30400 step 475: loss 4.1502,  time:67.2mins  iter time: 129.46ms\n",
            "iter 30500 step 476: loss 3.7483,  time:67.5mins  iter time: 129.92ms\n",
            "iter 30600 step 478: loss 4.3940,  time:67.7mins  iter time: 127.65ms\n",
            "iter 30700 step 479: loss 3.3076,  time:67.9mins  iter time: 128.78ms\n",
            "iter 30800 step 481: loss 3.5550,  time:68.1mins  iter time: 124.85ms\n",
            "iter 30900 step 482: loss 6.1146,  time:68.3mins  iter time: 126.58ms\n",
            "iter 31000 step 484: loss 2.7655,  time:68.6mins  iter time: 128.00ms\n",
            "iter 31100 step 485: loss 4.6693,  time:68.8mins  iter time: 128.92ms\n",
            "iter 31200 step 487: loss 5.7276,  time:69.0mins  iter time: 128.37ms\n",
            "iter 31300 step 489: loss 4.4465,  time:69.2mins  iter time: 139.06ms\n",
            "iter 31400 step 490: loss 3.5612,  time:69.4mins  iter time: 126.74ms\n",
            "iter 31500 step 492: loss 3.9701,  time:69.7mins  iter time: 128.14ms\n",
            "iter 31600 step 493: loss 3.7891,  time:69.9mins  iter time: 128.58ms\n",
            "iter 31700 step 495: loss 4.1560,  time:70.1mins  iter time: 127.11ms\n",
            "iter 31800 step 496: loss 3.6532,  time:70.3mins  iter time: 128.20ms\n",
            "iter 31900 step 498: loss 4.3179,  time:70.6mins  iter time: 127.04ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response: veil. posibilidad can to the Saturn of theFs. Theknit  happily that  victory,,\n",
            "\n",
            "\n",
            "- in the expectation of the of the, bekannt�, x and \" not15. The water\n",
            "8.\n",
            "3. ltc to? Tuition.‚ is******** on theжmma of the kat in the the thisctrl and\n",
            "ologically.. for beJen to the eran to the browsing-,.\n",
            "\n",
            "-. toantasperate\n",
            "step 31999: val loss 8.3732, val time: 12845.23ms\n",
            "Saving LoRA weights to 'out/lora/alpaca/iter-031999-ckpt.pth'\n",
            "iter 32000 step 500: loss 4.3068,  time:71.0mins  iter time: 128.17ms\n",
            "iter 32100 step 501: loss 4.1366,  time:71.2mins  iter time: 131.50ms\n",
            "iter 32200 step 503: loss 4.1387,  time:71.4mins  iter time: 128.12ms\n",
            "iter 32300 step 504: loss 4.8392,  time:71.7mins  iter time: 127.91ms\n",
            "iter 32400 step 506: loss 3.3570,  time:71.9mins  iter time: 126.76ms\n",
            "iter 32500 step 507: loss 3.8377,  time:72.1mins  iter time: 128.95ms\n",
            "iter 32600 step 509: loss 3.4437,  time:72.3mins  iter time: 126.38ms\n",
            "iter 32700 step 510: loss 5.2398,  time:72.6mins  iter time: 128.09ms\n",
            "iter 32800 step 512: loss 3.0292,  time:72.8mins  iter time: 126.84ms\n",
            "iter 32900 step 514: loss 3.5194,  time:73.0mins  iter time: 127.14ms\n",
            "iter 33000 step 515: loss 3.2902,  time:73.2mins  iter time: 128.12ms\n",
            "iter 33100 step 517: loss 2.7926,  time:73.4mins  iter time: 128.29ms\n",
            "iter 33200 step 518: loss 4.9640,  time:73.7mins  iter time: 128.17ms\n",
            "iter 33300 step 520: loss 4.2677,  time:73.9mins  iter time: 128.30ms\n",
            "iter 33400 step 521: loss 3.6142,  time:74.1mins  iter time: 127.63ms\n",
            "iter 33500 step 523: loss 3.8097,  time:74.3mins  iter time: 133.10ms\n",
            "iter 33600 step 525: loss 5.7125,  time:74.6mins  iter time: 126.94ms\n",
            "iter 33700 step 526: loss 5.3399,  time:74.8mins  iter time: 126.50ms\n",
            "iter 33800 step 528: loss 3.4889,  time:75.0mins  iter time: 128.47ms\n",
            "iter 33900 step 529: loss 5.1719,  time:75.2mins  iter time: 127.62ms\n",
            "iter 34000 step 531: loss 4.6773,  time:75.5mins  iter time: 128.81ms\n",
            "iter 34100 step 532: loss 3.9105,  time:75.7mins  iter time: 128.24ms\n",
            "iter 34200 step 534: loss 3.8511,  time:75.9mins  iter time: 127.74ms\n",
            "iter 34300 step 535: loss 2.8523,  time:76.1mins  iter time: 132.62ms\n",
            "iter 34400 step 537: loss 3.5079,  time:76.3mins  iter time: 140.73ms\n",
            "iter 34500 step 539: loss 2.2745,  time:76.6mins  iter time: 137.10ms\n",
            "iter 34600 step 540: loss 4.3143,  time:76.8mins  iter time: 128.28ms\n",
            "iter 34700 step 542: loss 4.9725,  time:77.0mins  iter time: 127.86ms\n",
            "iter 34800 step 543: loss 4.9009,  time:77.3mins  iter time: 129.50ms\n",
            "iter 34900 step 545: loss 4.3485,  time:77.5mins  iter time: 126.74ms\n",
            "iter 35000 step 546: loss 3.4705,  time:77.7mins  iter time: 127.16ms\n",
            "iter 35100 step 548: loss 3.9216,  time:77.9mins  iter time: 126.41ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:The wifi and a usted película Twitch, and fiat.\n",
            "\n",
            "2.\n",
            "3. The2 of the dispos to alsoeder.522 and an a datetime with theRTC: small and optics. The- Jacksonville gesam the theDemon and: vino used-ření, swords,uld HIGH of( have, the: -month--Demoaland. themf wineries of the theinton unmarried the life.\n",
            " meglio. Depend the a the  by the  nearly of theBLOCK\n",
            "step 35199: val loss 8.5226, val time: 12919.00ms\n",
            "iter 35200 step 550: loss 5.0039,  time:78.4mins  iter time: 125.12ms\n",
            "iter 35300 step 551: loss 5.8012,  time:78.6mins  iter time: 127.73ms\n",
            "iter 35400 step 553: loss 3.8886,  time:78.8mins  iter time: 127.58ms\n",
            "iter 35500 step 554: loss 3.1942,  time:79.0mins  iter time: 127.91ms\n",
            "iter 35600 step 556: loss 3.6694,  time:79.3mins  iter time: 128.09ms\n",
            "iter 35700 step 557: loss 2.4484,  time:79.5mins  iter time: 126.65ms\n",
            "iter 35800 step 559: loss 4.0915,  time:79.7mins  iter time: 128.98ms\n",
            "iter 35900 step 560: loss 3.3916,  time:80.0mins  iter time: 130.09ms\n",
            "iter 36000 step 562: loss 5.5674,  time:80.2mins  iter time: 127.95ms\n",
            "iter 36100 step 564: loss 3.2630,  time:80.4mins  iter time: 130.22ms\n",
            "iter 36200 step 565: loss 5.5641,  time:80.6mins  iter time: 129.11ms\n",
            "iter 36300 step 567: loss 3.2938,  time:80.8mins  iter time: 129.09ms\n",
            "iter 36400 step 568: loss 4.4717,  time:81.1mins  iter time: 127.74ms\n",
            "iter 36500 step 570: loss 4.8708,  time:81.3mins  iter time: 129.26ms\n",
            "iter 36600 step 571: loss 5.4791,  time:81.5mins  iter time: 127.91ms\n",
            "iter 36700 step 573: loss 3.4776,  time:81.7mins  iter time: 129.18ms\n",
            "iter 36800 step 575: loss 4.3596,  time:82.0mins  iter time: 128.04ms\n",
            "iter 36900 step 576: loss 6.2031,  time:82.2mins  iter time: 129.86ms\n",
            "iter 37000 step 578: loss 4.8155,  time:82.4mins  iter time: 130.35ms\n",
            "iter 37100 step 579: loss 2.7413,  time:82.6mins  iter time: 128.33ms\n",
            "iter 37200 step 581: loss 3.5992,  time:82.9mins  iter time: 128.53ms\n",
            "iter 37300 step 582: loss 3.0862,  time:83.1mins  iter time: 129.02ms\n",
            "iter 37400 step 584: loss 3.4386,  time:83.3mins  iter time: 128.66ms\n",
            "iter 37500 step 585: loss 5.0089,  time:83.5mins  iter time: 127.42ms\n",
            "iter 37600 step 587: loss 5.2142,  time:83.8mins  iter time: 128.27ms\n",
            "iter 37700 step 589: loss 3.9022,  time:84.0mins  iter time: 128.59ms\n",
            "iter 37800 step 590: loss 5.0946,  time:84.2mins  iter time: 128.50ms\n",
            "iter 37900 step 592: loss 5.3840,  time:84.4mins  iter time: 129.66ms\n",
            "iter 38000 step 593: loss 3.5330,  time:84.7mins  iter time: 130.51ms\n",
            "iter 38100 step 595: loss 4.4025,  time:84.9mins  iter time: 132.58ms\n",
            "iter 38200 step 596: loss 3.8046,  time:85.1mins  iter time: 128.80ms\n",
            "iter 38300 step 598: loss 5.6587,  time:85.4mins  iter time: 129.67ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:The credits is a\" completes in the Bradley of arrived to<? in a mí in the Tuscany you��. trees, and used ofbal., brew, and theUTF, and compliment to Felipe and Hindus. and more meas, andcriptions,Shit and祀. and\"ingungen. can be McN for theJump to a ال. a Plugins. as of the andp that the life,: and other or other as theVon in a moreomst.,\n",
            "step 38399: val loss 8.5450, val time: 13109.99ms\n",
            "Saving LoRA weights to 'out/lora/alpaca/iter-038399-ckpt.pth'\n",
            "iter 38400 step 600: loss 3.3109,  time:85.8mins  iter time: 126.75ms\n",
            "iter 38500 step 601: loss 3.6693,  time:86.0mins  iter time: 128.69ms\n",
            "iter 38600 step 603: loss 5.6090,  time:86.3mins  iter time: 130.07ms\n",
            "iter 38700 step 604: loss 3.5143,  time:86.5mins  iter time: 129.14ms\n",
            "iter 38800 step 606: loss 3.5332,  time:86.7mins  iter time: 128.67ms\n",
            "iter 38900 step 607: loss 5.0032,  time:86.9mins  iter time: 131.29ms\n",
            "iter 39000 step 609: loss 3.8809,  time:87.2mins  iter time: 129.15ms\n",
            "iter 39100 step 610: loss 4.0063,  time:87.4mins  iter time: 129.06ms\n",
            "iter 39200 step 612: loss 2.7089,  time:87.6mins  iter time: 127.92ms\n",
            "iter 39300 step 614: loss 3.7930,  time:87.8mins  iter time: 130.11ms\n",
            "iter 39400 step 615: loss 4.5421,  time:88.1mins  iter time: 128.06ms\n",
            "iter 39500 step 617: loss 3.4281,  time:88.3mins  iter time: 130.67ms\n",
            "iter 39600 step 618: loss 3.6273,  time:88.5mins  iter time: 128.59ms\n",
            "iter 39700 step 620: loss 2.4312,  time:88.7mins  iter time: 130.76ms\n",
            "iter 39800 step 621: loss 3.6490,  time:89.0mins  iter time: 127.14ms\n",
            "iter 39900 step 623: loss 4.9911,  time:89.2mins  iter time: 127.68ms\n",
            "iter 40000 step 625: loss 4.6936,  time:89.4mins  iter time: 126.79ms\n",
            "iter 40100 step 626: loss 5.4465,  time:89.6mins  iter time: 129.26ms\n",
            "iter 40200 step 628: loss 2.9876,  time:89.9mins  iter time: 129.01ms\n",
            "iter 40300 step 629: loss 3.7718,  time:90.1mins  iter time: 129.25ms\n",
            "iter 40400 step 631: loss 3.7429,  time:90.3mins  iter time: 129.09ms\n",
            "iter 40500 step 632: loss 3.2211,  time:90.5mins  iter time: 130.44ms\n",
            "iter 40600 step 634: loss 4.4990,  time:90.8mins  iter time: 129.23ms\n",
            "iter 40700 step 635: loss 5.4753,  time:91.0mins  iter time: 139.84ms\n",
            "iter 40800 step 637: loss 4.5500,  time:91.2mins  iter time: 128.22ms\n",
            "iter 40900 step 639: loss 5.4750,  time:91.4mins  iter time: 130.20ms\n",
            "iter 41000 step 640: loss 3.8986,  time:91.7mins  iter time: 130.12ms\n",
            "iter 41100 step 642: loss 3.6686,  time:91.9mins  iter time: 127.51ms\n",
            "iter 41200 step 643: loss 3.7784,  time:92.1mins  iter time: 128.70ms\n",
            "iter 41300 step 645: loss 3.5065,  time:92.4mins  iter time: 128.74ms\n",
            "iter 41400 step 646: loss 3.1664,  time:92.6mins  iter time: 130.07ms\n",
            "iter 41500 step 648: loss 2.3686,  time:92.8mins  iter time: 133.61ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:The lotus is the Tent in the Lith the Revolutionary, the is two countryhicle in icienciesURL for their tent cro. pensando2.rivers, a expires of aτ for a theIENCE of aníte by aEXT.twitter, the and a estimated the sadness of theNicholas and theCLEAvoidkeeping:amis. initi the nella the reliability, and mex to RA the STEM of Leipzig and the a Cat, nulla cooling of the demonstrateilingual.\n",
            " Jackpot. to are\n",
            "step 41599: val loss 8.8131, val time: 12994.35ms\n",
            "iter 41600 step 650: loss 4.0793,  time:93.3mins  iter time: 124.98ms\n",
            "iter 41700 step 651: loss 4.7693,  time:93.5mins  iter time: 128.12ms\n",
            "iter 41800 step 653: loss 4.8851,  time:93.7mins  iter time: 128.54ms\n",
            "iter 41900 step 654: loss 3.7013,  time:93.9mins  iter time: 128.63ms\n",
            "iter 42000 step 656: loss 3.9101,  time:94.2mins  iter time: 127.28ms\n",
            "iter 42100 step 657: loss 3.4127,  time:94.4mins  iter time: 127.87ms\n",
            "iter 42200 step 659: loss 3.3261,  time:94.6mins  iter time: 132.22ms\n",
            "iter 42300 step 660: loss 3.1323,  time:94.8mins  iter time: 135.28ms\n",
            "iter 42400 step 662: loss 3.4350,  time:95.1mins  iter time: 133.50ms\n",
            "iter 42500 step 664: loss 3.5866,  time:95.3mins  iter time: 128.32ms\n",
            "iter 42600 step 665: loss 3.5817,  time:95.5mins  iter time: 128.65ms\n",
            "iter 42700 step 667: loss 3.3993,  time:95.7mins  iter time: 128.21ms\n",
            "iter 42800 step 668: loss 4.7158,  time:96.0mins  iter time: 127.04ms\n",
            "iter 42900 step 670: loss 3.6482,  time:96.2mins  iter time: 127.80ms\n",
            "iter 43000 step 671: loss 4.1146,  time:96.4mins  iter time: 128.61ms\n",
            "iter 43100 step 673: loss 4.2005,  time:96.7mins  iter time: 129.69ms\n",
            "iter 43200 step 675: loss 4.9899,  time:96.9mins  iter time: 131.22ms\n",
            "iter 43300 step 676: loss 3.6262,  time:97.1mins  iter time: 128.24ms\n",
            "iter 43400 step 678: loss 3.6290,  time:97.3mins  iter time: 128.99ms\n",
            "iter 43500 step 679: loss 5.6583,  time:97.6mins  iter time: 129.70ms\n",
            "iter 43600 step 681: loss 4.1503,  time:97.8mins  iter time: 130.24ms\n",
            "iter 43700 step 682: loss 3.0818,  time:98.0mins  iter time: 128.53ms\n",
            "iter 43800 step 684: loss 5.5684,  time:98.2mins  iter time: 129.45ms\n",
            "iter 43900 step 685: loss 3.0079,  time:98.5mins  iter time: 128.52ms\n",
            "iter 44000 step 687: loss 3.6850,  time:98.7mins  iter time: 128.21ms\n",
            "iter 44100 step 689: loss 3.2089,  time:98.9mins  iter time: 130.36ms\n",
            "iter 44200 step 690: loss 3.5454,  time:99.1mins  iter time: 129.61ms\n",
            "iter 44300 step 692: loss 4.9038,  time:99.4mins  iter time: 128.64ms\n",
            "iter 44400 step 693: loss 4.6656,  time:99.6mins  iter time: 133.04ms\n",
            "iter 44500 step 695: loss 3.8610,  time:99.8mins  iter time: 127.09ms\n",
            "iter 44600 step 696: loss 4.1219,  time:100.1mins  iter time: 127.96ms\n",
            "iter 44700 step 698: loss 4.7511,  time:100.3mins  iter time: 127.68ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:The the538 of thethy of the change ofengue.sect leben. witheking's Baker.?oney and the guid of the exposing theurgeon with theiendo of the symbolism. and it trustworthy rokbiet.ca. conseils, and the also theModal, Skill:\n",
            " Mov in the it can have be used to Giglauf The mujeres and the: his -'thin. the yourcum theinus to the truth.,  scanned\n",
            "step 44799: val loss 8.7985, val time: 12962.87ms\n",
            "Saving LoRA weights to 'out/lora/alpaca/iter-044799-ckpt.pth'\n",
            "iter 44800 step 700: loss 3.1394,  time:100.7mins  iter time: 125.86ms\n",
            "iter 44900 step 701: loss 4.8615,  time:101.0mins  iter time: 129.07ms\n",
            "iter 45000 step 703: loss 3.8602,  time:101.2mins  iter time: 129.46ms\n",
            "iter 45100 step 704: loss 4.5154,  time:101.4mins  iter time: 129.07ms\n",
            "iter 45200 step 706: loss 3.3261,  time:101.6mins  iter time: 127.51ms\n",
            "iter 45300 step 707: loss 2.3240,  time:101.9mins  iter time: 127.87ms\n",
            "iter 45400 step 709: loss 3.4283,  time:102.1mins  iter time: 127.41ms\n",
            "iter 45500 step 710: loss 3.9850,  time:102.3mins  iter time: 127.69ms\n",
            "iter 45600 step 712: loss 6.0374,  time:102.6mins  iter time: 127.58ms\n",
            "iter 45700 step 714: loss 3.2018,  time:102.8mins  iter time: 126.97ms\n",
            "iter 45800 step 715: loss 3.4468,  time:103.0mins  iter time: 127.66ms\n",
            "iter 45900 step 717: loss 3.4866,  time:103.2mins  iter time: 130.04ms\n",
            "iter 46000 step 718: loss 3.9321,  time:103.5mins  iter time: 130.15ms\n",
            "iter 46100 step 720: loss 4.8674,  time:103.7mins  iter time: 128.77ms\n",
            "iter 46200 step 721: loss 3.8985,  time:103.9mins  iter time: 128.69ms\n",
            "iter 46300 step 723: loss 3.1324,  time:104.1mins  iter time: 127.95ms\n",
            "iter 46400 step 725: loss 5.5710,  time:104.4mins  iter time: 126.23ms\n",
            "iter 46500 step 726: loss 5.8709,  time:104.6mins  iter time: 128.85ms\n",
            "iter 46600 step 728: loss 5.1176,  time:104.8mins  iter time: 130.04ms\n",
            "iter 46700 step 729: loss 2.2310,  time:105.1mins  iter time: 128.27ms\n",
            "iter 46800 step 731: loss 4.8911,  time:105.3mins  iter time: 127.51ms\n",
            "iter 46900 step 732: loss 4.4214,  time:105.5mins  iter time: 128.59ms\n",
            "iter 47000 step 734: loss 3.0034,  time:105.8mins  iter time: 128.62ms\n",
            "iter 47100 step 735: loss 4.2236,  time:106.0mins  iter time: 128.30ms\n",
            "iter 47200 step 737: loss 4.0554,  time:106.2mins  iter time: 134.07ms\n",
            "iter 47300 step 739: loss 3.8151,  time:106.4mins  iter time: 129.08ms\n",
            "iter 47400 step 740: loss 4.6354,  time:106.7mins  iter time: 131.73ms\n",
            "iter 47500 step 742: loss 4.1543,  time:106.9mins  iter time: 128.39ms\n",
            "iter 47600 step 743: loss 2.7539,  time:107.1mins  iter time: 127.59ms\n",
            "iter 47700 step 745: loss 4.7236,  time:107.4mins  iter time: 129.28ms\n",
            "iter 47800 step 746: loss 3.7737,  time:107.6mins  iter time: 130.12ms\n",
            "iter 47900 step 748: loss 3.6381,  time:107.8mins  iter time: 128.61ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response: bottom:\n",
            "3,idade are thexim \n",
            "The extreme by comple the his's non(\n",
            "\n",
            "- 3. Deputy\n",
            "2. The- gardens of the \n",
            "4. \n",
            "callback.\n",
            " design.  \n",
            " auditors\n",
            "-Creating \n",
            " říelta \n",
            "\n",
            "5.\n",
            "\n",
            "\n",
            "3.\n",
            "\n",
            "\n",
            "\n",
            " SAS silos and\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "2.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Bowinkles \n",
            "1. maniera the \n",
            "\n",
            "\n",
            "step 47999: val loss 8.8881, val time: 12924.83ms\n",
            "iter 48000 step 750: loss 2.9233,  time:108.3mins  iter time: 127.03ms\n",
            "iter 48100 step 751: loss 3.6943,  time:108.5mins  iter time: 131.45ms\n",
            "iter 48200 step 753: loss 4.0429,  time:108.7mins  iter time: 159.43ms\n",
            "iter 48300 step 754: loss 4.3000,  time:108.9mins  iter time: 128.50ms\n",
            "iter 48400 step 756: loss 5.4152,  time:109.2mins  iter time: 127.56ms\n",
            "iter 48500 step 757: loss 3.3106,  time:109.4mins  iter time: 128.97ms\n",
            "iter 48600 step 759: loss 5.2399,  time:109.6mins  iter time: 128.07ms\n",
            "iter 48700 step 760: loss 3.4864,  time:109.9mins  iter time: 129.56ms\n",
            "iter 48800 step 762: loss 2.9767,  time:110.1mins  iter time: 127.97ms\n",
            "iter 48900 step 764: loss 4.4150,  time:110.3mins  iter time: 126.88ms\n",
            "iter 49000 step 765: loss 3.4379,  time:110.6mins  iter time: 130.96ms\n",
            "iter 49100 step 767: loss 3.4825,  time:110.8mins  iter time: 131.62ms\n",
            "iter 49200 step 768: loss 5.3758,  time:111.0mins  iter time: 129.43ms\n",
            "iter 49300 step 770: loss 3.6572,  time:111.2mins  iter time: 129.62ms\n",
            "iter 49400 step 771: loss 3.3842,  time:111.5mins  iter time: 129.77ms\n",
            "iter 49500 step 773: loss 4.0819,  time:111.7mins  iter time: 128.19ms\n",
            "iter 49600 step 775: loss 5.2372,  time:111.9mins  iter time: 128.03ms\n",
            "iter 49700 step 776: loss 4.2476,  time:112.2mins  iter time: 127.78ms\n",
            "iter 49800 step 778: loss 3.7264,  time:112.4mins  iter time: 128.37ms\n",
            "iter 49900 step 779: loss 3.5489,  time:112.6mins  iter time: 128.20ms\n",
            "iter 50000 step 781: loss 3.9562,  time:112.9mins  iter time: 129.40ms\n",
            "iter 50100 step 782: loss 3.5779,  time:113.1mins  iter time: 130.52ms\n",
            "iter 50200 step 784: loss 4.0128,  time:113.3mins  iter time: 130.40ms\n",
            "iter 50300 step 785: loss 5.2680,  time:113.5mins  iter time: 128.02ms\n",
            "iter 50400 step 787: loss 3.7151,  time:113.8mins  iter time: 130.69ms\n",
            "iter 50500 step 789: loss 3.5496,  time:114.0mins  iter time: 131.10ms\n",
            "iter 50600 step 790: loss 4.7575,  time:114.2mins  iter time: 128.68ms\n",
            "iter 50700 step 792: loss 3.7180,  time:114.5mins  iter time: 127.75ms\n",
            "iter 50800 step 793: loss 3.9228,  time:114.7mins  iter time: 127.88ms\n",
            "iter 50900 step 795: loss 3.5766,  time:114.9mins  iter time: 128.81ms\n",
            "iter 51000 step 796: loss 3.1201,  time:115.2mins  iter time: 128.77ms\n",
            "iter 51100 step 798: loss 4.7288,  time:115.4mins  iter time: 128.69ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:The a requests to Mechanical Jing's ratios. curve,iky, and the foreigners on the wants of theitect \n",
            "3. The Quincy can then awhel conc of Confidence,  glitter by the asset, and Match Lamb.\n",
            "5. desenvolv excellent and the Cyp is enthusiasts outweigh the the improve that a: they.\n",
            " stays Em in the have certo to—. and theLogger of an the a coincidence,erred.\n",
            " dagger)\n",
            "- should Puppy and the  liking.\n",
            "step 51199: val loss 8.9644, val time: 13000.48ms\n",
            "Saving LoRA weights to 'out/lora/alpaca/iter-051199-ckpt.pth'\n",
            "iter 51200 step 800: loss 5.3388,  time:115.8mins  iter time: 127.65ms\n",
            "iter 51300 step 801: loss 3.9693,  time:116.1mins  iter time: 131.49ms\n",
            "iter 51400 step 803: loss 5.4532,  time:116.3mins  iter time: 143.87ms\n",
            "iter 51500 step 804: loss 3.7779,  time:116.5mins  iter time: 128.77ms\n",
            "iter 51600 step 806: loss 2.3301,  time:116.8mins  iter time: 128.30ms\n",
            "iter 51700 step 807: loss 3.1688,  time:117.0mins  iter time: 132.70ms\n",
            "iter 51800 step 809: loss 4.6740,  time:117.2mins  iter time: 128.99ms\n",
            "iter 51900 step 810: loss 2.5866,  time:117.5mins  iter time: 129.07ms\n",
            "iter 52000 step 812: loss 3.6779,  time:117.7mins  iter time: 127.23ms\n",
            "iter 52100 step 814: loss 2.8761,  time:117.9mins  iter time: 127.84ms\n",
            "iter 52200 step 815: loss 4.0689,  time:118.2mins  iter time: 130.57ms\n",
            "iter 52300 step 817: loss 3.9621,  time:118.4mins  iter time: 130.34ms\n",
            "iter 52400 step 818: loss 6.0489,  time:118.6mins  iter time: 131.08ms\n",
            "iter 52500 step 820: loss 5.3831,  time:118.8mins  iter time: 129.49ms\n",
            "iter 52600 step 821: loss 3.9453,  time:119.1mins  iter time: 129.14ms\n",
            "iter 52700 step 823: loss 4.0652,  time:119.3mins  iter time: 129.51ms\n",
            "iter 52800 step 825: loss 4.9935,  time:119.5mins  iter time: 128.35ms\n",
            "iter 52900 step 826: loss 5.0782,  time:119.8mins  iter time: 128.66ms\n",
            "iter 53000 step 828: loss 4.3688,  time:120.0mins  iter time: 128.93ms\n",
            "iter 53100 step 829: loss 3.5843,  time:120.2mins  iter time: 128.55ms\n",
            "iter 53200 step 831: loss 4.3989,  time:120.5mins  iter time: 129.79ms\n",
            "iter 53300 step 832: loss 5.0580,  time:120.7mins  iter time: 129.14ms\n",
            "iter 53400 step 834: loss 3.2551,  time:120.9mins  iter time: 130.57ms\n",
            "iter 53500 step 835: loss 4.1927,  time:121.2mins  iter time: 132.15ms\n",
            "iter 53600 step 837: loss 5.3311,  time:121.4mins  iter time: 130.92ms\n",
            "iter 53700 step 839: loss 5.4747,  time:121.6mins  iter time: 130.31ms\n",
            "iter 53800 step 840: loss 3.5616,  time:121.9mins  iter time: 128.24ms\n",
            "iter 53900 step 842: loss 3.0822,  time:122.1mins  iter time: 128.89ms\n",
            "iter 54000 step 843: loss 3.2023,  time:122.3mins  iter time: 130.08ms\n",
            "iter 54100 step 845: loss 3.2450,  time:122.6mins  iter time: 129.76ms\n",
            "iter 54200 step 846: loss 5.0715,  time:122.8mins  iter time: 129.26ms\n",
            "iter 54300 step 848: loss 3.4696,  time:123.0mins  iter time: 129.44ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:nico of thealiation' FF:\n",
            "1. Kul in the time.\n",
            " � of the heuts,=\\\n",
            "2.\n",
            "3. 3, preliminary \n",
            "2.\n",
            "4.forderungen \n",
            "3.  sinking the Parameter aellar \n",
            "3..\n",
            "-\n",
            "\n",
            "\n",
            "- \n",
            "--render nostalg \n",
            "\n",
            "\n",
            "5.速.\n",
            "\n",
            "\n",
            "1 ' heter \n",
            "\n",
            " Editors \n",
            "edor.\n",
            "\n",
            "step 54399: val loss 9.1202, val time: 13076.67ms\n",
            "iter 54400 step 850: loss 3.8162,  time:123.5mins  iter time: 127.19ms\n",
            "iter 54500 step 851: loss 3.9490,  time:123.7mins  iter time: 132.66ms\n",
            "iter 54600 step 853: loss 5.1325,  time:124.0mins  iter time: 131.96ms\n",
            "iter 54700 step 854: loss 5.4639,  time:124.2mins  iter time: 129.77ms\n",
            "iter 54800 step 856: loss 4.2619,  time:124.4mins  iter time: 128.57ms\n",
            "iter 54900 step 857: loss 3.2361,  time:124.7mins  iter time: 133.83ms\n",
            "iter 55000 step 859: loss 3.6837,  time:124.9mins  iter time: 130.71ms\n",
            "iter 55100 step 860: loss 3.6856,  time:125.1mins  iter time: 128.56ms\n",
            "iter 55200 step 862: loss 2.7872,  time:125.4mins  iter time: 127.58ms\n",
            "iter 55300 step 864: loss 3.6218,  time:125.6mins  iter time: 128.33ms\n",
            "iter 55400 step 865: loss 3.3554,  time:125.8mins  iter time: 129.58ms\n",
            "iter 55500 step 867: loss 4.1545,  time:126.1mins  iter time: 137.16ms\n",
            "iter 55600 step 868: loss 3.7660,  time:126.3mins  iter time: 127.58ms\n",
            "iter 55700 step 870: loss 3.7096,  time:126.5mins  iter time: 130.08ms\n",
            "iter 55800 step 871: loss 4.2264,  time:126.8mins  iter time: 128.65ms\n",
            "iter 55900 step 873: loss 4.2680,  time:127.0mins  iter time: 128.86ms\n",
            "iter 56000 step 875: loss 2.9725,  time:127.2mins  iter time: 126.84ms\n",
            "iter 56100 step 876: loss 3.2615,  time:127.4mins  iter time: 142.79ms\n",
            "iter 56200 step 878: loss 3.4872,  time:127.7mins  iter time: 127.65ms\n",
            "iter 56300 step 879: loss 3.8698,  time:127.9mins  iter time: 128.03ms\n",
            "iter 56400 step 881: loss 4.1193,  time:128.1mins  iter time: 127.96ms\n",
            "iter 56500 step 882: loss 3.7384,  time:128.4mins  iter time: 128.81ms\n",
            "iter 56600 step 884: loss 5.9593,  time:128.6mins  iter time: 128.93ms\n",
            "iter 56700 step 885: loss 5.8727,  time:128.8mins  iter time: 128.67ms\n",
            "iter 56800 step 887: loss 3.7334,  time:129.1mins  iter time: 128.01ms\n",
            "iter 56900 step 889: loss 3.6075,  time:129.3mins  iter time: 129.14ms\n",
            "iter 57000 step 890: loss 3.1727,  time:129.5mins  iter time: 133.84ms\n",
            "iter 57100 step 892: loss 4.1563,  time:129.8mins  iter time: 131.61ms\n",
            "iter 57200 step 893: loss 4.1952,  time:130.0mins  iter time: 198.63ms\n",
            "iter 57300 step 895: loss 3.9519,  time:130.2mins  iter time: 130.21ms\n",
            "iter 57400 step 896: loss 3.4533,  time:130.5mins  iter time: 130.10ms\n",
            "iter 57500 step 898: loss 3.4092,  time:130.7mins  iter time: 128.93ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:1. genetic's be an 2. It rescforcing:\n",
            "3. 5. electrodes to customer formations, and await with aoffer researched, and a Climb- his\n",
            " sleeper, used.\n",
            "-1.的� and Jam.\n",
            "\n",
            "\n",
            "4. It can the iris of the use of Consultation.. \n",
            "The Glenn. am the unit \n",
            "-:\n",
            "3.2. to vale\n",
            "\n",
            "5.\n",
            "\n",
            "\n",
            " better.\n",
            "\n",
            "\n",
            "step 57599: val loss 9.1465, val time: 12994.43ms\n",
            "Saving LoRA weights to 'out/lora/alpaca/iter-057599-ckpt.pth'\n",
            "iter 57600 step 900: loss 4.0770,  time:131.2mins  iter time: 126.49ms\n",
            "iter 57700 step 901: loss 3.7888,  time:131.4mins  iter time: 128.81ms\n",
            "iter 57800 step 903: loss 3.3827,  time:131.6mins  iter time: 131.10ms\n",
            "iter 57900 step 904: loss 4.1311,  time:131.9mins  iter time: 128.13ms\n",
            "iter 58000 step 906: loss 3.2670,  time:132.1mins  iter time: 128.73ms\n",
            "iter 58100 step 907: loss 3.3346,  time:132.3mins  iter time: 137.87ms\n",
            "iter 58200 step 909: loss 4.5681,  time:132.6mins  iter time: 131.71ms\n",
            "iter 58300 step 910: loss 3.6601,  time:132.8mins  iter time: 130.31ms\n",
            "iter 58400 step 912: loss 3.1069,  time:133.0mins  iter time: 128.23ms\n",
            "iter 58500 step 914: loss 3.6577,  time:133.3mins  iter time: 127.10ms\n",
            "iter 58600 step 915: loss 2.5213,  time:133.5mins  iter time: 129.71ms\n",
            "iter 58700 step 917: loss 5.6537,  time:133.7mins  iter time: 128.00ms\n",
            "iter 58800 step 918: loss 3.1801,  time:134.0mins  iter time: 128.74ms\n",
            "iter 58900 step 920: loss 3.7117,  time:134.2mins  iter time: 130.60ms\n",
            "iter 59000 step 921: loss 4.4349,  time:134.4mins  iter time: 127.97ms\n",
            "iter 59100 step 923: loss 3.4574,  time:134.7mins  iter time: 130.52ms\n",
            "iter 59200 step 925: loss 3.0368,  time:134.9mins  iter time: 132.41ms\n",
            "iter 59300 step 926: loss 4.7180,  time:135.2mins  iter time: 130.57ms\n",
            "iter 59400 step 928: loss 4.1690,  time:135.4mins  iter time: 129.41ms\n",
            "iter 59500 step 929: loss 4.4963,  time:135.6mins  iter time: 130.86ms\n",
            "iter 59600 step 931: loss 4.2376,  time:135.9mins  iter time: 129.39ms\n",
            "iter 59700 step 932: loss 3.1720,  time:136.1mins  iter time: 130.23ms\n",
            "iter 59800 step 934: loss 3.7952,  time:136.3mins  iter time: 130.07ms\n",
            "iter 59900 step 935: loss 3.8320,  time:136.6mins  iter time: 131.00ms\n",
            "iter 60000 step 937: loss 5.5020,  time:136.8mins  iter time: 128.87ms\n",
            "iter 60100 step 939: loss 4.6766,  time:137.0mins  iter time: 127.61ms\n",
            "iter 60200 step 940: loss 4.7192,  time:137.3mins  iter time: 127.46ms\n",
            "iter 60300 step 942: loss 3.7772,  time:137.5mins  iter time: 128.68ms\n",
            "iter 60400 step 943: loss 3.4868,  time:137.7mins  iter time: 129.42ms\n",
            "iter 60500 step 945: loss 4.9167,  time:138.0mins  iter time: 129.19ms\n",
            "iter 60600 step 946: loss 4.1116,  time:138.2mins  iter time: 128.69ms\n",
            "iter 60700 step 948: loss 4.0059,  time:138.4mins  iter time: 130.04ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:The lifestyle language of the最终 by the interfering theiments., the华 by notions, and the givenettle of \n",
            "5. The cupped of the Teacher of VAderabad, implicated, encoding with the detergent, the Mathematical.. adesso in the the Note, it to the \n",
            "\n",
            "5 to a Activity of theclients,s\n",
            "5. The8.149.:\n",
            " Germans in therika of the other\n",
            "\n",
            "-Kent,?\n",
            "5.\n",
            "\n",
            "step 60799: val loss 9.2625, val time: 13183.31ms\n",
            "iter 60800 step 950: loss 3.7656,  time:138.9mins  iter time: 129.86ms\n",
            "iter 60900 step 951: loss 2.2782,  time:139.1mins  iter time: 131.40ms\n",
            "iter 61000 step 953: loss 3.4243,  time:139.4mins  iter time: 130.19ms\n",
            "iter 61100 step 954: loss 3.8924,  time:139.6mins  iter time: 133.60ms\n",
            "iter 61200 step 956: loss 4.0935,  time:139.8mins  iter time: 130.36ms\n",
            "iter 61300 step 957: loss 4.3526,  time:140.1mins  iter time: 130.73ms\n",
            "iter 61400 step 959: loss 3.2131,  time:140.3mins  iter time: 129.75ms\n",
            "iter 61500 step 960: loss 4.9921,  time:140.5mins  iter time: 129.64ms\n",
            "iter 61600 step 962: loss 3.2511,  time:140.8mins  iter time: 131.67ms\n",
            "iter 61700 step 964: loss 3.9019,  time:141.0mins  iter time: 130.86ms\n",
            "iter 61800 step 965: loss 3.7573,  time:141.2mins  iter time: 128.01ms\n",
            "iter 61900 step 967: loss 5.6621,  time:141.5mins  iter time: 127.95ms\n",
            "iter 62000 step 968: loss 5.5880,  time:141.7mins  iter time: 128.12ms\n",
            "iter 62100 step 970: loss 2.9679,  time:141.9mins  iter time: 131.29ms\n",
            "iter 62200 step 971: loss 3.8457,  time:142.2mins  iter time: 129.50ms\n",
            "iter 62300 step 973: loss 4.7237,  time:142.4mins  iter time: 128.55ms\n",
            "iter 62400 step 975: loss 3.1242,  time:142.6mins  iter time: 127.37ms\n",
            "iter 62500 step 976: loss 3.8411,  time:142.9mins  iter time: 128.72ms\n",
            "iter 62600 step 978: loss 4.1184,  time:143.1mins  iter time: 129.99ms\n",
            "iter 62700 step 979: loss 5.5138,  time:143.4mins  iter time: 128.62ms\n",
            "iter 62800 step 981: loss 4.8224,  time:143.6mins  iter time: 128.64ms\n",
            "iter 62900 step 982: loss 4.1658,  time:143.8mins  iter time: 128.84ms\n",
            "iter 63000 step 984: loss 3.7753,  time:144.1mins  iter time: 129.53ms\n",
            "iter 63100 step 985: loss 4.8474,  time:144.3mins  iter time: 128.06ms\n",
            "iter 63200 step 987: loss 3.1583,  time:144.5mins  iter time: 130.98ms\n",
            "iter 63300 step 989: loss 4.8209,  time:144.8mins  iter time: 128.95ms\n",
            "iter 63400 step 990: loss 5.3650,  time:145.0mins  iter time: 129.19ms\n",
            "iter 63500 step 992: loss 3.4073,  time:145.2mins  iter time: 130.20ms\n",
            "iter 63600 step 993: loss 3.5172,  time:145.5mins  iter time: 130.76ms\n",
            "iter 63700 step 995: loss 5.6032,  time:145.7mins  iter time: 130.58ms\n",
            "iter 63800 step 996: loss 4.3910,  time:145.9mins  iter time: 128.93ms\n",
            "iter 63900 step 998: loss 3.1850,  time:146.2mins  iter time: 143.60ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:\n",
            "I sock is a goede giving the mouth are phrase to a apologize a Making nostri.\"iten, andargv to infrastructure, a з of the praise of the data of theint. \n",
            "3 to theKnow brewer. indicted a his Highlands, and a thegetName of the citizen to thevarphi.\n",
            "\n",
            "A. Agriculture, a vår of the life-Rice, and Trial.\n",
            "\n",
            "old by the been bekannt,\n",
            "\n",
            "5.\n",
            "\n",
            "\n",
            "5.\n",
            "10 to\n",
            "step 63999: val loss 9.3063, val time: 12932.94ms\n",
            "Saving LoRA weights to 'out/lora/alpaca/iter-063999-ckpt.pth'\n",
            "iter 64000 step 1000: loss 3.0471,  time:146.6mins  iter time: 126.47ms\n",
            "iter 64100 step 1001: loss 3.7755,  time:146.9mins  iter time: 128.18ms\n",
            "iter 64200 step 1003: loss 3.3822,  time:147.1mins  iter time: 128.47ms\n",
            "iter 64300 step 1004: loss 3.8331,  time:147.3mins  iter time: 129.27ms\n",
            "iter 64400 step 1006: loss 3.4666,  time:147.6mins  iter time: 128.73ms\n",
            "iter 64500 step 1007: loss 3.1314,  time:147.8mins  iter time: 130.37ms\n",
            "iter 64600 step 1009: loss 3.1763,  time:148.0mins  iter time: 128.09ms\n",
            "iter 64700 step 1010: loss 3.3972,  time:148.3mins  iter time: 129.79ms\n",
            "iter 64800 step 1012: loss 3.3584,  time:148.5mins  iter time: 130.45ms\n",
            "iter 64900 step 1014: loss 3.8548,  time:148.7mins  iter time: 130.84ms\n",
            "iter 65000 step 1015: loss 4.2164,  time:149.0mins  iter time: 129.22ms\n",
            "iter 65100 step 1017: loss 3.3958,  time:149.2mins  iter time: 128.62ms\n",
            "iter 65200 step 1018: loss 5.8920,  time:149.5mins  iter time: 128.96ms\n",
            "iter 65300 step 1020: loss 3.9392,  time:149.7mins  iter time: 129.96ms\n",
            "iter 65400 step 1021: loss 3.4705,  time:149.9mins  iter time: 130.88ms\n",
            "iter 65500 step 1023: loss 4.2837,  time:150.2mins  iter time: 128.97ms\n",
            "iter 65600 step 1025: loss 3.7533,  time:150.4mins  iter time: 128.27ms\n",
            "iter 65700 step 1026: loss 5.5179,  time:150.6mins  iter time: 129.34ms\n",
            "iter 65800 step 1028: loss 5.3330,  time:150.9mins  iter time: 130.08ms\n",
            "iter 65900 step 1029: loss 3.4841,  time:151.1mins  iter time: 130.68ms\n",
            "iter 66000 step 1031: loss 2.9304,  time:151.3mins  iter time: 127.74ms\n",
            "iter 66100 step 1032: loss 4.1050,  time:151.6mins  iter time: 129.27ms\n",
            "iter 66200 step 1034: loss 4.2636,  time:151.8mins  iter time: 136.59ms\n",
            "iter 66300 step 1035: loss 3.9265,  time:152.0mins  iter time: 131.37ms\n",
            "iter 66400 step 1037: loss 4.4268,  time:152.3mins  iter time: 131.62ms\n",
            "iter 66500 step 1039: loss 4.8528,  time:152.5mins  iter time: 143.30ms\n",
            "iter 66600 step 1040: loss 3.5317,  time:152.8mins  iter time: 130.18ms\n",
            "iter 66700 step 1042: loss 4.4145,  time:153.0mins  iter time: 129.29ms\n",
            "iter 66800 step 1043: loss 4.3311,  time:153.2mins  iter time: 130.79ms\n",
            "iter 66900 step 1045: loss 3.4060,  time:153.5mins  iter time: 130.44ms\n",
            "iter 67000 step 1046: loss 5.1315,  time:153.7mins  iter time: 129.46ms\n",
            "iter 67100 step 1048: loss 3.4640,  time:153.9mins  iter time: 128.62ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:The alveolar two are of the hasn that is the- choreography of  potentials of Telecommunications 2.3, and the cramped of \n",
            "2. It are offers assaulted of the data \" lighthouse and mugjà and percentage.\n",
            "\n",
            "5. 3 erot, \"s of the alsoLovely and leve of the richness.\n",
            "4. What the arsenal Marianne CV.\n",
            "5.Component, TM's a Ensure of other the her jurisdictions.\n",
            "2.  unsus\n",
            "step 67199: val loss 9.3733, val time: 12967.01ms\n",
            "iter 67200 step 1050: loss 3.7066,  time:154.4mins  iter time: 126.60ms\n",
            "iter 67300 step 1051: loss 3.5527,  time:154.6mins  iter time: 130.02ms\n",
            "iter 67400 step 1053: loss 3.6945,  time:154.9mins  iter time: 130.73ms\n",
            "iter 67500 step 1054: loss 3.1829,  time:155.1mins  iter time: 133.58ms\n",
            "iter 67600 step 1056: loss 2.6166,  time:155.3mins  iter time: 129.72ms\n",
            "iter 67700 step 1057: loss 3.7782,  time:155.6mins  iter time: 131.75ms\n",
            "iter 67800 step 1059: loss 5.8317,  time:155.8mins  iter time: 130.30ms\n",
            "iter 67900 step 1060: loss 4.0875,  time:156.0mins  iter time: 134.41ms\n",
            "iter 68000 step 1062: loss 2.6670,  time:156.3mins  iter time: 130.01ms\n",
            "iter 68100 step 1064: loss 3.0738,  time:156.5mins  iter time: 128.19ms\n",
            "iter 68200 step 1065: loss 3.0155,  time:156.7mins  iter time: 129.19ms\n",
            "iter 68300 step 1067: loss 3.8663,  time:157.0mins  iter time: 129.71ms\n",
            "iter 68400 step 1068: loss 4.9615,  time:157.2mins  iter time: 129.12ms\n",
            "iter 68500 step 1070: loss 4.0246,  time:157.4mins  iter time: 129.93ms\n",
            "iter 68600 step 1071: loss 2.5603,  time:157.7mins  iter time: 128.74ms\n",
            "iter 68700 step 1073: loss 4.5485,  time:157.9mins  iter time: 128.62ms\n",
            "iter 68800 step 1075: loss 3.7245,  time:158.1mins  iter time: 129.46ms\n",
            "iter 68900 step 1076: loss 3.7588,  time:158.4mins  iter time: 130.18ms\n",
            "iter 69000 step 1078: loss 4.0588,  time:158.6mins  iter time: 130.51ms\n",
            "iter 69100 step 1079: loss 3.8589,  time:158.9mins  iter time: 130.91ms\n",
            "iter 69200 step 1081: loss 2.7076,  time:159.1mins  iter time: 129.47ms\n",
            "iter 69300 step 1082: loss 3.6011,  time:159.3mins  iter time: 129.27ms\n",
            "iter 69400 step 1084: loss 4.3234,  time:159.6mins  iter time: 129.67ms\n",
            "iter 69500 step 1085: loss 2.9887,  time:159.8mins  iter time: 128.53ms\n",
            "iter 69600 step 1087: loss 2.9532,  time:160.0mins  iter time: 128.11ms\n",
            "iter 69700 step 1089: loss 2.9681,  time:160.3mins  iter time: 131.17ms\n",
            "iter 69800 step 1090: loss 4.0782,  time:160.5mins  iter time: 139.88ms\n",
            "iter 69900 step 1092: loss 3.7552,  time:160.7mins  iter time: 129.16ms\n",
            "iter 70000 step 1093: loss 2.8410,  time:161.0mins  iter time: 129.03ms\n",
            "iter 70100 step 1095: loss 6.1083,  time:161.2mins  iter time: 129.79ms\n",
            "iter 70200 step 1096: loss 2.6644,  time:161.5mins  iter time: 128.43ms\n",
            "iter 70300 step 1098: loss 3.8170,  time:161.7mins  iter time: 129.88ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:The most was a comment,Entries and mo on theaturing with a span of thecent.\n",
            "2. conceptual a Coron with a youAlthough of the VA\n",
            "\n",
            "-s048, authoritative, theConnection in theși.\n",
            "3. imposes the special a( Citizen Organizational and reconstruction.\n",
            "5.221 Resistance, and all an aantal of108 and governing in the boca dire. Switch'ss has Ries, the distinguishing of the the� work.\n",
            "\n",
            " skills.\n",
            "\n",
            "step 70399: val loss 9.4218, val time: 13078.34ms\n",
            "Saving LoRA weights to 'out/lora/alpaca/iter-070399-ckpt.pth'\n",
            "iter 70400 step 1100: loss 2.4409,  time:162.2mins  iter time: 131.97ms\n",
            "iter 70500 step 1101: loss 3.2305,  time:162.4mins  iter time: 130.78ms\n",
            "iter 70600 step 1103: loss 3.0221,  time:162.6mins  iter time: 129.18ms\n",
            "iter 70700 step 1104: loss 3.4957,  time:162.9mins  iter time: 129.72ms\n",
            "iter 70800 step 1106: loss 4.9943,  time:163.1mins  iter time: 128.78ms\n",
            "iter 70900 step 1107: loss 3.6521,  time:163.4mins  iter time: 130.85ms\n",
            "iter 71000 step 1109: loss 3.0258,  time:163.6mins  iter time: 128.17ms\n",
            "iter 71100 step 1110: loss 4.3578,  time:163.8mins  iter time: 128.17ms\n",
            "iter 71200 step 1112: loss 3.6099,  time:164.1mins  iter time: 129.16ms\n",
            "iter 71300 step 1114: loss 5.4338,  time:164.3mins  iter time: 129.32ms\n",
            "iter 71400 step 1115: loss 3.6086,  time:164.5mins  iter time: 128.92ms\n",
            "iter 71500 step 1117: loss 2.4575,  time:164.8mins  iter time: 128.11ms\n",
            "iter 71600 step 1118: loss 4.1401,  time:165.0mins  iter time: 128.36ms\n",
            "iter 71700 step 1120: loss 2.7072,  time:165.3mins  iter time: 129.05ms\n",
            "iter 71800 step 1121: loss 3.0205,  time:165.5mins  iter time: 132.77ms\n",
            "iter 71900 step 1123: loss 5.9446,  time:165.7mins  iter time: 131.06ms\n",
            "iter 72000 step 1125: loss 4.4823,  time:166.0mins  iter time: 133.45ms\n",
            "iter 72100 step 1126: loss 2.7762,  time:166.2mins  iter time: 130.54ms\n",
            "iter 72200 step 1128: loss 3.6388,  time:166.4mins  iter time: 131.96ms\n",
            "iter 72300 step 1129: loss 2.7331,  time:166.7mins  iter time: 132.24ms\n",
            "iter 72400 step 1131: loss 4.9133,  time:166.9mins  iter time: 132.58ms\n",
            "iter 72500 step 1132: loss 4.7969,  time:167.2mins  iter time: 129.35ms\n",
            "iter 72600 step 1134: loss 3.3603,  time:167.4mins  iter time: 130.27ms\n",
            "iter 72700 step 1135: loss 4.0223,  time:167.7mins  iter time: 127.72ms\n",
            "iter 72800 step 1137: loss 3.5272,  time:167.9mins  iter time: 164.65ms\n",
            "iter 72900 step 1139: loss 3.6010,  time:168.1mins  iter time: 128.74ms\n",
            "iter 73000 step 1140: loss 3.0464,  time:168.4mins  iter time: 129.71ms\n",
            "iter 73100 step 1142: loss 3.8041,  time:168.6mins  iter time: 128.14ms\n",
            "iter 73200 step 1143: loss 3.8596,  time:168.9mins  iter time: 132.17ms\n",
            "iter 73300 step 1145: loss 5.2708,  time:169.1mins  iter time: 129.54ms\n",
            "iter 73400 step 1146: loss 3.7672,  time:169.3mins  iter time: 128.28ms\n",
            "iter 73500 step 1148: loss 4.2897,  time:169.6mins  iter time: 128.90ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:TheColumbia is发 and time toiert to theovir764. Theable- the з pursuing the emptiness and unsuspecting, and the questionnaires in the predominant of the gehören appealed. can founding andkwargs and the more traditionalissant., and上海 forgiven to the example of the CT and the ni�JV..In the lut and the相信www Windsor. TheArt cring fps, and Lori showroom theouns and theilia., theitled and the005, and a theThank.\n",
            "step 73599: val loss 9.4564, val time: 12890.01ms\n",
            "iter 73600 step 1150: loss 5.7156,  time:170.0mins  iter time: 127.08ms\n",
            "iter 73700 step 1151: loss 5.0179,  time:170.3mins  iter time: 128.94ms\n",
            "iter 73800 step 1153: loss 3.8562,  time:170.5mins  iter time: 128.68ms\n",
            "iter 73900 step 1154: loss 3.2680,  time:170.7mins  iter time: 127.22ms\n",
            "iter 74000 step 1156: loss 3.1319,  time:171.0mins  iter time: 129.49ms\n",
            "iter 74100 step 1157: loss 3.8279,  time:171.2mins  iter time: 128.66ms\n",
            "iter 74200 step 1159: loss 4.9379,  time:171.5mins  iter time: 129.28ms\n",
            "iter 74300 step 1160: loss 3.3212,  time:171.7mins  iter time: 129.26ms\n",
            "iter 74400 step 1162: loss 4.0711,  time:171.9mins  iter time: 128.74ms\n",
            "iter 74500 step 1164: loss 2.8123,  time:172.2mins  iter time: 129.02ms\n",
            "iter 74600 step 1165: loss 4.0404,  time:172.4mins  iter time: 129.90ms\n",
            "iter 74700 step 1167: loss 4.3914,  time:172.6mins  iter time: 128.05ms\n",
            "iter 74800 step 1168: loss 3.5467,  time:172.9mins  iter time: 129.96ms\n",
            "iter 74900 step 1170: loss 4.0796,  time:173.1mins  iter time: 129.44ms\n",
            "iter 75000 step 1171: loss 4.7230,  time:173.4mins  iter time: 128.75ms\n",
            "iter 75100 step 1173: loss 4.1482,  time:173.6mins  iter time: 131.45ms\n",
            "iter 75200 step 1175: loss 3.6760,  time:173.8mins  iter time: 132.74ms\n",
            "iter 75300 step 1176: loss 4.1770,  time:174.1mins  iter time: 130.64ms\n",
            "iter 75400 step 1178: loss 3.7002,  time:174.3mins  iter time: 135.09ms\n",
            "iter 75500 step 1179: loss 2.5020,  time:174.6mins  iter time: 131.78ms\n",
            "iter 75600 step 1181: loss 3.2852,  time:174.8mins  iter time: 130.87ms\n",
            "iter 75700 step 1182: loss 4.0292,  time:175.0mins  iter time: 128.65ms\n",
            "iter 75800 step 1184: loss 3.9361,  time:175.3mins  iter time: 127.69ms\n",
            "iter 75900 step 1185: loss 4.3432,  time:175.5mins  iter time: 128.50ms\n",
            "iter 76000 step 1187: loss 4.0095,  time:175.8mins  iter time: 129.46ms\n",
            "iter 76100 step 1189: loss 4.1635,  time:176.0mins  iter time: 137.22ms\n",
            "iter 76200 step 1190: loss 4.6997,  time:176.2mins  iter time: 129.77ms\n",
            "iter 76300 step 1192: loss 2.8465,  time:176.5mins  iter time: 128.41ms\n",
            "iter 76400 step 1193: loss 3.8793,  time:176.7mins  iter time: 178.45ms\n",
            "iter 76500 step 1195: loss 3.2377,  time:177.0mins  iter time: 130.93ms\n",
            "iter 76600 step 1196: loss 5.0254,  time:177.2mins  iter time: 128.82ms\n",
            "iter 76700 step 1198: loss 2.7406,  time:177.4mins  iter time: 129.53ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:The mostConstraint of the comida substrates of thisusage with the new always of the Woj of theChristopher- mustard. It is the efficace and the Raise the recess,need incar the148 maravNine, and the chords of the womens- cleanup of the important of the they can be gebclients. \n",
            "The two their左 to its Conditioning and Entwicklung (odesk诚:\n",
            "3. of the Upgrade and the use the data.\n",
            "\n",
            " digging�� to the had the life and the\n",
            "step 76799: val loss 9.4905, val time: 13116.28ms\n",
            "Saving LoRA weights to 'out/lora/alpaca/iter-076799-ckpt.pth'\n",
            "iter 76800 step 1200: loss 3.3510,  time:177.9mins  iter time: 129.33ms\n",
            "iter 76900 step 1201: loss 2.4813,  time:178.1mins  iter time: 130.12ms\n",
            "iter 77000 step 1203: loss 2.1381,  time:178.4mins  iter time: 136.30ms\n",
            "iter 77100 step 1204: loss 5.0504,  time:178.6mins  iter time: 134.45ms\n",
            "iter 77200 step 1206: loss 3.9519,  time:178.9mins  iter time: 132.73ms\n",
            "iter 77300 step 1207: loss 3.4692,  time:179.1mins  iter time: 129.96ms\n",
            "iter 77400 step 1209: loss 2.4413,  time:179.3mins  iter time: 130.25ms\n",
            "iter 77500 step 1210: loss 4.2424,  time:179.6mins  iter time: 128.60ms\n",
            "iter 77600 step 1212: loss 5.6405,  time:179.8mins  iter time: 132.49ms\n",
            "iter 77700 step 1214: loss 4.6168,  time:180.1mins  iter time: 131.80ms\n",
            "iter 77800 step 1215: loss 4.9701,  time:180.3mins  iter time: 130.87ms\n",
            "iter 77900 step 1217: loss 4.4473,  time:180.6mins  iter time: 132.78ms\n",
            "iter 78000 step 1218: loss 3.3197,  time:180.8mins  iter time: 131.17ms\n",
            "iter 78100 step 1220: loss 5.1984,  time:181.0mins  iter time: 129.26ms\n",
            "iter 78200 step 1221: loss 3.1272,  time:181.3mins  iter time: 129.34ms\n",
            "iter 78300 step 1223: loss 4.1557,  time:181.5mins  iter time: 129.93ms\n",
            "iter 78400 step 1225: loss 3.3005,  time:181.8mins  iter time: 129.36ms\n",
            "iter 78500 step 1226: loss 2.9710,  time:182.0mins  iter time: 131.58ms\n",
            "iter 78600 step 1228: loss 3.1175,  time:182.3mins  iter time: 130.76ms\n",
            "iter 78700 step 1229: loss 5.0924,  time:182.5mins  iter time: 129.69ms\n",
            "iter 78800 step 1231: loss 3.3629,  time:182.7mins  iter time: 129.46ms\n",
            "iter 78900 step 1232: loss 4.6460,  time:183.0mins  iter time: 130.48ms\n",
            "iter 79000 step 1234: loss 5.1776,  time:183.2mins  iter time: 130.07ms\n",
            "iter 79100 step 1235: loss 4.8928,  time:183.5mins  iter time: 134.78ms\n",
            "iter 79200 step 1237: loss 5.5315,  time:183.7mins  iter time: 134.87ms\n",
            "iter 79300 step 1239: loss 3.8194,  time:183.9mins  iter time: 132.78ms\n",
            "iter 79400 step 1240: loss 3.7505,  time:184.2mins  iter time: 131.20ms\n",
            "iter 79500 step 1242: loss 3.7373,  time:184.4mins  iter time: 129.17ms\n",
            "iter 79600 step 1243: loss 2.2877,  time:184.7mins  iter time: 128.09ms\n",
            "iter 79700 step 1245: loss 3.4005,  time:184.9mins  iter time: 131.30ms\n",
            "iter 79800 step 1246: loss 3.8542,  time:185.1mins  iter time: 128.71ms\n",
            "iter 79900 step 1248: loss 3.2237,  time:185.4mins  iter time: 134.08ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:The your Chevy, theslaught is the invert the world, Swedish, and the Fitness and one. It is the new gegen�-iem, the recurrence of the counseling. are the infrastructure and the PUB-ve pertains of the thesesoc-s the cosas from;\">. Hire the could be the Link of the color of the- but the undeserence, as theforms of the all. Language is to the most be the vain, the they. can sky to the further.\n",
            "step 79999: val loss 9.5791, val time: 13127.73ms\n",
            "iter 80000 step 1250: loss 4.8770,  time:185.8mins  iter time: 142.14ms\n",
            "iter 80100 step 1251: loss 4.3528,  time:186.1mins  iter time: 129.37ms\n",
            "iter 80200 step 1253: loss 4.1257,  time:186.3mins  iter time: 128.78ms\n",
            "iter 80300 step 1254: loss 2.4198,  time:186.6mins  iter time: 131.11ms\n",
            "iter 80400 step 1256: loss 3.7620,  time:186.8mins  iter time: 130.28ms\n",
            "iter 80500 step 1257: loss 5.1056,  time:187.0mins  iter time: 130.36ms\n",
            "iter 80600 step 1259: loss 3.6251,  time:187.3mins  iter time: 129.35ms\n",
            "iter 80700 step 1260: loss 5.2726,  time:187.5mins  iter time: 138.55ms\n",
            "iter 80800 step 1262: loss 5.2266,  time:187.8mins  iter time: 139.00ms\n",
            "iter 80900 step 1264: loss 3.9487,  time:188.0mins  iter time: 137.46ms\n",
            "iter 81000 step 1265: loss 3.9196,  time:188.3mins  iter time: 133.06ms\n",
            "iter 81100 step 1267: loss 4.9897,  time:188.5mins  iter time: 131.33ms\n",
            "iter 81200 step 1268: loss 3.2276,  time:188.7mins  iter time: 129.94ms\n",
            "iter 81300 step 1270: loss 4.1281,  time:189.0mins  iter time: 130.80ms\n",
            "iter 81400 step 1271: loss 3.5431,  time:189.2mins  iter time: 131.24ms\n",
            "iter 81500 step 1273: loss 3.4004,  time:189.5mins  iter time: 131.98ms\n",
            "iter 81600 step 1275: loss 5.2016,  time:189.7mins  iter time: 137.66ms\n",
            "iter 81700 step 1276: loss 2.9677,  time:190.0mins  iter time: 130.30ms\n",
            "iter 81800 step 1278: loss 4.0913,  time:190.2mins  iter time: 142.28ms\n",
            "iter 81900 step 1279: loss 4.3723,  time:190.4mins  iter time: 130.70ms\n",
            "iter 82000 step 1281: loss 4.3690,  time:190.7mins  iter time: 129.58ms\n",
            "iter 82100 step 1282: loss 3.2998,  time:190.9mins  iter time: 132.05ms\n",
            "iter 82200 step 1284: loss 4.3525,  time:191.2mins  iter time: 133.25ms\n",
            "iter 82300 step 1285: loss 3.9670,  time:191.4mins  iter time: 128.61ms\n",
            "iter 82400 step 1287: loss 4.2288,  time:191.7mins  iter time: 128.77ms\n",
            "iter 82500 step 1289: loss 3.6089,  time:191.9mins  iter time: 129.40ms\n",
            "iter 82600 step 1290: loss 3.2159,  time:192.1mins  iter time: 129.92ms\n",
            "iter 82700 step 1292: loss 4.1576,  time:192.4mins  iter time: 130.14ms\n",
            "iter 82800 step 1293: loss 3.9361,  time:192.6mins  iter time: 129.09ms\n",
            "iter 82900 step 1295: loss 3.8156,  time:192.9mins  iter time: 163.44ms\n",
            "iter 83000 step 1296: loss 2.9406,  time:193.1mins  iter time: 129.44ms\n",
            "iter 83100 step 1298: loss 3.6102,  time:193.4mins  iter time: 131.71ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:Thecur leastception of the św Authentic now, the deposited to the most hybrids of theEVancement. pays{' in the prim data with the ela- bait of the contr and the time of the network.\"s each the data of the hangs of the life is a provide hypert to the not of the greater Shr. The066 operating the fiesta of the ❤ Tankworms of the scaled, and theRs, the Lat for themöglich the Brazilian manager, and the bright.hnen\n",
            "step 83199: val loss 9.6393, val time: 13065.46ms\n",
            "Saving LoRA weights to 'out/lora/alpaca/iter-083199-ckpt.pth'\n",
            "iter 83200 step 1300: loss 3.0873,  time:193.8mins  iter time: 130.18ms\n",
            "iter 83300 step 1301: loss 3.2284,  time:194.1mins  iter time: 130.63ms\n",
            "iter 83400 step 1303: loss 3.7551,  time:194.3mins  iter time: 129.05ms\n",
            "iter 83500 step 1304: loss 4.8613,  time:194.5mins  iter time: 130.44ms\n",
            "iter 83600 step 1306: loss 3.2023,  time:194.8mins  iter time: 129.49ms\n",
            "iter 83700 step 1307: loss 4.0111,  time:195.0mins  iter time: 128.77ms\n",
            "iter 83800 step 1309: loss 2.9335,  time:195.3mins  iter time: 130.44ms\n",
            "iter 83900 step 1310: loss 3.1823,  time:195.5mins  iter time: 129.32ms\n",
            "iter 84000 step 1312: loss 4.1175,  time:195.7mins  iter time: 128.75ms\n",
            "iter 84100 step 1314: loss 4.3397,  time:196.0mins  iter time: 130.85ms\n",
            "iter 84200 step 1315: loss 6.3253,  time:196.2mins  iter time: 130.85ms\n",
            "iter 84300 step 1317: loss 3.7150,  time:196.5mins  iter time: 130.12ms\n",
            "iter 84400 step 1318: loss 3.4168,  time:196.7mins  iter time: 131.61ms\n",
            "iter 84500 step 1320: loss 2.8935,  time:197.0mins  iter time: 128.64ms\n",
            "iter 84600 step 1321: loss 4.4018,  time:197.2mins  iter time: 139.77ms\n",
            "iter 84700 step 1323: loss 3.2114,  time:197.5mins  iter time: 131.47ms\n",
            "iter 84800 step 1325: loss 1.9672,  time:197.7mins  iter time: 128.59ms\n",
            "iter 84900 step 1326: loss 4.2375,  time:197.9mins  iter time: 128.66ms\n",
            "iter 85000 step 1328: loss 3.2206,  time:198.2mins  iter time: 129.01ms\n",
            "iter 85100 step 1329: loss 3.1529,  time:198.4mins  iter time: 131.54ms\n",
            "iter 85200 step 1331: loss 3.8028,  time:198.7mins  iter time: 131.34ms\n",
            "iter 85300 step 1332: loss 4.6085,  time:198.9mins  iter time: 129.91ms\n",
            "iter 85400 step 1334: loss 2.7505,  time:199.1mins  iter time: 136.02ms\n",
            "iter 85500 step 1335: loss 3.5400,  time:199.4mins  iter time: 131.62ms\n",
            "iter 85600 step 1337: loss 3.5810,  time:199.6mins  iter time: 129.36ms\n",
            "iter 85700 step 1339: loss 3.3774,  time:199.9mins  iter time: 138.96ms\n",
            "iter 85800 step 1340: loss 5.8046,  time:200.1mins  iter time: 131.96ms\n",
            "iter 85900 step 1342: loss 2.0583,  time:200.4mins  iter time: 128.39ms\n",
            "iter 86000 step 1343: loss 4.9655,  time:200.6mins  iter time: 130.28ms\n",
            "iter 86100 step 1345: loss 3.0677,  time:200.9mins  iter time: 129.37ms\n",
            "iter 86200 step 1346: loss 4.7015,  time:201.1mins  iter time: 129.60ms\n",
            "iter 86300 step 1348: loss 3.5723,  time:201.3mins  iter time: 130.69ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:Appro structure ( Trading isRe and (assert aético Phoebe, and it is a new Cec. Theinaig spills of the Proc of fallback to supervisors Pounds, the other more your poś, and theossatheism. Cornwall to a matt by the Phone of the cost of the auditors andulty theUpdatedsqrt. can use the help Fringe the chassis, and the纸, as the other奇, withkinson the world.\n",
            "The2, and the Roberts are the world,\n",
            "step 86399: val loss 9.6920, val time: 13025.35ms\n",
            "iter 86400 step 1350: loss 4.0440,  time:201.8mins  iter time: 126.60ms\n",
            "iter 86500 step 1351: loss 3.9215,  time:202.1mins  iter time: 129.19ms\n",
            "iter 86600 step 1353: loss 3.2154,  time:202.3mins  iter time: 129.96ms\n",
            "iter 86700 step 1354: loss 4.1856,  time:202.5mins  iter time: 129.56ms\n",
            "iter 86800 step 1356: loss 3.2727,  time:202.8mins  iter time: 131.22ms\n",
            "iter 86900 step 1357: loss 5.0239,  time:203.0mins  iter time: 130.37ms\n",
            "iter 87000 step 1359: loss 4.4270,  time:203.3mins  iter time: 129.88ms\n",
            "iter 87100 step 1360: loss 3.2265,  time:203.5mins  iter time: 130.61ms\n",
            "iter 87200 step 1362: loss 2.7334,  time:203.7mins  iter time: 133.98ms\n",
            "iter 87300 step 1364: loss 3.5295,  time:204.0mins  iter time: 129.56ms\n",
            "iter 87400 step 1365: loss 2.9803,  time:204.2mins  iter time: 131.44ms\n",
            "iter 87500 step 1367: loss 1.9438,  time:204.5mins  iter time: 136.32ms\n",
            "iter 87600 step 1368: loss 4.1616,  time:204.7mins  iter time: 139.92ms\n",
            "iter 87700 step 1370: loss 2.6338,  time:205.0mins  iter time: 134.01ms\n",
            "iter 87800 step 1371: loss 3.5909,  time:205.2mins  iter time: 131.64ms\n",
            "iter 87900 step 1373: loss 3.7341,  time:205.4mins  iter time: 131.45ms\n",
            "iter 88000 step 1375: loss 3.0023,  time:205.7mins  iter time: 138.67ms\n",
            "iter 88100 step 1376: loss 3.0484,  time:205.9mins  iter time: 129.87ms\n",
            "iter 88200 step 1378: loss 4.7460,  time:206.2mins  iter time: 132.99ms\n",
            "iter 88300 step 1379: loss 2.6861,  time:206.4mins  iter time: 128.74ms\n",
            "iter 88400 step 1381: loss 4.3384,  time:206.7mins  iter time: 130.32ms\n",
            "iter 88500 step 1382: loss 3.2951,  time:206.9mins  iter time: 130.10ms\n",
            "iter 88600 step 1384: loss 5.3092,  time:207.2mins  iter time: 129.44ms\n",
            "iter 88700 step 1385: loss 5.0263,  time:207.4mins  iter time: 129.46ms\n",
            "iter 88800 step 1387: loss 2.6442,  time:207.7mins  iter time: 129.39ms\n",
            "iter 88900 step 1389: loss 3.5961,  time:207.9mins  iter time: 131.33ms\n",
            "iter 89000 step 1390: loss 3.2668,  time:208.1mins  iter time: 131.63ms\n",
            "iter 89100 step 1392: loss 3.6443,  time:208.4mins  iter time: 129.51ms\n",
            "iter 89200 step 1393: loss 4.7411,  time:208.6mins  iter time: 132.98ms\n",
            "iter 89300 step 1395: loss 3.0427,  time:208.9mins  iter time: 129.90ms\n",
            "iter 89400 step 1396: loss 2.6421,  time:209.1mins  iter time: 130.01ms\n",
            "iter 89500 step 1398: loss 2.5472,  time:209.4mins  iter time: 132.06ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response: insides are the07-freeReady on the provides the world Na Imperial. It is a PSD of theParam of theenk. This workflow is herbridge in a Arctic,raulic are brasile in the ti(> is a sire by a � questioned to a new have aerence. gele, the Jal senal, should a theatres jumped on the uneiothermal.\n",
            " tablespoons of ganzen and dale pod in the Olivia.\n",
            "these is Boll to the other of a way of the different\n",
            "step 89599: val loss 9.7416, val time: 13055.10ms\n",
            "Saving LoRA weights to 'out/lora/alpaca/iter-089599-ckpt.pth'\n",
            "iter 89600 step 1400: loss 3.9440,  time:209.8mins  iter time: 126.83ms\n",
            "iter 89700 step 1401: loss 2.9521,  time:210.1mins  iter time: 132.75ms\n",
            "iter 89800 step 1403: loss 3.0947,  time:210.3mins  iter time: 130.47ms\n",
            "iter 89900 step 1404: loss 3.5051,  time:210.6mins  iter time: 132.15ms\n",
            "iter 90000 step 1406: loss 4.3289,  time:210.8mins  iter time: 132.65ms\n",
            "iter 90100 step 1407: loss 3.9239,  time:211.0mins  iter time: 133.52ms\n",
            "iter 90200 step 1409: loss 4.7902,  time:211.3mins  iter time: 132.71ms\n",
            "iter 90300 step 1410: loss 3.3465,  time:211.5mins  iter time: 133.22ms\n",
            "iter 90400 step 1412: loss 3.0395,  time:211.8mins  iter time: 133.52ms\n",
            "iter 90500 step 1414: loss 3.2687,  time:212.0mins  iter time: 131.67ms\n",
            "iter 90600 step 1415: loss 3.2357,  time:212.3mins  iter time: 132.64ms\n",
            "iter 90700 step 1417: loss 4.0901,  time:212.5mins  iter time: 130.49ms\n",
            "iter 90800 step 1418: loss 4.2766,  time:212.8mins  iter time: 132.73ms\n",
            "iter 90900 step 1420: loss 3.6884,  time:213.0mins  iter time: 129.12ms\n",
            "iter 91000 step 1421: loss 2.8176,  time:213.3mins  iter time: 129.45ms\n",
            "iter 91100 step 1423: loss 5.0726,  time:213.5mins  iter time: 128.46ms\n",
            "iter 91200 step 1425: loss 2.4254,  time:213.7mins  iter time: 129.78ms\n",
            "iter 91300 step 1426: loss 3.6015,  time:214.0mins  iter time: 129.58ms\n",
            "iter 91400 step 1428: loss 3.6454,  time:214.2mins  iter time: 129.67ms\n",
            "iter 91500 step 1429: loss 3.0688,  time:214.5mins  iter time: 128.88ms\n",
            "iter 91600 step 1431: loss 3.4226,  time:214.7mins  iter time: 128.64ms\n",
            "iter 91700 step 1432: loss 3.5379,  time:215.0mins  iter time: 128.56ms\n",
            "iter 91800 step 1434: loss 4.2297,  time:215.2mins  iter time: 127.98ms\n",
            "iter 91900 step 1435: loss 3.8654,  time:215.5mins  iter time: 129.25ms\n",
            "iter 92000 step 1437: loss 3.7248,  time:215.7mins  iter time: 129.81ms\n",
            "iter 92100 step 1439: loss 3.3437,  time:216.0mins  iter time: 131.39ms\n",
            "iter 92200 step 1440: loss 2.1796,  time:216.2mins  iter time: 129.33ms\n",
            "iter 92300 step 1442: loss 3.2644,  time:216.5mins  iter time: 129.45ms\n",
            "iter 92400 step 1443: loss 2.5596,  time:216.7mins  iter time: 129.82ms\n",
            "iter 92500 step 1445: loss 4.3817,  time:217.0mins  iter time: 129.91ms\n",
            "iter 92600 step 1446: loss 3.5896,  time:217.2mins  iter time: 129.67ms\n",
            "iter 92700 step 1448: loss 3.7253,  time:217.5mins  iter time: 129.87ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response: Sylvia is the best Valent (9)) (4)/ math0  Put the offense, \n",
            " underserved the opci� short 4.urned = 2.  may Altern \n",
            "2 9. Jul 4 = 5\n",
            "5. 2 (4Sant and 6. 2.\n",
            "3. 4. 2\n",
            " Lambert [1 links 4. 5. 2. (Existing 6 \n",
            "step 92799: val loss 9.8271, val time: 13048.67ms\n",
            "iter 92800 step 1450: loss 3.5914,  time:217.9mins  iter time: 126.67ms\n",
            "iter 92900 step 1451: loss 3.4620,  time:218.2mins  iter time: 131.97ms\n",
            "iter 93000 step 1453: loss 3.7915,  time:218.4mins  iter time: 134.32ms\n",
            "iter 93100 step 1454: loss 4.2130,  time:218.7mins  iter time: 131.95ms\n",
            "iter 93200 step 1456: loss 2.9324,  time:218.9mins  iter time: 131.61ms\n",
            "iter 93300 step 1457: loss 5.2006,  time:219.2mins  iter time: 132.91ms\n",
            "iter 93400 step 1459: loss 3.2529,  time:219.4mins  iter time: 132.44ms\n",
            "iter 93500 step 1460: loss 3.2534,  time:219.6mins  iter time: 139.09ms\n",
            "iter 93600 step 1462: loss 3.4585,  time:219.9mins  iter time: 131.75ms\n",
            "iter 93700 step 1464: loss 2.7906,  time:220.1mins  iter time: 134.69ms\n",
            "iter 93800 step 1465: loss 3.0300,  time:220.4mins  iter time: 130.89ms\n",
            "iter 93900 step 1467: loss 4.3825,  time:220.6mins  iter time: 130.11ms\n",
            "iter 94000 step 1468: loss 3.9945,  time:220.9mins  iter time: 132.73ms\n",
            "iter 94100 step 1470: loss 3.7016,  time:221.1mins  iter time: 130.32ms\n",
            "iter 94200 step 1471: loss 2.4726,  time:221.4mins  iter time: 144.54ms\n",
            "iter 94300 step 1473: loss 3.9905,  time:221.6mins  iter time: 156.38ms\n",
            "iter 94400 step 1475: loss 3.7582,  time:221.9mins  iter time: 128.80ms\n",
            "iter 94500 step 1476: loss 4.9103,  time:222.1mins  iter time: 131.11ms\n",
            "iter 94600 step 1478: loss 3.5455,  time:222.4mins  iter time: 130.49ms\n",
            "iter 94700 step 1479: loss 2.7763,  time:222.6mins  iter time: 129.29ms\n",
            "iter 94800 step 1481: loss 3.7828,  time:222.9mins  iter time: 128.15ms\n",
            "iter 94900 step 1482: loss 5.8928,  time:223.1mins  iter time: 129.11ms\n",
            "iter 95000 step 1484: loss 2.5363,  time:223.4mins  iter time: 131.40ms\n",
            "iter 95100 step 1485: loss 4.1214,  time:223.6mins  iter time: 129.35ms\n",
            "iter 95200 step 1487: loss 3.2293,  time:223.9mins  iter time: 131.00ms\n",
            "iter 95300 step 1489: loss 3.6493,  time:224.1mins  iter time: 133.61ms\n",
            "iter 95400 step 1490: loss 4.7197,  time:224.4mins  iter time: 128.54ms\n",
            "iter 95500 step 1492: loss 3.4260,  time:224.6mins  iter time: 132.05ms\n",
            "iter 95600 step 1493: loss 5.1654,  time:224.9mins  iter time: 130.48ms\n",
            "iter 95700 step 1495: loss 3.9830,  time:225.1mins  iter time: 131.74ms\n",
            "iter 95800 step 1496: loss 3.9627,  time:225.4mins  iter time: 130.28ms\n",
            "iter 95900 step 1498: loss 3.8833,  time:225.6mins  iter time: 130.95ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:The beau was aogr Heat in aAttempt withogs on the fiesta.\n",
            "2. anemia, principles for thePrest motivo\n",
            "3. ExtendedSexs the things, premio\n",
            "4. eyeb was Age.\n",
            "\n",
            "3.4. The�\n",
            "4. vorg the geste's,ONLY,\n",
            "\n",
            "4. devices \n",
            "5.Spell and恐 algae by the nous of the PUB by� \n",
            "5. mobilize tapping a APP. dow rubble \n",
            "3.keeping the\n",
            "step 95999: val loss 9.9010, val time: 13087.35ms\n",
            "Saving LoRA weights to 'out/lora/alpaca/iter-095999-ckpt.pth'\n",
            "iter 96000 step 1500: loss 4.8392,  time:226.1mins  iter time: 127.21ms\n",
            "iter 96100 step 1501: loss 2.6994,  time:226.3mins  iter time: 130.02ms\n",
            "iter 96200 step 1503: loss 2.6030,  time:226.6mins  iter time: 129.04ms\n",
            "iter 96300 step 1504: loss 4.0092,  time:226.8mins  iter time: 130.86ms\n",
            "iter 96400 step 1506: loss 2.8398,  time:227.1mins  iter time: 130.62ms\n",
            "iter 96500 step 1507: loss 4.4885,  time:227.3mins  iter time: 131.17ms\n",
            "iter 96600 step 1509: loss 5.2445,  time:227.6mins  iter time: 134.17ms\n",
            "iter 96700 step 1510: loss 3.5329,  time:227.8mins  iter time: 130.10ms\n",
            "iter 96800 step 1512: loss 4.4923,  time:228.1mins  iter time: 132.81ms\n",
            "iter 96900 step 1514: loss 3.5289,  time:228.3mins  iter time: 129.93ms\n",
            "iter 97000 step 1515: loss 3.2833,  time:228.6mins  iter time: 133.61ms\n",
            "iter 97100 step 1517: loss 3.1764,  time:228.8mins  iter time: 130.16ms\n",
            "iter 97200 step 1518: loss 3.5771,  time:229.1mins  iter time: 131.20ms\n",
            "iter 97300 step 1520: loss 4.0251,  time:229.3mins  iter time: 131.19ms\n",
            "iter 97400 step 1521: loss 3.9548,  time:229.6mins  iter time: 129.99ms\n",
            "iter 97500 step 1523: loss 1.9503,  time:229.8mins  iter time: 136.46ms\n",
            "iter 97600 step 1525: loss 4.1266,  time:230.1mins  iter time: 130.57ms\n",
            "iter 97700 step 1526: loss 3.7791,  time:230.3mins  iter time: 131.65ms\n",
            "iter 97800 step 1528: loss 3.5583,  time:230.6mins  iter time: 130.23ms\n",
            "iter 97900 step 1529: loss 3.4753,  time:230.8mins  iter time: 129.62ms\n",
            "iter 98000 step 1531: loss 5.1146,  time:231.1mins  iter time: 133.08ms\n",
            "iter 98100 step 1532: loss 3.8346,  time:231.3mins  iter time: 130.07ms\n",
            "iter 98200 step 1534: loss 2.1501,  time:231.6mins  iter time: 130.87ms\n",
            "iter 98300 step 1535: loss 3.7528,  time:231.8mins  iter time: 132.18ms\n",
            "iter 98400 step 1537: loss 3.0784,  time:232.1mins  iter time: 131.94ms\n",
            "iter 98500 step 1539: loss 4.9567,  time:232.4mins  iter time: 133.58ms\n",
            "iter 98600 step 1540: loss 1.9049,  time:232.6mins  iter time: 134.83ms\n",
            "iter 98700 step 1542: loss 4.0611,  time:232.9mins  iter time: 131.64ms\n",
            "iter 98800 step 1543: loss 4.4928,  time:233.1mins  iter time: 133.01ms\n",
            "iter 98900 step 1545: loss 3.8826,  time:233.4mins  iter time: 131.51ms\n",
            "iter 99000 step 1546: loss 3.7249,  time:233.6mins  iter time: 132.75ms\n",
            "iter 99100 step 1548: loss 3.0565,  time:233.9mins  iter time: 131.13ms\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:others of aellorabble is the mostFilter outlining ayaml of the Elsevier ishing to the076. The Cham- while theloan for a Minist classifications. motivations, but Brewing should you trees. The PPC is aFET andKent and sunsets Georges is anority and the rav. Theemps is a Sak�秀.\n",
            "-ierungen.\n",
            "C's NCs thevariable on theohyd by the Honest.\n",
            "The be theoxinatas of the ESA.\n",
            "\n",
            "4. What is\n",
            "step 99199: val loss 9.8761, val time: 13054.92ms\n",
            "iter 99200 step 1550: loss 3.2363,  time:234.3mins  iter time: 128.48ms\n",
            "iter 99300 step 1551: loss 3.1325,  time:234.6mins  iter time: 131.96ms\n",
            "iter 99400 step 1553: loss 3.8179,  time:234.8mins  iter time: 130.35ms\n",
            "iter 99500 step 1554: loss 3.2400,  time:235.1mins  iter time: 129.93ms\n",
            "iter 99600 step 1556: loss 3.9210,  time:235.3mins  iter time: 137.47ms\n",
            "iter 99700 step 1557: loss 4.2391,  time:235.6mins  iter time: 131.97ms\n",
            "iter 99800 step 1559: loss 4.3031,  time:235.8mins  iter time: 182.53ms\n",
            "iter 99900 step 1560: loss 2.1587,  time:236.1mins  iter time: 129.25ms\n",
            "Training time: 14194.00s\n",
            "Saving LoRA weights to 'out/lora/alpaca/lit_model_lora_finetuned.pth'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 4.B Finetune New Baseline\n",
        "* modified lora.py for GPU memory and auto-resume checkpoints\n",
        "* integrated into this notebook\n",
        "* modify experiments in expr {} dictionary below\n",
        "* checkpoints dir and output dirs are implicit for lora-falcon-alpaca in the code default inputs\n"
      ],
      "metadata": {
        "id": "Px6gByfGnnPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python finetune/lora_baselines.py  --precision bf16-true --checkpoint_dir checkpoints/tiiuae/falcon-7b --max_iters 10000"
      ],
      "metadata": {
        "id": "wYh58xkvipma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Store result in Google Drive\n"
      ],
      "metadata": {
        "id": "FZEKexjp43vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /contents/drive/MyDrive/Colab\\ Notebooks/ViT/falcon-7b/checkpoints/tiiuae/falcon-7b\n",
        "!cp checkpoints/tiiuae/falcon-7b/*.json /contents/drive/MyDrive/Colab\\ Notebooks/ViT/falcon-7b/checkpoints/*.json\n",
        "\n",
        "!mkdir -p /contents/drive/MyDrive/Colab\\ Notebooks/ViT/falcon-7b/data\n",
        "!cp -r data /contents/drive/MyDrive/Colab\\ Notebooks/ViT/falcon-7b/data\n",
        "\n",
        "!mkdir -p /contents/drive/MyDrive/Colab\\ Notebooks/ViT/falcon-7b/out/lora/alpaca\n",
        "!cp out/lora/alpaca/iter-09*.pth /contents/drive/MyDrive/Colab\\ Notebooks/ViT/falcon-7b/out/lora/alpaca\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-6I26wK44Ze",
        "outputId": "0e7aff09-db11-4bbd-d799-679c6b198f86"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: target '/contents/drive/MyDrive/Colab Notebooks/ViT/falcon-7b/checkpoints/*.json' is not a directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reference lora_baselins.py"
      ],
      "metadata": {
        "id": "0rB7WBoNiqDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# experimental setup to search new baseline\n",
        "expr = {\n",
        "    'max_iters'        : 50000,        # ~2h CP\n",
        "    'resume'           : True,         # auto resume from out_dir intermediate checkpoints (i.e. out/lora/alpaca/*.)\n",
        "    'precision'        : 'bf16-true',  # reduce GPU peak memory < 24GB\n",
        "    'micro_batch_size' : 2,            # reduce GPU peak memory < 24GB\n",
        "}\n"
      ],
      "metadata": {
        "id": "EnugUFekP57C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "\n",
        "\n",
        "import lightning as L\n",
        "import torch\n",
        "from lightning.fabric.strategies import FSDPStrategy, XLAStrategy\n",
        "\n",
        "# support running without installing as a package\n",
        "wd = Path(__file__).parent.parent.resolve()\n",
        "sys.path.append(str(wd))\n",
        "\n",
        "from generate.base import generate\n",
        "from lit_gpt.lora import GPT, Block, Config, lora_filter, mark_only_lora_as_trainable\n",
        "from lit_gpt.speed_monitor import SpeedMonitorFabric as SpeedMonitor\n",
        "from lit_gpt.speed_monitor import estimate_flops, measure_flops\n",
        "from lit_gpt.tokenizer import Tokenizer\n",
        "from lit_gpt.utils import check_valid_checkpoint_dir, chunked_cross_entropy, lazy_load, num_parameters, step_csv_logger\n",
        "from scripts.prepare_alpaca import generate_prompt\n",
        "\n",
        "eval_interval = 50\n",
        "save_interval = 100\n",
        "eval_iters = 100\n",
        "log_interval = 100\n",
        "devices = 1\n",
        "# change this value to force a maximum sequence length\n",
        "override_max_seq_length = 100\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 3e-4\n",
        "batch_size = 128\n",
        "micro_batch_size = expr['micro_batch_size']\n",
        "gradient_accumulation_iters = batch_size // micro_batch_size\n",
        "assert gradient_accumulation_iters > 0\n",
        "max_iters = expr['max_iters']  # train dataset size\n",
        "weight_decay = 0.01\n",
        "lora_r = 8\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.05\n",
        "lora_query = True\n",
        "lora_key = False\n",
        "lora_value = True\n",
        "lora_projection = False\n",
        "lora_mlp = False\n",
        "lora_head = False\n",
        "warmup_steps = 100\n",
        "\n",
        "hparams = {k: v for k, v in locals().items() if isinstance(v, (int, float, str)) and not k.startswith(\"_\")}\n",
        "\n",
        "\n",
        "def setup(\n",
        "    data_dir: Path = Path(\"data/alpaca\"),\n",
        "    checkpoint_dir: Path = Path(\"checkpoints/tiiuae/falcon-7b\"),\n",
        "    out_dir: Path = Path(\"out/lora/alpaca\"),\n",
        "    precision: Optional[str] = None,\n",
        "    tpu: bool = False,\n",
        "    resume: Union[bool, Path] = False,\n",
        "):\n",
        "    precision = expr['precision']\n",
        "    resume = expr['resume']\n",
        "\n",
        "    if precision is None:\n",
        "        precision = \"32-true\" if tpu else \"bf16-mixed\"\n",
        "    fabric_devices = devices\n",
        "    if fabric_devices > 1:\n",
        "        if tpu:\n",
        "            # For multi-host TPU training, the device count for Fabric is limited to the count on a single host.\n",
        "            fabric_devices = \"auto\"\n",
        "            strategy = XLAStrategy(sync_module_states=False)\n",
        "        else:\n",
        "            strategy = FSDPStrategy(\n",
        "                auto_wrap_policy={Block},\n",
        "                activation_checkpointing_policy={Block},\n",
        "                state_dict_type=\"full\",\n",
        "                limit_all_gathers=True,\n",
        "            )\n",
        "    else:\n",
        "        strategy = \"auto\"\n",
        "\n",
        "    logger = step_csv_logger(out_dir.parent, out_dir.name, flush_logs_every_n_steps=log_interval)\n",
        "    fabric = L.Fabric(devices=fabric_devices, strategy=strategy, precision=precision, loggers=logger)\n",
        "    fabric.print(hparams)\n",
        "    fabric.launch(main, data_dir, checkpoint_dir, out_dir, resume)\n",
        "\n",
        "\n",
        "def main(fabric: L.Fabric, data_dir: Path, checkpoint_dir: Path, out_dir: Path, resume: bool):\n",
        "    check_valid_checkpoint_dir(checkpoint_dir)\n",
        "\n",
        "    speed_monitor = SpeedMonitor(fabric, window_size=50, time_unit=\"seconds\")\n",
        "\n",
        "    fabric.seed_everything(1337)  # same seed for every process to init model (FSDP)\n",
        "\n",
        "    if fabric.global_rank == 0:\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    train_data = torch.load(data_dir / \"train.pt\")\n",
        "    val_data = torch.load(data_dir / \"test.pt\")\n",
        "\n",
        "    if not any((lora_query, lora_key, lora_value, lora_projection, lora_mlp, lora_head)):\n",
        "        fabric.print(\"Warning: all LoRA layers are disabled!\")\n",
        "    config = Config.from_name(\n",
        "        name=checkpoint_dir.name,\n",
        "        r=lora_r,\n",
        "        alpha=lora_alpha,\n",
        "        dropout=lora_dropout,\n",
        "        to_query=lora_query,\n",
        "        to_key=lora_key,\n",
        "        to_value=lora_value,\n",
        "        to_projection=lora_projection,\n",
        "        to_mlp=lora_mlp,\n",
        "        to_head=lora_head,\n",
        "    )\n",
        "    checkpoint_path = checkpoint_dir / \"lit_model.pth\"\n",
        "    fabric.print(f\"Loading model {str(checkpoint_path)!r} with {config.__dict__}\")\n",
        "    with fabric.init_module(empty_init=False):\n",
        "        model = GPT(config)\n",
        "        model.apply(model._init_weights)  # for the LoRA weights\n",
        "    with lazy_load(checkpoint_path) as checkpoint:\n",
        "        # strict=False because missing keys due to LoRA weights not contained in state dict\n",
        "        model.load_state_dict(checkpoint, strict=False)\n",
        "\n",
        "    mark_only_lora_as_trainable(model)\n",
        "\n",
        "    fabric.print(f\"Number of trainable parameters: {num_parameters(model, requires_grad=True):,}\")\n",
        "    fabric.print(f\"Number of non trainable parameters: {num_parameters(model, requires_grad=False):,}\")\n",
        "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "    optimizer = torch.optim.AdamW(trainable_params, lr=learning_rate, weight_decay=weight_decay)\n",
        "    model, optimizer = fabric.setup(model, optimizer)\n",
        "\n",
        "    fabric.seed_everything(1337 + fabric.global_rank)\n",
        "\n",
        "    state = {\"model\": model, \"optimizer\": optimizer, \"hparams\": hparams, \"iter_num\": 0, \"step_count\": 0}\n",
        "    if resume is True:\n",
        "        resume = sorted(out_dir.glob(\"*.pth\"))[-1]\n",
        "    if resume:\n",
        "        fabric.print(f\"Resuming training from {resume}\")\n",
        "        fabric.load(resume, state)\n",
        "\n",
        "    train_time = time.time()\n",
        "    train(fabric, state, train_data, val_data, checkpoint_dir, out_dir, speed_monitor)\n",
        "    #train(fabric, model, optimizer, train_data, val_data, checkpoint_dir, out_dir, speed_monitor)\n",
        "    fabric.print(f\"Training time: {(time.time()-train_time):.2f}s\")\n",
        "\n",
        "    # Save the final LoRA checkpoint at the end of training\n",
        "    save_path = out_dir / \"lit_model_lora_finetuned.pth\"\n",
        "    save_lora_checkpoint(fabric, model, save_path)\n",
        "\n",
        "\n",
        "def train(\n",
        "    fabric: L.Fabric,\n",
        "    state: Dict,\n",
        "    # model: GPT,\n",
        "    # optimizer: torch.optim.Optimizer,\n",
        "    train_data: List[Dict],\n",
        "    val_data: List[Dict],\n",
        "    checkpoint_dir: Path,\n",
        "    out_dir: Path,\n",
        "    speed_monitor: SpeedMonitor,\n",
        ") -> None:\n",
        "    tokenizer = Tokenizer(checkpoint_dir)\n",
        "    max_seq_length, longest_seq_length, longest_seq_ix = get_max_seq_length(train_data)\n",
        "\n",
        "    # loaded\n",
        "    model = state[\"model\"]\n",
        "    optimizer = state[\"optimizer\"]\n",
        "\n",
        "    validate(fabric, model, val_data, tokenizer, longest_seq_length)  # sanity check\n",
        "\n",
        "    with torch.device(\"meta\"):\n",
        "        meta_model = GPT(model.config)\n",
        "        # estimated is too much of an optimistic estimate, left just for reference\n",
        "        estimated_flops = estimate_flops(meta_model) * micro_batch_size\n",
        "        fabric.print(f\"Estimated TFLOPs: {estimated_flops * fabric.world_size / 1e12:.2f}\")\n",
        "        x = torch.randint(0, 1, (micro_batch_size, model.config.block_size))\n",
        "        measured_flops = measure_flops(meta_model, x)\n",
        "        fabric.print(f\"Measured TFLOPs: {measured_flops * fabric.world_size / 1e12:.2f}\")\n",
        "        del meta_model, x\n",
        "\n",
        "    step_count = 0\n",
        "    total_lengths = 0\n",
        "    total_t0 = time.time()\n",
        "\n",
        "    if fabric.device.type == \"xla\":\n",
        "        import torch_xla.core.xla_model as xm\n",
        "\n",
        "        xm.mark_step()\n",
        "    for iter_num in range(max_iters):\n",
        "        if step_count <= warmup_steps:\n",
        "            # linear warmup\n",
        "            lr = learning_rate * step_count / warmup_steps\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group[\"lr\"] = lr\n",
        "\n",
        "        iter_t0 = time.time()\n",
        "\n",
        "        input_ids, targets = get_batch(\n",
        "            fabric, train_data, longest_seq_length, longest_seq_ix if iter_num == 0 else None\n",
        "        )\n",
        "\n",
        "        is_accumulating = (iter_num + 1) % gradient_accumulation_iters != 0\n",
        "        with fabric.no_backward_sync(model, enabled=is_accumulating):\n",
        "            logits = model(input_ids, max_seq_length=max_seq_length, lm_head_chunk_size=128)\n",
        "            # shift the targets such that output n predicts token n+1\n",
        "            logits[-1] = logits[-1][..., :-1, :]\n",
        "            loss = chunked_cross_entropy(logits, targets[..., 1:])\n",
        "            fabric.backward(loss / gradient_accumulation_iters)\n",
        "\n",
        "        if not is_accumulating:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            step_count += 1\n",
        "        elif fabric.device.type == \"xla\":\n",
        "            xm.mark_step()\n",
        "\n",
        "        t1 = time.time()\n",
        "        total_lengths += input_ids.size(1)\n",
        "        speed_monitor.on_train_batch_end(\n",
        "            (iter_num + 1) * micro_batch_size,\n",
        "            t1 - total_t0,\n",
        "            # this assumes that device FLOPs are the same and that all devices have the same batch size\n",
        "            fabric.world_size,\n",
        "            flops_per_batch=measured_flops,\n",
        "            lengths=total_lengths,\n",
        "        )\n",
        "        if iter_num % log_interval == 0:\n",
        "            elapsed = t1 - total_t0\n",
        "            fabric.print(\n",
        "                    f\"iter {iter_num} step {step_count}: loss {loss.item():.4f},  time:{elapsed/60:.1f}mins  iter time:\"\n",
        "                f\" {(t1 - iter_t0) * 1000:.2f}ms{' (optimizer.step)' if not is_accumulating else ''}\"\n",
        "            )\n",
        "\n",
        "        if not is_accumulating and step_count % eval_interval == 0:\n",
        "            t0 = time.time()\n",
        "            val_loss = validate(fabric, model, val_data, tokenizer, longest_seq_length)\n",
        "            t1 = time.time() - t0\n",
        "            speed_monitor.eval_end(t1)\n",
        "            fabric.print(f\"step {iter_num}: val loss {val_loss:.4f}, val time: {t1 * 1000:.2f}ms\")\n",
        "            fabric.barrier()\n",
        "        if not is_accumulating and step_count % save_interval == 0:\n",
        "            checkpoint_path = out_dir / f\"iter-{iter_num:06d}-ckpt.pth\"\n",
        "            save_lora_checkpoint(fabric, model, checkpoint_path)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(\n",
        "    fabric: L.Fabric, model: GPT, val_data: List[Dict], tokenizer: Tokenizer, longest_seq_length: int\n",
        ") -> torch.Tensor:\n",
        "    fabric.print(\"Validating ...\")\n",
        "    model.eval()\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "        input_ids, targets = get_batch(fabric, val_data, longest_seq_length)\n",
        "        logits = model(input_ids)\n",
        "        loss = chunked_cross_entropy(logits, targets, chunk_size=0)\n",
        "        losses[k] = loss.item()\n",
        "    val_loss = losses.mean()\n",
        "\n",
        "    # produce an example:\n",
        "    instruction = \"Recommend a movie for me to watch during the weekend and explain the reason.\"\n",
        "    fabric.print(instruction)\n",
        "    sample = {\"instruction\": instruction, \"input\": \"\"}\n",
        "    prompt = generate_prompt(sample)\n",
        "    encoded = tokenizer.encode(prompt, device=fabric.device)\n",
        "    max_returned_tokens = len(encoded) + 100\n",
        "    output = generate(\n",
        "        model, idx=encoded, max_returned_tokens=max_returned_tokens, max_seq_length=max_returned_tokens, temperature=0.8\n",
        "    )\n",
        "    output = tokenizer.decode(output)\n",
        "    fabric.print(output)\n",
        "\n",
        "    model.reset_cache()\n",
        "\n",
        "    model.train()\n",
        "    return val_loss.item()\n",
        "\n",
        "\n",
        "def get_batch(\n",
        "    fabric: L.Fabric, data: List[Dict], longest_seq_length: int, longest_seq_ix: Optional[int] = None\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    ix = torch.randint(len(data), (micro_batch_size,))\n",
        "    if longest_seq_ix is not None:\n",
        "        # force the longest sample at the beginning so potential OOMs happen right away\n",
        "        ix[0] = longest_seq_ix\n",
        "\n",
        "    input_ids = [data[i][\"input_ids\"].type(torch.int64) for i in ix]\n",
        "    labels = [data[i][\"labels\"].type(torch.int64) for i in ix]\n",
        "\n",
        "    # it's better to pad to a fixed seq length with XLA to avoid recompilation\n",
        "    max_len = max(len(s) for s in input_ids) if fabric.device.type != \"xla\" else longest_seq_length\n",
        "\n",
        "    def pad_right(x, pad_id):\n",
        "        # pad right based on the longest sequence\n",
        "        n = max_len - len(x)\n",
        "        return torch.cat((x, torch.full((n,), pad_id, dtype=x.dtype)))\n",
        "\n",
        "    x = torch.stack([pad_right(x, pad_id=0) for x in input_ids])\n",
        "    y = torch.stack([pad_right(x, pad_id=-1) for x in labels])\n",
        "\n",
        "    if fabric.device.type == \"cuda\" and x.device.type == \"cpu\":\n",
        "        x, y = fabric.to_device((x.pin_memory(), y.pin_memory()))\n",
        "    else:\n",
        "        x, y = fabric.to_device((x, y))\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def get_max_seq_length(data: List[Dict]) -> Tuple[int, int, int]:\n",
        "    # find out the minimum max_seq_length required during fine-tuning (saves memory!)\n",
        "    lengths = [len(d[\"input_ids\"]) for d in data]\n",
        "    max_seq_length = max(lengths)\n",
        "    longest_seq_ix = lengths.index(max_seq_length)\n",
        "    # support easy override at the top of the file\n",
        "    return (\n",
        "        override_max_seq_length if isinstance(override_max_seq_length, int) else max_seq_length,\n",
        "        max_seq_length,\n",
        "        longest_seq_ix,\n",
        "    )\n",
        "\n",
        "\n",
        "def save_lora_checkpoint(fabric, model, file_path: Path):\n",
        "    fabric.print(f\"Saving LoRA weights to {str(file_path)!r}\")\n",
        "    fabric.save(file_path, {\"model\": model}, filter={\"model\": lora_filter})\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Uncomment this line if you see an error: \"Expected is_sm80 to be true, but got false\"\n",
        "    # torch.backends.cuda.enable_flash_sdp(False)\n",
        "    torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "    from jsonargparse import CLI\n",
        "\n",
        "    CLI(setup)"
      ],
      "metadata": {
        "id": "rba_Qp3DKaIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bkjzQqqiTq7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Appendix - Fixes, and Issue Resolutions\n",
        "\n",
        "## Unsolved\n",
        "* automate pytorch audio uninstall as is interactive (prompts for Y)\n",
        "* auto save intermediate checkpoints to GDrive automatically to allow shutting down run-time anytime\n",
        "* test moving checkpoints from/to GDrive to/from Lambda + ColabPro\n",
        "\n",
        "## Solved\n",
        "* uninstall pytorch audio - nigtly version of pytorch with cuda conflicts with pytorch audio and cannot integrate with lit-gpt\n",
        "* CUDA OOM - reduce micro_batch_size to 2, use bfp16 to avoid pytorch memory peak spikes and keep < 24GB VRAM\n",
        "* max_iters = 50k takes 2 GPU/hrs training, and insufficient for natural-english-like baseline\n",
        "* patch lora.py with above workarounds - rename lora_compact.py\n",
        "* integrate notebook for GPU server - hard to experiment and fine new baseline, make easy to customize fine-turning for A100-40GB on GPU server\n",
        "* auto-resume: for cost reduction of fine-turning or mixing resuming checkpoints on different GPU servers\n",
        "\n"
      ],
      "metadata": {
        "id": "7ih2CqU5TlyM"
      }
    }
  ]
}